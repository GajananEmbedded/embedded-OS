why do we need IPCs/IPC mechanisms ??

--->refer to slide 40 of freehand_diagrams_06.12.2022.pdf
    -->in this slide, refer to the horizontal red-arrows ??
       ---> a processi/applicationi cannot access another
            processi+1's address-space, so cannot access
            data-segments of another processi+1 - so, 
            cannot interact/exchange data
       --->private address-spaces and private memory-mappings, 
           so there is a form of memory-protection ??
--->so, we need some form of IPC mechanism to 
    interact and exchange data ??   

--->IPC means inter-process communication,
    but in real designs/implementations, 
    there are several forms of IPCS, including 
    mechanisms, for gpos/generic multi-threading and 
    embedded multithreading/multi-tasking - which 
    means, IPCs apply to other contexts, as well - 
    it is just a title....
---->the basic-principles remain the same, with 
     practical-changes 
     --->in the context of processes, there are
         inter-process communication-mechanisms
     --->there are inter-thread communication- 
         mechanisms, in the context of threads - 
         still, treated, as IPCs 
     --->there are inter-task/embedded communication 
         mechanisms, in the context of EOS/RTOS 
         platforms - still, treated, as IPCs
 
     --->there are other forms of communication, like 
         notifications/synchronizations/locking-mechanisms - 
         we saw one of these, in the case of Unix-signals 
     --->a  typical form of IPC mechanism is data-exchange 
         mechanism - again, there are different forms
         of data-exchange mechanisms....

  why do we need IPC mechanisms ?? 
  - if processes need to communicate, 
    for data-exchange - in addition, there will be 
    certain control of concurrent executions of
    processes 
  - there will be needs, for processes to co-ordinate 
    and synchronize - this is for controlling 
    the concurrent executions of multiple processes/threads/
    tasks, as per application's requirements
  - in the case of shared-data problems and shared-resource 
    problems, certain IPC mechanisms are used, 
    to solve certain concurrency issues... 
    
  - there are many forms of IPCs - some 
    are standard, like process to process, 
    thread to thread, and similarly, task-to-task - 
    some of these 
    mechanisms are conventional(regular) - 
    some are unconventional(special cases), 
    like process to device-driver(to the system) or 
    device-driver(from the system) to process, 
    thread to device-driver(system), or 
    device-driver(system) to thread - some
    of these mechanisms and skills are 
    very much needed, for EOS/RTOS platforms
---->in some cases, these IPC-mechanisms and 
     skills will be used, at 
     high-level - some, peculiarities
---->in some cases, these IPC-mechanisms and 
     skills will be used, at 
     low-level - some other, peculiarities
 

---->first, complete the conventional IPC mechanisms and
     you will have a strong-background - next, you will 
     be exposed to unconventional/embedded scenarios 
     --->in addition, multi-threading IPCs
     ---->in addition, unconventional/embedded scenarios are
          covered, in other references...
 
--->these are standard-descriptions and we 
    will provide specific-descriptions, during embedded
    contexts/scenarios, in other documents.... 

--->just for introduction, refer to section 15.6/pages of 
    rt_e_concepts.pdf
    --->many design-patterns using IPCs are 
        presented for embedded scenarios      

what are the different IPC mechanisms ??
  - there are message-queue IPCs
     -->initially, let us understand message 
        queues, in the GPOS context and the 
        main intention will be OS perspective, 
        not application/embedded perspective
    --->explained in a section below ??
============================================================
     -->subsequently, we will be dealing, with 
        embedded/RTOS /application perspectives
        and related IPC mechanisms
--->there are very good embedded learning 
    resources, that 
    provide designs/patterns, for using 
    multitasking and different IPC mechanisms, 
    for embedded
    applications/scenarios
---->developers /other engineers may provide
     good code samples, based on good 
     designs/patterns - however, we need to go
     , with very good conceptual and practical 
     background    
===============================================================

-->to start with, let us understand a simple pipe 
   ipc-mechanism ??

   --->refer to freehand_diagrams_21.12.2021.pdf 
         --->slide nos......1 and 2
         --->this is a typical set-up of an IPC 
             mechanism used to transfer data between 
             processes...
     --->in a typical OS, IPC-objects are present in 
         system-space, so their handles/descriptors
         are passed to user-space - processes can access
         these IPCs, using system-call APIs + handles/
         descriptors - we cannot access directly ?? 
--->handles/descriptors are certain numbers, which 
    are mapped internally to pipe-objects - there is 
    an abstraction layer present 

- there are pipe-IPC objects/mechanisms
     --->pipe is a simple IPC mechanism 
         used, in GPOS, as well as 
         embedded contexts
        --->the implementation details may 
            vary, in other systems, like embedded contexts ??
-->you can refer to section 8.2 of rt_e_concepts for 
   an embedded OS set-up - implementation details and
   programming will differ 
 
    --->initially, let us understand a typical 
        pipe-IPC usage, in a GPOS-context 

============================================================
        --->also, refer to  
               dac_class_queries_25.03.2019.txt
============================================================
        -->in this typical discussion, pipe-IPC 
           usage is described, at the high-level
        --->in further discussions, in this 
            document, we will 
            understand IPCs and their low-level 
            details
 
---->what happens, if we type 
     for "ls -l  | less", in a shell instance ??
      --->this is a typical GPOS usage
       (--->embedded usages will differ  and 
           examples will also, differ)  

--->lunch-break on 21.12.2021.....

--->current, shell-instance will interpret 
    this command and its components ??
     --->ls -l ---> will be treated separately, using 
                    a child-processi+1  
     --->less  ---> will be treated separately, using 
                    another, child-processi+2 
     ----> | (pipe IPC symbol ) 
         ---->based on this | symbol, 
              shell will set-up a pipe-IPC and 
              use the handles - appropriate 
              system-call APis will be used , to 
              set-up a pipe objecti, like in 
              slides 1 and 2 of freehand_diagrams_21.12.2021.pdf 

--->for this scenario, 
    -  we are loading/
      launching two system-utilities, ls 
      and less
    - as per operating-system rules, 
      these will be managed/executed, 
      using two, different-processes
---->as per requirements, two-processes/two active, 
     system-utilities need to 
     communicate-data - that is the requirement
---->can these processes /active, system-utilities 
     communicate , without any OS-service , 
     like IPC mechanisms ?? NO
     ---> if NO, why ??
          ---->these are two, independent-processes/
               active system-utilities, which are 
               assigned independent/private, address-spaces
               and hence, they cannot share any data- 
               segment/page/page-frame... 
 ---->now, the further set of points will be 
      clear ??

    - in this scenario, 
      these two-processes/active system-utilities
      need to be 
      linked, using another OS core-service, 
      called "pipe IPC-mechanism - an inter-
      process communication-mechanism" ??? in this case, 
      the "IPC-mechanism will be an unnamed-pipe",
      instancei, in the system-space - it is a 
      core-service -  this is created, 
      using a system-call API, like pipe() - current, 
      shell-instance of CLI is responsible, for creating 
      a pipe instance and linking the processes, and hence, 
      linking ls and less system-utilities - there
      are some more system-call APIs, for linking 
      the processes,using pipe-IPC -  
      for linking the processes, using an unnamed 
      pipe IPC,shell-instance uses some more 
      OS-services/system-call APIs - in this context, 
      let us ignore other system-call APIs...
       --->refer to code-samples of unnamed-pipes/named-pipes ?? 
--->refer to sys_p_pipe.c and sys_p_pipe1.c 
    --->read the comments 

---->let us refer to a free-hand diagram 
     illustrating the connections between 
     two processes and a pipe IPC mechanism ??
      --->this diagram provides certain 
          illustration of user-space and
          system-space connections
          --->slides 1/2 of freehand_diagrams_21.12.2021.pdf
 

---->most of this set-up is done, by current 
     CLI, shell-instance, along with several core 
     services/system call APIs, in the background
--->if we are programming, we need to use all the
    system-call APIs and make the scenario work  

--->assuming the above steps are successful, 
    following are the operations allowed - refer 
    to the illustration provided below, in this
    document : 
       
---->refer to slides 1 and 2 of freehand_diagrams_21.12.2021.pdf/
     --->Pi/Ai and Pj/Aj are the two children-processes
         described here 
---->in this context, Pi is the current, shell-instance
     --->Pi+1 is one of the children-processes
     --->Pi+2 is another child-process

--->in this context, Pi+1/active-instance of /usr/bin/ls 
    will generate a stream of 
    data,using "file-IO, core-services/system-call APIs" and 
    this data will be copied into  
    a pipe IPC instance's buffer -  a pipe-handle(write-handle)
    and system-call APIs are used, for sending data -  
    likewise, 
     Pi+2//active instance of 
    /usr/bin/less
    will read data-stored, in the pipe-IPC 
    instance's buffer and print it on the current 
    terminal(this is displayed)  -  again a pipe-handle(read-handle)
    and system-call APIs are used, for receiving data - 
    less system utility is convenient to scroll 
    the presented data, a single-page of data, at a time  

--->"less will output collected data to the terminal, 
    one page, at a time" - we can scroll the data
    presented, by less - this is the convenience  
     
         Pi+1--->ls                                  Pi+2---->less   (u/s)
    -----|-------------------------------------------|---------
         +-->(a system-space pipe-objecti/bufferi)--->               (s/s)
      
---->connect all these details, along with slides 1 and
     2 of freehand_slides_21.12.2021.pdf


           - in this context, unnamed-pipe is a 
             core-service managed, by OS/kernel(a shell-CLI is involved) 
           - for this entire set-up, who 
             is setting-up the children processes, 
             a pipe-IPC objecti, and linking them ???
             - children processes are created 
               , with the help of 
              system-call APIs and a
              shell-instance invokes the system-call 
              APIs, for children-processes' creation  
             - pipe-objecti is created, using 
               a system-call API and 
               shell-instance does the request 
             - linking the children-processes,using 
               pipe is done, 
               using system-call APIs, 
               but requested by shell-instance
    ---> there is a lot of 
         background-processing done, in this case 
----->in this context, a CLI's shell-instance is an interpreter
      and based on the command-line/operators, 
      does the appropriate interpretation and 
      several actions 
---->refer to ls -l | less, or 
      ps -e | less
--->in the second case of ps, data is retrieved
    from process manager, using special-interfaces
    and system-call APIs - the extracted data 
    is copied into another pipe-instance,using a  
    pipe-handle(write-handle)  - after this, 
    less(and its process) will read the other end(read-handle) of 
    the pipe to extract this data and list it 
    conveniently - a pipe-handle(read-handle) is used   
 
     - in a modern, OS-system, processes 
       are managed, using certain 
       hierarchy - there will be a well 
       defined parent-children set-up, 
       in the system - processes are 
       created and managed systematically 
     - in a modern OS system, processes 
       are the main entity, which are used 
       to manage active applications and their 
       resources    
        - in the above case, a shell process
          is the parent process(Pi) and it 
          has created two children processes(Pi+1/Pi+2)
          to manage ls -l(ps)  and  less
        - shell has created children processes
          with the help of core of the OS 
 
--->in the above case of command line, 
    there are two processes 
    dealing, with 2 related sub-jobs, meaning 
    the processes/their sub-jobs are serving 
    a single application/job 
    --->a case, for multiple processes, but 
        single application/main job

--->what happens, if the following commands are
    executed ?? in many of these commands, multiple 
    utilities are loaded/executed and linked ??

--->ps -e -o pid,ppid,user,cmd,vsz,rsz |  grep 'root' | less  (or) 
--->ps -e -o pid,ppid,user,cmd,vsz,rsz |  grep -v 'root' | less
    --->how many children processes are created, 
        in the above case ?? 3 
    --->how many pipe IPC mechanisms/instances are 
        created and set-up, in the above case ??  2
    --->is the first child process/active application and 
        second child process/active application linked/
        connect  ?? how ??
         --->using the first pipe IPC instance 
    --->is the second child process/active application 
        and third child process/active application linked  ?? how ??
          --->using another pipe IPC instance 
    --->is the first child process and third  child 
        process linked  ?? as per the command line 
           and operators, first process/active application 
           and third process/active application are
           not linked  
    ---->what is the job done, by ps command ??
            --->list several process/pd details  
    ---->what is the job done, by grep command ?? 
            --->search for certain pattern/patterns
                and output the matching lines - it is 
                a form of filtering 
    ---->what is the job done, by less command ??
            --->take the output of grep, buffer, and 
                present one page , at a time 
                and support scrolling 

 
------>in the above context, there are few interesting 
       OS-features/rules - let us highlight below :

---->a typical pipe object is present, in kernel 
     space and has a system-space buffer - data can be 
     written into this pipe buffer, using system 
     call API - data can be read from this system-space
     buffer, using system call API  
---->for practical purposes, there is a single 
     IPC buffer of limited-size only ...
---->there is no data separation of messages sent,
     to a pipe IPC buffer - compare these features,against 
     message-queue's working - you will find 
     clear differences ....
----->we will resume on 08.07.2021....at 10 am....

--->let us refer to a code-samples - sys_p_pipe.c/sys_p_pipe1.c
 
    -->initially, understand pipe() system-call APi 
    --->understand handles to a pipe-objecti
    --->how to pass these handles to  read() system-call 
        API and write() system-call APi ??  

--->we will resume on 30.05.2022....at 9.30 am....


--->let us refer to a code-samples - sys_p_pipe.c/sys_p_pipe1.c
      ---->what happens, if a processi+1/Pi+1/active-ls
           or active-ps(any active-applicationi) 
           writes to a pipe-objectj's buffer- 
           instance and there is no more space to 
           write into the buffer of the pipe-objectj ??
           --->current active, processi+1/active-applicationi+1
               will be blocked(active-application/ls or ps 
               will be blocked), in a system-call API - in this 
               case, it will be a write() system-call API...
           --->this blocking-operation is, due to 
               pipe-object instance and its wq/wait-queue list,
               as per certain conditions..
           --->so, pdi+1 will be added to wq/wait-queue list of 
              pipe-objectj 
           --->refer to slides 1 and 2 of freehand_diagrams_21.12.2021.pdf

      ---->what happens, when processi+2/Pi+2/active-grep, or 
           active-less
           is scheduled/dispatched/executed and attempts
           to read data from a pipe-IPC objectj, but 
           there is no data, yet ??
           --->current, active, processi+2/active-applicationi+2
               will be blocked, in a system-call API, read() 
               system-call API...
           --->this blocking-operation is due to 
               pipe-object instance and its wq/wait-queue list, 
               as per certain conditions...
           --->since there is no data, processi+2/pdi+2 
               will be blocked and added to pipe-object's
               wq  

--->synchronization is a form of co-ordination between 
    processes and is done, using IPC mechanisms 
--->data-exchange is different and synchronization is 
    different - however, they can be used together 

----->the above actions and below actions,
       are part of "synchronization-mechanism", which 
       is built into the 
      pipe-IPC mechanism and its operations of system-call 
      APIs.. - unblocking
      will be done, when a receiver, receives data, or 
      a sender, sends data, as per the context
      --->for instance, if a sender-processi+1 is blocked, 
          for space, it will be unblocked, due to  a
          read() system-call operation, by a receiver- 
          processi+2 - read() system-call routine 
          will do the unblocking  
      --->for instance, if a receiver-processi+2 is blocked, 
          for data availability, it will be unblocked, due to  a
          write() system-call operation, by a sender- 
          processi+1 - in this case, write() system-call routine
          is doing the unblocking  
---->the above actions are part of synchronization-mechanisms
     of pipe-IPC object's operations 
 
--->let us refer to freehand_diagrams_21.12.2021.pdf/ 
    slide 1 and related
    comments 
  ---->similar discussions will be repeated, in certain     
       code samples and scenaios 
          ---->sys_p_pipe.c/sys_p_pipe1.c
              --->read the comments...
              --->read() and write() system call APIs are
                  used
              --->there are many background processing rules..
              --->we will come back to this code, after 
                  learning more on read()/write() system call APis
                  and related topics...
             ---->there are special-tables, in pds, which 
                  manage these handles and their rules....??
                  --->we will come back to this code, after 
                      learning more on these special-tables
                      and related topics...
 
           
  - there are IPC signals/notifications...signals/notifications
    are part of IPC mechanisms...
     --->switch to 8_signals.txt  and 
         come back - in our case, we have already 
         complete this part - connect the details of
         this document, with SIGPIPE signal and its actions
     --->if we access a pipe-objectj, using system-call 
         APIs, but break certain rules, system will 
         generate SIGPIPE to the respective process/
         active-application - there will be a forced-
         termination....
   
     --->we will see more details on pipes and notifications, 
         in the context of embedded, where there
         are more customized-forms of IPCs....? 

--->third-break on 21.12.2021.....at 4.45 pm....

  - there are  shared memory IPCs..also, for data-exchange.. 
         --->described, in this document - in one of 
             the sections
=========================================================
  - you may study and work, with 
    network socket IPCs, for connecting 
    client - server sw models
       -->you may use system call APIs or 
          utilities, for accessing socket 
          services/IPCs of core of the OS   
========================================================

   --->another set of IPCs are not for 
       data-exchange, but synchronization/
       locking  
       between processes/threads/other 
       entities,like RTOS tasks 
   	- semaphores are commonly used 
   	- mutexes are commonly used

   - there may be other implementations we may 
     come across, as we progress, in our discussions
 
--->we will come across IPCs, in the context of 
    embedded, as well as several interesting 
    application scenarios

---->initially, understand the different-mechanisms and
     their internal-working
      ---->this first-part,  is very critical 
---->as we progress, coding and other specific-issues will
     be explored 


-->initially, we will understand and use 
   GPOS mechanisms
      -->this will clear all the basic 
         principles  
-->later, we will come across EOS and RTOS 
   mechanisms and scenarios  
    --->certain features will be different, 
        in EOS/RTOS platforms
     --->these are more flexible and well designed, 
         for "embedded applications"
     --->system-call APIs will change
     --->system-APIs will change 
     --->certain rules will change and be more stricter
  

--->most of the basic principles of IPC mechanisms 
    apply to "processes/threads and embedded 
    scenarios"
     -->however, embedded-scenarios will be 
        more peculiar, than typical GPOS  

---->once we understand the basic-mechanisms, there will 
     more specialized and possibly, hybrid-mechanisms...
 

what is the use of system call APIs/system APIs, in most of 
the IPC mechanims ??
   --->most of these mechanisms are managed, in 
       certain core component of OS - to access 
       these services, user-space code requires
       system call APIs and their service-routines - 
       no direct access is 
       possible
   ---->in certain cases, we may use system 
        utilities, but these system utilities 
        will be using system call APIs, in their 
        background processing 
   ---->in certain scenarios, we will be using 
        system-APIs, not system-call APIs...
        --->these are , for specialized-scenarios 
        --->however, we will be using core-services
            of IPC-mechanisms of OS 
---->in the above pipe-related discussions, 
     we used read() and write() system call APis, 
     for reading data and writing data
     -->for creating a pipe-object, we need another, 
        system-call API... 

--->to access IPC-mechanisms and their services/
    operations,we need to use appropriate 
    system-call APIs
    -->there is another, set of message-queue system-call APIs
       --->for creation of message queues
       --->for deletion of message queues
       -->for Tx and Rx messages to /from message
          queues /queue objects, we need system call APIs
    ---->the details are following below  
    -->there is a set of system call APIs to 
       access services of shared-memory data segments
        --->for creation/set-up of shared-memory objects
        --->for attaching processes' shared-data
            segments to shared-memory system objects
    ---->the details are following below  

    -->there is another set of system-call APIs
       which are used , for semaphores/mutexes
         -->creation of mutexes/semaphores IPC 
            instances 
         -->initialization  of mutexes/semaphores
            instances 
         -->for decrementing/incrementing  
            semaphores' count - these are certain 
            semaphore operations
         -->for locking /unlocking mutexes' states
            --->these are mutex operations... 
         -->similar operations require system-call APIs 

   -->concurrent-programming, using operating- 
      system services/system-call APIs
      will be involved 
        --->there will be more practical 
            issues, when we use IPC mechanisms  
   --->along with multi-tasking services, 
       IPC services can enable more intelligent,
       concurrent-programming, for applications 
       --->ultimately, it is for providing 
           services to applications  

how to use them, in our applications ??

--->initially, we will use GPOS examples
--->later, we will have embedded-scenarios and 
    code samples - it is a challenge ??
     ---->we need to have a different-perspective
          of embedded-programming ?? 

--->the following discussions are based on 
    GPOS-systems and scenarios 

======================================================
- ideally, processes cannot communicate, with
  each other - they are unable to access each  
  others' private data-segments
    -typically, in user-space, there are
     no shared data-segments, only 
     private data-segments 
--->most of the core services/mechanisms
    are maintained, in system-space,so that 
    they can be accessed from different 
    processes, including shared IPC-services 
    ---->why are most of the IPC services/objects/
         buffers located, in the kernel-space ??
         --->kernel-space/kernel-code/
             kernel-data is shared among 
             all the processes, as well as 
             isolated from user-space/user-land access
         --->still, can be accessed, using 
             system-call APIs  

    ---->why not maintain IPC objects/buffers, in the user-space ??
         --->since each process has its 
             own, private address-space, it is not possible 
    ---->however, there are special-scenarios, 
         which we will see, in this and other documents
         --->based on the OS platform's 
             archictecture and design, some of 
             these set-up details will differ

---->so, initially, let us follow a GPOS architecture
     and design, which is standarized...so, most 
     of the IPC-mechanisms are conventionally set-up 
     and managed...
  
--->many of the system-objects related 
    to core-services/mechanisms are 
    maintained, in kernel-space/system-space
    --->as part of this, IPC-mechanisms 
        and their system objects are 
        mostly, maintained in system-space/
        kernel-space - reasoning is as above 

- so, in most cases, there are IPC mechanisms
  that are provided by the core of the OS 
  kernel - meaning, these IPC mechanisms 
  are supported by the kernel - these IPC 
  mechanisms reside, in the kernel/kernel-space-
  the kernel maintains the system 
  objects/buffers and system call service 
  routines - if one or more processes need to
  access IPC mechanisms,system call APIs are 
  needed to access the IPC services
     -->how  are these IPC objects/buffers 
        maintained, in the system-space 
        accessible to different processes ??
          -->see the discussions below 
       
    --->in a typical, GPOS-system, every process
        has a dedicated kernel-space/system-space
        ,but all these kernel-space/system-space
        segments of a process are mapped to the 
        same set of page-frames
    -->so, effectively, all the processes share 
       a common system-space - "shared system-space"
    -->due this set-up, "several IPC-mechanisms 
       are resident", in the "shared system-space" - 
       their respective system objects and
       system buffers are shared - one or 
       more processes can access the shared IPC 
       mechanisms/objects/buffers, using system- 
       call APIs
----->refer to 4_intro_mem_mgmt.txt.....   
===============================================================
 
---->we are switching to specifically,, 
     message-queue IPC mechanisms??
--->a message-queue  is also a 
    popular IPC-mechanism  

--->there are similarities and differences
    between message-queue IPCs 
    and pipe-IPCs
        --->in the case of pipe-IPCs, a stream 
             of data is exchanged
        ---->in the case a message-queue, 
             messages are
             exchanged - meaning, well-defined, messages, 
             with 
             data-boundaries are used 
--->refer to freehand_diagrams_21.12.2021.pdf/
    slides 6-8  
    of message-queue objects

---->what are the differences between a pipe- 
     IPC mechanism-instance explained above 
     and a message-queue IPC-instance explained
     here ??
     ---->there is just a single, undivided
          pipe-IPC buffer...all the data 
          are stored, without boundaries, in 
          a single-IPC buffer 
     ---->there are several well separated, 
          message-queue buffers - each message
          is stored, in a single-buffer- well-defined,
          message-boundaries are maintained
     ---->more features and rules will be 
          explained, in different-contexts 

---->how do these mechanisms differ from application 
     perspective - data-point of view ??
     --->pipe just maintains a stream of data, 
         which cannot separate, different-messages
         and maintain boundaries - no separation of messages

     ---->message-queue can separate and maintain 
          different-messages, using well-defined
          message-boundaries, due to separate
          buffers...

   ----->based on the above characteristics, you 
         can decide, what to use and when ?? 
         --->do not be biased, for now...??

======================================================================
---->we will see enough code-samples/scenarios, 
     in the near-future
---->just for introduction, refer to one of the 
     RTOS-slides, which shows certain code and 
     usage diagrams ,4_rtos_internals_frtos.pdf,  slides 4 and 24
     ---->in these slides, you will come across typical 
          usage models of message queues, in embedded
          scenarios - mostly, application perspective
     ---->the APIs will differ
     ---->basic principles will remain the same - 
          message boundaries,  and blocking an d
          unbocking operations will be similar...
     ---->usage-models are based on certain application 
          designs/ models
     ---->you need to have all the basics, in place,
          before touching these slides/scenarios/code samples
     ---->in these contexts, the discussions will be more
          on the usage models/application design models, 
          from application perspective, not OS perspective
     ---->EOS/rtos documentation/manuals will be similarly written
     ---->there are certain specialized-references, for 
          such study and work 
======================================================================
 
----->in this document, we will stick to GPOS perspectives 

--->first break on 30.05.2022...

 - to start with, we will be dealing with message
   queue IPC mechanism :
--->in addition, refer to usage models of 
    message queues, as discussed earlier, 
    during chapter 5/6 of text book - revise
    these chapters, along with the following 
    discussions
    ----->refer to cc_2.txt, and related 
          freehand_diagrams_04.12.2021.pdf/slides 9-11...

         --->slide 9 - a receiver processj is blocked, since
                       there is no message in mqi
         --->slide 10 - a sender processi sends a message to 
                       mqi and also, unblocks processj
         --->slide 11 - after unblocking, sometime in the future
                        there will be a trigger and scheduler will 
                        be invoked - as part of this, receive 
                        system-call routine is resumed and 
                        completed - as part completing the system-call
                        routine, data will be copied into user-space
                        buffer - finally, processj is resumed in 
                        user-space and continued ..... 
      

    - in this context, we will be using 
      a typical GPOS message queue - 
      in GPOS as well as other OS platforms, 
      we may come across other forms of 
      message queues, as well - their basic principles
      will be the same, but may support 
      more features or less features 
    - "each message-queue IPC instancei" is 
      represented and managed, "using 
      a system objecti"
---->refer to freehand_diagrams_21.12.2021.pdf, slides 6-9
      and    freehand_diagrams_04.12.2021.pdf/slides 9-11...
 
    - this "message queue IPC objecti" maintains a queue
      of messages/kernel buffers - these are  a list of 
      kernel buffers, along with message headers, 
      which contain process/application data 
      to be exchanged - messages sent, by applications/
      processes are stored, in these kernel buffers and
      these kernel buffers are queued, in a message queue 
    - a message-queue objecti also maintains a wqi/
      wait-listi, for
      implicit co-ordination and synchronization of 
      processes,during exchange of data
---->refer to freehand_diagrams_21.12.2021.pdf/ slides 6-9 
         and  freehand_diagrams_04.12.2021.pdf/slides 9-11...
         -->this involves blocking/unblocking 
            of processes, as part of message
            queue system call APIs and related
            services - supports "blocking/unblocking 
            operations", in processes/threads of 
            application code
         --->in most of the IPC mechanisms, there
             will be some form of co-ordination/
             synchronization and related operations, 
             which involve blocking and unblocking 
             of processes/threads involved , in 
             the applications  
   ---->what is an asynchronous, IPC-mechanism, in
        this context ?? typically, a message-queue
        is an asynchronous, IPC-mechanism...
             --->can a sender-processi+1, send a message
        	 to a message-queue objecti, when there is no 
                 receiver is currently, scheduled/dispatched/
                 executing ? YES
            ---->can a receiver-processi+2,
                 receive a message, when 
                 no sender is currently, scheduled/
                 dispatched/executing ? YES
      
    -     in this context, a message queue mechanism is 
      used and it "supports asynchronous-  
      services" - meaning, two or more processes
      can communicate, with a message-queue, 
      asynchronously(processes can access the 
      message-queue object/buffers, at any point of time-
      fairly, independently ), 
      using system-call APIs - 
      means, a processi can send/receive data to
      /from  the 
      message queuej, at any time, independent of 
      other processes - this means, asynchronous
      , in this context 

        -->the message-queue objecti/a resourcei 
           is a shared resource-entityi, but 
           can be accessed, concurrently - 
           if needed, message-queue services
           /system-call APIs will take care
           of any co-ordination/synchronization/
           locking - these operations are taken
           care, implicitly - the system/service
           routines will intelligently take care 
---->refer to slides of freehand_diagrams_21.12.2021.pdf/
     slide 9
       --->some of these points will be clear, as we
           understand the system-space set-up ??

       -->see the detailed discussions on 
          concurrent access of resources/objects/
          buffers/lists, 
          below   
            --->two or more processes may 
            access a message queue objecti/
            related buffers, concurrently, 
            in an uniprocessor system 

            --->two or more processes may 
            access a message queue objecti/
            related buffers, concurrently, 
            in a multiprocessor system - this is more 
            common and creates more problems...
---->some of these scenarios and their problems will be 
     described below...??

---->first break on 08.07.2021.. 

---->we will resume on 22.12.2021....at 10.30 am ....
             
    - to send any message to a message-queue objj
      , we need a system-call API - sender-processi
      will typically, use "send system-call APIs" 
  
    - similarly, receiver-processes will use 
      receive system-call APIs to receive 
      one or messages, from a message-queue objj
---->following a typical, scenario of concurrency 
     and a message-queue IPC mechanism, along 
     with frame-work and rules :
 
    - what happens, if a sender-processi
      sends a messagek/datak 
      , before the receiverj is ready, for receiving 
      the messages ?
---->meaning, asynchronously, sent from the sender-
     processi, before the receiverj is requesting 
     for data/messages  
       --->meaning, sender-processi sends a messagek, 
           before the receiverj is scheduled/dispatched/
           executed 
         --> the message(s)/data from the sender- 
             processi will 
             be copied into a  message-queue
             object's bufferk and added to a queue, 
             in the message-queue objectj 
             --->a queue/list of messages will be 
                 maintained, in the message queue 
                 objectj 
----->refer to the freehand_diagram_21.12..2021.pdf/
      slides 6/7 of 
      message queue object, once more and 
      continue reading 
---->so, if there is a processi, which wishes to 
     send
     a messagek/datak to processj, using a message-queue objectj, 
     what actually is done, in the background ??
      --->uses a system-call APIi, along with message-queue 
          object handlej, to send a messagek/datak's user-space
          buffer/pointer - we will see more, in the coding ??
      --->service-routinei of system call APIi will copy 
          this datak/messagek from user-space buffer
           into a kernel, message-bufferk
      --->this message bufferk will be added to a queue of 
           messages, in the message-queue objectj's 
      --->once all these are done, typically, system-call 
          processing is completed and system-call API 
            returns back to
          processi 
      --->in this context, we assume, that there is no
          other processj currently requesting for 
          messages/data from message-queue objectj

  --->what will happen to this stored messagek/datak, 
      in the near future ?? we expect that a 
      receiver-processj will receive this stored
      messagek, using another system call API  
      --->similar to the sender process' background 
          processing, there will be background 
          processing, when receiver processj 
          invokes a system call APIi+1
          --->in this context, the system call 
              service routinei+1 will copy data
              from oldest messagek of the message-queue list into 
              user-space buffer/pointer of processj and 
              this oldest-messagek will be consumed
              --->what is the meaning of consumed ??
                  ---->used, deleted and freed - meaning, 
                       header/kernel-buffer of the respective
                       message is deleted from message-queue
--->system-call APis pass appropriate parameters, 
    like pointer to user-space buffer and message-queue
    object's handle...
    ---->again, we will see more details, in coding ?? 
 
-->refer to  freehand_diagrams_04.12.2021.pdf/slides 9-11...
    - what happens, if a receiver-processj of your 
      application attempts to 
      receive a messagek, before a sender has 
      sent a message ?? another scenario   
        --->if the receiver processj/pdj is 
            scheduled/dispatched/executed, 
            before the sender processi is
            scheduled/dispatched/executed
       ---->in this case, there will be no 
            messagek/datak, in the message queue objectj, since 
            receiver processj is scheduled/dispatched
            /executed, before the sender processi ??
        --> in this case, receiver-processj 
            will be blocked, in the wqj of 
            the respective message-queue objectj
            and scheduler will be invoked - 
            receiver processj will be blocked and 
            scheduler will schedule/dispatch another 
            eligible process?? - in this context, 
            system call APIi+1's service-routine has done its 
            job, for 
            now  
            - in near the future, when there is 
            a new messagek's arrival, due to 
            a sender-processi sending a messagek/datak, 
            using system-call APIi, the receiver-
            processj will be unblocked and 
            added to a Rq - how this unblocking of 
            processj happens ?? who does this unblocking ?? 

-->refer to  freehand_diagrams_04.12.2021.pdf/slides 9-11...
---->in this context, as part of send system call APIi, 
     a messagek/datak is copied into message queue of
     message queue objectj, as well as wqj of the
     message queue objectj is scanned - so, unblocking
     of processj is done, as part of system call APIi/
     sending message system call APIi, in the background 
     processing - based on such a set-up, processes/threads
     and a message queue of an application are loosely 
     coupled, at the OS level and tightly coupled, at 
     the application level - however, background-processing 
     of OS plays its important role 
---->at this stage, receiver-processj is just unblocked 
     and added to an Rq 
      ----> in the near 
            future, when scheduled/
            dispatched/executed,the receiver processj will 
            copy the messagek/datak from 
            a message queue objectj's message buffer(s)
            --->since the receiver-processj was blocked
                , in system-call processing of system-call
                APIi+1, when resumed, it will continue and 
                complete the system call processing of 
                system call APIi+1's service-routine 
            -->there will a copy of message/data from 
               a system-space buffer to an user-space
               buffer 
            -->once done, system call processing will 
               complete and returns back to user-space, in 
               processj
---->try to understand the receive system-call APi/its
     service-routine, during the above cycle of invokation,blocking, 
     unblocking, rescheduling and resuming, and finally, 
     completing and returning back to user-space ?? 
  
     - for very low-level details of system call APIs
       and blocking/unblocking details, refer to 
       ch5/ch6 crowley slides(from the book's author) 
       and notes,cc_2.txt - 
       --->as you see these slides/diagrams, as well as
           revise to cc_2.txt - lines 1090 onwards.....  

---->in all these cases, let us assume, that a 
     message queue maintains incoming messages, 
     in the order of arrival only - FIFO only - 
     so, if a message is requested, the oldest 
     message is delivered and consumed 
---->however, there are special implementations/
     scenarios, where a message-queue object/
     its APIs can support sending messages to 
     the front of the queue, by jumping the queue 
     , for important messages - so, it is upto 
     the application-code to send messages , 
     as per its importance
---->in embedded scenarios, we may come across other
     message queue list management, like urgent 
     messages, or adding messages to the front of 
     a message queue list, not to the back of a 
     message queue list...
---->refer to page 135-141  of 
     MasteringFreeRTOStutorial.......pdf..... 
      --->there are special RTOS-apis, for 
          sending messages ?? try to connect the details ??

(in the future, you will deal with a slighly different 
   set of message queues , in embedded platforms-
    basics do not change, but implementation and 
    programming details will change )
----->for now, just use the basic code from msg_client.c and
       msg_server.c - ignore other features used, 
        in these code samples  
----->we will encounter one or two problems on 
      message queues,  in 
      assignment-4


--->in certain message-queue IPCs, fixed-size
    messages/data are allowed, only - this is 
    their design of message-queues - one form 
    of message-queue IPC design 
 
 - certain message-queues' implementations may 
       support variable size messages/buffers - 
       another form of message-queue IPC design 
   --->we need to understand specific details 
       of message-queues, as per OS platform 
---->just be aware of these details, for now ??

---->we will pend the detailed, code sample discussions, 
     for now ?? --->pushed down to the end of 
     this document 
==============================================================
--->we will study /discuss system call APIs and
    related programming, at the end of this 
    document    
           ---->refer to Linux message queues, in 
                msg_server.c / msg_client.c
               -->we will refer to code/demos, 
                  at the end of document
=============================================================== 
     - certain message queues may support 
       a fixed size messages/buffers, where 
       the size is fixed, at the time of creation 
       of the message queue  
        --->GPOS message queue mechanisms 
            may support variable size messages and
            fairly, flexible no. of messages...  
        --->embedded / RTOS platforms may 
            support fixed-size message queues
            and in addition, variable size 
            message queues - more flexible 
            forms of message queues - in addition, 
            no. of messages will be of pre-defined...
---->some of these are very practical features, so 
     you will understand, during system-call APIs and practicals... 

        --->GPOS may support certain features, 
            which may not be available, in 
            a typical RTOS /eos platforms
            -->there is a message type feature
               , in GPOS, which is not 
               supported, in a typical, RTOS - meaning, certain 
               features not supported, at the operating
               system level
            --->still, we may find, certain work-arounds... 

        ---> in  a typical, GPOS, "keys/ids" 
             are used to manage message-queue
             objects/

        ---> in an RTOS/eos platforms, handles 
             (pointers) to IPC-objects may 
             be used to manage/
             access message-queues - in many cases, 
             we may directly access pointers to 
             message queue objects..
--->abuse of certain message-queue handles may lead
    to strange crashes..

--->there are certain, specific timing related 
    features, in RTOS platforms, for message- 
    queue operations - these will be additional 
    features, in a specific EOS/RTOS platform 


--->synchronization of processes/threads/embedded
    tasks is a form of co-ordination of processes/
    threads/embedded tasks, using certain 
    mechanisms of OS - in many cases, there is 
    support, for implicit-sychronization of 
    processes/threads/tasks... 
 
  - in a typical message-queue mechanism of GPOS/EOS, 
         there is support for "implicit 
         synchronization between processes/threads", 
         using wq(a wait queue of processes/pds) and 
         blocking/unblocking operations
         of system call APIs 
           -->this is a form of implicit
              synchronization/co-ordination
              supported by OS/OS mechanisms  
           --->meaning, when we use a typical
               message queue IPC mechanism, 
               synchronization is taken care, 
               implicitly  
--->as part of implicit synchronization/
    co-ordination, processes/pds are blocked, 
    for certain conditions and unblocked, for 
    certain conditions
--->many of these synchronizations/co-ordinations
    support applications' designs/models
    --->these OS-features support sophisticated,
        application-designs
   
======================================================
 
--->assuming you do not have an OS-platform, 
    how do you handle such synchronization/
    co-ordination issues, in your application code ??
    --->certain sw-flags may be used ??
    ---->are these flags efficient, from application 
         perspective ?? too much polling/scanning 
         may be needed 
    ---->in addition, hw interrupt events are also 
         used to trigger synchronization/co-ordination 
         --->interrupts/ISRs will be partly good -
              interrupts/ISRs can provide a less-efficient, 
              synchronization/co-ordination
   ---->however, without an OS platform , we may 
        not be able to provide an efficient and 
        deterministic/predictable solutions
        --->specifically, RTOS platforms will 
            provide more customizable-features ??


     --->what is the use of implicit synchronization, 
         for applications/their jobs ??
       --->these synchronization mechanisms 
           help applications/developers to 
           satisfy the requirements of concurrent 
           jobs, efficiently and deterministically 
           -->we will be using typical OS 
              synchronization-mechanisms, 
              along with embedded-techniques, 
              like polling/interrupt notification 
              --->some of these will be understood, 
                  in eos/RTOS-contexts  
    --->these implicit synchronization/
        co-ordination mechanisms are useful, 
        for most applications, including 
        embedded-scenarios - for the same 
        reasons highlighted above - due to 
        such implicit-synchronizations, application 
        code is cleaner and robust...
    --->we will understand these details more, 
        during programming , in different 
        platforms and scenarios ?? 
    
           ---> we will come across more cases
                of "implicit" and "explicit" 
                synchronizations supported by OS
              --->in the context of implicit 
                  synchronization scenarios, 
                  developer's code will not 
                  handle synchronization issues -
                  minimal-efforts from 
                  developer's side
              --->in the context of explicit 
                  synchronization scenarios,
                  developer's code will handle
                  synchronization issues - more
                  coding is involved, along with 
                  more responsibilities, from 
                  developer's side 
         --->implicit synchronization, as well 
             as explicit synchronization will affect 
             our application's behaviour - so, we 
             need to understand thoroughly and implement 
             our coding.... 
         --->in the case of explicit-synchronization, we need to 
             have thorough understanding, anyways
  ============================================================

---->depending upon the OS platforms and our 
     applications, we may need to use implicit, 
     as well as explicit synchronizations, as needed
     --->after more study,we will understand
         different usage scenarios...  
 
       - in GPOS,as well as other eOS/rtos platforms, 
         there is support for special flags/
         options, 
         in system call APIs, which can be 
         used to change the behaviour of the 
         services /features, as per the 
         requirements of applications
          -->some of the features will be 
             discussed, during programming  
             and application context 
       -most of the features are provided to 
        support requirements of applications, 
        in different systems
--->by changing certain flags/options, in system call 
    APIs, we can change the behaviour of these 
    system call APIs, in our application code - 
    this will affect the behaviour of our application
    /application code
 
=========================================================
       -certain message queues will support 
        uni-directional data-exchange only 
        and others may support bi-directional 
        data-exchange - we need to understand
        and use them, in our applications 
         -->in  a GPOS system, bi-directional 
            communication may be supported
             --->in this context, overheads
                 will be more 
            --->accordingly, coding will differ 
         -->in an RTOS/embedded scenario, 
            only uni-directional support is 
            provided - a simple, light-weight 
            support is provided
            --->in this context, overheads will 
                be minimal/less     
            --->accordingly, coding will differ 

--->due to these differences, in the features, 
    programming techniques will be different
--->the following message type feature will 
    be discussed, along with system call APIs 
    and programming 
---->you may read the following, along with 
     code samples, in this scenario
 
       -in a certain message queue design, where there
        is support, for bi-directional data-exchange, 
        message type is suppoted, meaning every 
        message is assoicated, with a message 
        type and the sending and receiving 
        processes co-ordinate, accordingly
          --->for instance, process A and 
              process B may share a bi-directional 
              message queue 
          --->process A will send messages of type B
              and process B will receive messages
              of type B only 
          --->similarly, process B will send messages
              of type A and process A will receive 
              messages of type A only
          -->for this set-up to work, the system 
             call APIs and the messages must support
             type field and appropriate filtering
         --->some of these details will be 
             explained/commented, in the programs
    -->if a given OS platform does not support 
       bi-directional message queues, we need to 
       set-up separate message queues, for 
       bi-directional communication and use their
       services, appropriately
=================================================================

   -->in the context of GPOS, we will understand
      Linux system call APIs and code samples
         --->we will study the code samples
             at the end of this document
--->for now, we will just see the system-call APis
    in these code-samples ??

---->for now, let us go through msg_client.c 
     and msg_server.c, and understand certain, 
     system-call APis....?? read the comments and 
     understand the scenarios 
     --->first, refer to msg_client.c and comments 
         --->understand the program-logic and
             the coding, using system-call APis -
             msgsnd() is used to send an user-space 
             message to mqi 
     --->first, refer to msg_server.c and comments 
         --->understand the program-logic and
             the coding, using system-call APis - 
             this code will use msgrcv() system-call
             API to receive data and process it 
     --->we must use a common-KEY, to access a
         common, message-queue objecti, in 
         msg_client.c and msg_server.c 
---->based on the above discussions, we have a good understanding 
     of message-queue's system-call APis, in a typical, 
     GPOS
     --->in the above design of msg_server.c, fork() is 
         used to create children processes to process
         messages received - so, the parent process is 
         dedicated to msgrcv() and block for further messages - 
         further messages will be received and passed to 
         children processes - so, effectively, there is a 
         form of multi-tasking and concurrent executions of 
         parent and children processes - visualize the 
         above set-up in uni-processor and multi-processor
         systems ?? 

    --->in the above case, msg_client.c is a single-threaded
        application - only a main-thread is present
    --->in the above case, msg_server.c is a multi-threaded
        application - however, we are using unconventional, 
        multi-threading - when we study conventional,     
        multi-threading, we will understand the differences 
 

 
---->we will come-across similar services/apis, in 
     EOS/RTOS platforms, but there will be changes
     to apis, coding and rules..... 

---->first-break on 22.12.2021.....

--->we will resume at 3.00 pm on 30.05.2022...

---->shared-memory IPCs,  in OS platforms     
----->now, we are switching to shared-memory IPC
      mechanisms 
      ---->in this context, it will be process-
           specific ---> (a) 
--->refer to slides 11-13 of freehand_diagrams_21.12.2021.pdf - 
    also, read the comments 

      ---->still, most of the principles 
           apply to threads related 
           shared-memory IPC mechanisms as well
           --->set-up , APIs and programming will be 
               different, in different contexts ---> (b)
--->refer to slides 14-15 of freehand_diagrams_21.12.2021.pdf - 
    also, read the comments 

      ---->still, most of the principles of shared-memory  
           apply to embedded multi-tasking/
           concurrent programming related
           to shared-memory IPC mechanisms
             --->set-up, apis and programming will be 
                 different ----> (c)
--->refer to slide 5 of AzureRTOSWhitepaper.pdf 

      ---->many of these principles apply to 
           shared-memory IPC mechanisms, in 
           kernel-space coding ----> (d)
             ---->set-up, apis and programming will differ  
--->refer to slides 9-10 of freehand_diagrams_21.12.2021.pdf

============================================================
---->so far, we have covered the following 
     mechanisms:
     ---->Unix /Linux signals - refer to 
          8_signals.txt

     ---->in this document, we have covered 
          pipe 	IPC mechanisms - some more 
          programming is pending ?? 

     ---->we have also covered message queues, 
          in this document - also, certain 
          code-samples were discussed 

     ---->we will cover more and complete certain 
          IPC mechanisms
============================================================  
        
 - let us understand the "shared-memory IPC 
   mechanisms and their problems, 
   in OS-environments - there are different 
   forms of shared-memory scenarios  :
    -->refer to freehand_diagrams_21.12.2021.pdf  
     different forms of shared-memory

--->we will resume at 3 pm on 22.12.2021.....

--->we will resume at 2.20 pm on 08.07.2021...
 
--->in a typical, GPOS, we will come across 
    several different-contexts of shared-memory 
    scenarios and related problems :
     --->process-context is the current discussion
         --->in this document, process- 
             context related shared-memory 
             will be discussed 
              --->shared-memory segments 
              will be set-up between processes,
              using OS-services/system-call 
              APIs 
              --->however, these shared-memory, 
                  segments are resident, in 
                  user-space - code, as well as 
                  data segments can be shared - 
                  let us focus on certain, data-segments 
                  of processes, that are shared - what does
                  this mean ??
---->refer to slides 11-13 of 
     freehand_diagrams_21.12.2021.pdf 
     --->interpret the comments, along with diagrams

 --->normally, code-segments of processes
     are shared-segments among certain, 
     processes, using a common-application's code-segments/
     common-libraries' code-segments - 
     this was discussed earlier, 
     as well - however, this does not 
     provide shared, data-segments 
    --->what does this mean??
        --->just shared-code/shared, virtual-segments
        --->however, these are RO/read-only segments, which 
            cannot be used to exchange-data 

 
 --->normally, data-segments of process 
     address-spaces are private, virtual-segments
    --->typically, private-VASes of processes
    --->what is the meaning of private, 
        virtual data-segments of processes ??
        --->private, virtual-segments/pages
            of processes will have their 
            own, private physical-memory 
            mappings
  ---->refer to 4_introduction_mm_2.txt and
       5_vmm_2.txt, for more specific details  

--->refer to freehand_diagrams_21.12.2021.pdf/slides 11-13
 --->so, what is the meaning of shared, virtual-  
     segments, in the context of processes
     and their address-spaces ??
   --->so, effectively, a shared, virtual-segment/sdsi of
       Pi and a shared, virtual-segment/sdsj of Pj 
       are effectively mapped to a common- 
       set of page-frames, using respective 
       page-tables/ptes 
---->based on all the above details and illustrations, 
     are you able to visualize the set-up of sdsi/
     a shared, virtual-segment of Pi and sdsj,a 
     shared, virtual-segment of Pj ??
     --->as per our virtual address-space/memory-mgmt. study, 
         typically, there are private, virtual-segments, 
         in processes
     --->for these private, virtual-segments, there are
         private mappings/ptes/private page-frames managed, 
         in 
         system-space

     --->similarly, there are shared, virtual-segments, 
         in processes - they are separate, virtual-segments, 
         in different processes' VASes
     --->in addition, there are private page-tables/ptes
     --->however, these private page-tables/ptes of 
         shared, virtual-segments' virtual-pages are 
         mapped to a 
         common-set of page-frames
   ----->refer to a free-hand /slides 11-13 illustration again 

=============================================================    
--->certain free-hand diagrams/slides 14-15 ?? are 
    provided  
     --->threads'  contexts
         --->in this context, let us assume, 
             that there are multiple threads
             of execution, in a processi/active
             applicationi - this is common, in 
             GPOS systems, as well EOSes 
        ----->typically, there is single 
              address-spacei, for each processi and
              an active applicationi - inside this
              process/active application, multiple
              threads will be using/sharing a 
              common address-space
        ---->effectively, threads will share data-segments of
             processi
        ---->refer to a free-hand diagram /slides 14/15 
        ---->we will see more details, in a
             multi-threading document      
 ======================================================================
     --->system-space contexts
              ---->we will discuss this, in kernel-space
                   scenarios....this is discussed, in 
                   another document...
        ---->refer to the first, shared-data 
             free-hand illustration/slide 11 and comments

     --->embedded OS/RTOS contexts
       ---->the set-up of address-spaces is very 
            different, in a typical RTOS/eos, like 
            a single address-space only, or some other
            special address-space - in such cases, 
            there will be certain, form of shared-data
            segments
       ---->we will see these details, in another,
            set of documents  

--->when we move from one context to another
    context, shared-memory/shared-data 
    principles are
    the same, but the actual set-up will 
    differ - due to changes, in a set-up, concurrent- 
    programming/APIs and related issues will differ
    --->there are well-defined coding-models
        and rules, for different scenarios
   
--->shared-memory/shared-data problems are 
    one of the 
    common-set of memory-problems, in 
    real-world applications, in different 
    platforms... 
  -->shared-memory/shared-data problems are popular, in 
     embedded/RTOS/application scenarios 

     - in the current-case, we will deal, with GPOS 
       platforms and mostly, in "user-space"
     - in the future, we will also deal, with 
       "system-space" and "embedded scenarios/hybrid-scenarios"  

      --->the current set-up and 
          discussions are , for processes and
          user-space, shared-data segments
     --->we may not highlight application-perspectives, 
         in this context, but it is easier to set-up, 
         experiment, and understand 

-->refer to free-hand diagrams/slides 11-13, 
   before continuing 
   the discussions
 
     - let us "understand explicit shared 
       memory regions' IPC set-up" between processes:
       - two or more processes create 
         "shared-memory, virtual-segments" - 
          using system-call APIs/services
--->since shared-memory/shared-data segments
    are set-up, using system-call APIs, 
    it is an explicit set-up.... 


---->for any processi, 
     code/data/heap/stack segments are typically 
     set-up, by the process memory management, in 
     the background
---->however, additional segments, like shared-memory 
     virtual segments require system call APis to be
     explicitly invoked, in our code   
---->towards the end of this document, we will be
     referring to system call APIs/code samples related to 
     explicit shared-memory set-up 
     --->in the given code example below, there is a very 
         good application perspective and usage...

       - each of these "virtual segments 
         maintain their own shared 
         virtual pages, in respective
         processes/pds/address-spaces" - 
         these virtual segments/ 
         pages are private to each process 
          --->shared memory segment/pages 
              are actually private to a process 
       - "these virtual pages of shared segments
         of these processes are forced to 
         map their ptes to the same set of 
         page-frames", in this IPC set-up - 
         there is an explicit set-up, using 
         system call APis and system objects  

       - effectively, the mappings of virtual 
         pages of virtual segments of 
         different processes are shared
       
       - so, if any virtual-address/virtual-page of any 
         shared-segment is accessed, in any 
         of the sharing-processes, they map/point to the 
         same page-frame/shared physical 
         memory 
             -->effectively, shared-data 
                is accessed, concurrently, in 
                different processes    

--->now, let us understand shared-memory related
    problems - these are common in any set-up/
    platform 
 
--->if shared-data  stored, in a shared-memory 
    region/segment/page, is accessed concurrently, 
    there are problems and related issues - this is true, in 
    any context/scenario/platform discussed above 
    --->why ?? keep reading and you will 
                find answers ?? 
    --->most problems occur due to concurrent- 
        programming and shared-data access
        --->this combination is a problem ??

    --->for our convenience, we will be using 
        user-space, shared-data segements of processes,
        to experiment and understand...
        ---->for, initial learning, but can be extended 

--->if shared-data stored, in a shared-memory 
    region/segment/page is accessed sequentially, 
    by the processes, there are no issues/problems
      ---->keep reading and you will find answers
    --->most problems do not occur due to sequential 
        programming and shared-data access
    --->however, most problems occur, due to 
        concurrent/parallel access of shared-data 
        segments

--->refer to slides of chap6.pdf       
---->refer to freehand_diagrams_21.12.2021.pdf/16-25, again ??

=============================================================
  - such shared-memory access/shared-data
    "access does not 
     require system call APIs" and "switching 
     to system-space" - it is as good, as 
     pointers/virtual address access-  
     in addition,there is "no
     need to copy data between user-space and 
     system-space" - so, such shared memory 
     data, in shared-memory segments/regions 
     are more efficient, for 
     data-exchange between processes, than 
     typical IPC mechanisms, like message queues
     and pipes - however, we need to use 
     appropriate IPC mechanisms, for 
     different scenarios  - we cannot always 
     use a typical, shared-memory IPC mechanism, 
     as it may not suit a specific scenario, 
     in the application 
  
        - in this context, processes can access
          shared-data, in these shared-memory 
          regions, using virtual addresses/pointers, 
          not system call APIs 
           -->for conventional IPCs, system call 
              APIs are needed and data-exchange
              is needed between user-space 
              and system-space - in the 
              context of GPOS, it is typically, 
              message queue /pipe IPC mechanism
           -->in the case of shared-memory 
              /shared-data IPC, no system call 
              APIs and no exchange of data 
              between user-space and system-space   
  --->any form of shared-data/memory can be 
      efficiently accessed, with very less 
      overheads - that is why it is popular, in 
      every platform...         
        - to set-up a shared memory region,we need
          to use certain system call APIs
            -->these system-call APIs will be 
               discussed, during programming/
               code-samples below... 
       
--->since the basic set-up of shared-memory , 
    segments, is clear,let 
    us understand the different problems and 
    related solutions ??

---->based on the above discussions, there will be 
     some form of  "IPC mechanism and related shared-data 
     problems"  and issues - some of these shared-data 
     problems are, in user-space and some are , in 
     system-space, and some are, in other platforms... 
===========================================================
 
--->following discussions describe shared-data
    concurrent-problems and meaningful, solutions
    ---->most of these scenarios and problems are
         well understood and bullet-proof  
         solutions/models are already provided
         --->however, we still need to understand
             our problems/scenarios and apply these 
             models/solutions and test/verify, 
             as per the platforms and applications.... 

--->let us see a freehand_diagram_21.12.2021.pdf/
    slides 16-21...
    --->first-set of illustrations highlight 
        critical-sections of processes, in a multiprocessor 
        system...
  ---->critical-sections are code blocks, in 
       processes(threads), which access shared-data
       , concurrently  
        - let us assume, that there is a shared-memory
          region set-up, for two or more processes(
          threads/other scenarios) - 
          in this set-up, let us assume that 
          there are CS11 and CS12 of Pi and Pj, which are 
          critical (code) sections, in the two-processes, 
          that access shared-data, in a shared,
          memory-region/shared-segment - 
          let assume, that these 
          two critical (code) sections can be executed
          , concurrently/parallely - let us also assume, that 
          the operations/instructions of CS11 and 
          CS12 are read-modify-update instructions
--->all these assumptions are based on common-scenarios

--->again, refer to free-hand diagrams/slides 16-21...
      ---->these slides illustrate a multiprocessor 
           scenario...       
      ---->let us understand the scenarios and 
           related problems ??
---->resume on 23.12.2021....at 10.30 am....
 

--->third-break on 22.12.2021....

--->second break on 08.07.2021...
 
--->also refer to one more slide of chapter 6, chap6.pdf/
    of Charles crowley - it illustrates multi-processor
    scenario only / slides 9-10
    ---->only multi-processor scenarios are
         explained
    ---->such scenarios are known as race 
         conditions, in critical code sections
         of processes
         --->what do you understand from these 
             race-conditions ??
 
 ---->many of the freehand_diagrams_21.12.2021.pdf are influenced
      by these slides 9/10 of Charles crowley reference
       --->analyze, interpret and also, read the 
           comments  
  
--->read-modify-update operations, in critical-
    sections' code blocks cause certain, problems      
          -->in these discusssions, we will be 
             assuming a single, read-modify-update 
             operation, in each critical-section of 
             a processi
          -->in reality, each critical-section 
             will be having "multiple, read-modify-
             update operations" - however, the basic
             s remain the same - we need to extend 
             our understanding - instead of dealing
             with a single, shared-data element, 
             in a critical-section, we will be 
             dealing, with multiple, shared-data 
             elements, in a real-world, critical-section -
             there will more problems - the degree
             of problem will be higher
--->what does this mean practically ??
      --->in this context, we are providing an 
          OS-perspective answer:
          --->if we access several data-elements, or 
              a composite data-element, in a
              shared-data segment between processes
          --->due to this, there will be more 
              read-modify-update operations, in 
              each critical-section of a processi
             --->visualize and connect the details 

--->along with the above chap6.pdf/slides9-10 and 
    freehand_diagrams_21.12.2021.pdf /slides 16-21 ??
    , please keep reading and interpreting the 
      following : 
          -->certain critical-sections are shorter
             and others,  longer, but the problems
             remain the same
          -->in this context, what happens, if 
             read1-modify1-update1 operations of 
             CS11 and read2-modify2-update2 operations
             of CS12 are not interleaved 
             -->what happens, if read1-modify1-update1
                is executed and followed by 
                read2-modify2-update2 is executed, in 
                multiple, concurrent-processes ?
                --->operations are consistent 
                --->results are consistent 
---->CS11 ---> i++(i is a variable, in shared-data) 
---->CS12 ---> i--(i is a variable, in shared-data) 

             -->what happens, if read2-modify2-update2
                is executed and followed by 
                read1-modify1-update1 is executed ?
                --->operations are consistent 
                --->results are consistent 
                
--->read the following discussions and connect to 
    the above details :
   
          -->in this context, what happens, if 
             read1-modify1-update1 operations of 
             CS11 and read2-modify2-update2 operations
             of CS12 are interleaved,due to  
             concurrency/preemptions(uni-procesor
             scenarios) or parallelism(multi-processor
             scenarios) 

  --->meaning, what happens, if read1-modify1-read2-
      modify2-update2-update1 is executed,due to 
      concurrency/parallelism and interleaving ??
        --->this could be due to preemptions or
            parallel-executions
        --->are these operations consistent ?
         -->will the result of these operations
            be consistent ?? NO, final result 
             will be inconsistent - this is 
             unacceptable, in the real-world 
             applications 
  

  --->meaning, what happens, if read2-modify2-read1-
      modify1-update1-update2 is executed,due to 
      concurrency and interleaving ??
        --->see the above scenarios and explanations
---->resume here on 09.07.2021... at 10 am.

--->for the above scenarios, a sample-code is being 
    used to provide more explanations and illustrations - 
    so, continue reading and understand micro-level 
    details.....

---->refer to race_updated.c, which explains a scenario, 
     for shared-data segments, between processes
---->let us have a simple demo.
---->by running a simple demo. of race_updated.c/race
     on a multi-processor system(on multiple
     cores), we are illustrating 
     concurrent /parallel access of shared-data, 
     in a shared-data segment ??
     --->when we run several instances of race.c/race
         , we can see inconsistent final values of 
         the common shared-data,after concurrent 
         manipulations ??
     --->there are certain consistent results and 
         many inconsistent results ?? 
     ---->why is it so ??
      ---->similar to our hypothesis and visualization, 
           certain concurrent execution scenarios 
           provide non-interleaving/non-overlapping 
           concurrent 
           executions of critical sections
             --->there are consistent results 

      ---->similar to our hypothesis and visualization, 
           certain concurrent execution scenarios 
           provide interleaving/overlapping 
           concurrent 
           executions of critical sections
             --->inconsistent results 

--->also, refer to chapter 9  in Linux kernel Development, 3rd edition,
    Oreilly publications  

--->we will also, see a demo. of race_updated.c , in 
    an uni-processor scenario,as well as 
    multi-procsessor scenarios ??

--->we will resume on 31.05.2022...

--->refer to freehand_diagrams_21.12.2022.pdf/slides 22-23
----->Uni-processor discussions 
    -->in the following context, we are referring to 
       to uniprocessor/concurrency/       
       user-space preemptions, in a shared-data 
       set-up ----> we can visualize any of the 
       above examples and illustrations.....

          -->let us assume the following :
            CS11 of Pi   
             -->read1  -> load i's value into a cpu-reg/read
             -->modify1-> cpu reg++/modify
             -->update1-> store cpu-reg value into i/update 
       
            CS12 of Pj
             -->read2  -> load i's into a cpu reg/read 
             -->modify2-> cpu-reg--/modify
             -->update2-> store cpu-reg value into i/update 
        
---->let us refer to freehand_diagrams_21.12.2021.pdf
     /slides 22-24 
 
------>scenario1 :
        
       --->Pi and Pj execute their critical 
           sections, without user-space 
           preemption, in uni-processor systems
----->refer to a free-hand diagram /slides 22-24 ?? 

           --->u1,r1,m1, u2,r2,m2 are 
               executed, in this order, 
               without interleaving
           -->in this scenario, the final 
              result will be consistent 
              and correct 
           -->in this scenario, each critical 
              code block/section and its operations 
              are executed, atomically
           -->there is no interleaving of 
              operations of code-blocks of 
              different critical-sections 
   --->scenario1 is acceptable, in uni-processor
       systems - for multi-processor systems, 
       we need to understand more - you can 
       see other discussions above  

 ----->scenario2 :  

       --->Pi and Pj execute their critical 
           sections, with user-space 
           preemptions 
           
           --->r1(read into a cpu-reg),m1(increment this cpu-reg),
               r2(read into a cpu-reg, again),m2(increment this 
               cpu-reg),u2(store cpu-reg into i),u1(store cpu-reg into
               i) are 
               executed, in this order, 
               with interleaving 
          --->after m1, there is a preemption of 
              Pi - as part of preemption, current
              hw-context is saved, for Pi - along with 
              hw-context, cpu-reg/modified-value are
              saved, into a save-area of Pi
          --->so, r2 will read the older value from 
              i into cpu-reg - m2 will update the 
              value, in cpu-reg - u2 will update 
              this cpu-reg value into i 
          --->when Pi resumes, again, its saved 
              hw-context will be loaded into 
              the cpu-registers, including 
              saved cpu-reg/modified-value 
          --->now, u1 will take this cpu-reg/
              modified-value and store it into 
              i , again - however, this is 
              the older, modified-value ....    

           -->in this scenario, the final 
              result will be inconsistent 
              and incorrect 
           --->in this scenario, operations 
               of code-blocks of critical 
               sections are interleaved 
               --->this is a non-atomic 
                   execution of critical-sections
                   and causes inconsistency 
                   and incorrect-results
       --->cause of interleaving , is OS-concurrency 
       --->cause of inconsistency, is OS-concurrency 
---->ultimately, application is affected 


---->refer to the free-hand diagram/slides 24/25  
          --->in this case, let us assume, 
              that the initial value of 
              a shared data variable is 
              ?? - first process will 
              increment its value to ??, 
              but before it updates the 
              value to the memory location, 
              in the shared-data, it is 
              preempted and the second 
              process will execute its 
              critical section code block -
              this will still see the old 
              shared data and read ?? - 
              after reading ??, will 
              decrement the value to ??
              and update the shared data 
              to ??
              --->the preempted-process 
              will be resumed, in the future
              and it will use the saved, modified
              value, from its hw-context 
              --->next, this modified-value 
              will be stored into the shared- 
              memory variable - it will be 
              ?? - the final-result is wrong/
              inconsistent 
---->in the above case, the high-level issues are connected to  
     the low-level, context saving/switching/restoring 
---->such visualization helps, in understanding "race-conditions", 
     in uni-processor scenarios  
  
---->what is the meaning of atomic-execution 
     of a critical, code-sections,CS11 or CS12, in these 
     scenarios ??
     ---->if there are two or more related 
          "critical, code-sections" executed, concurrently, 
          without overlapping their 
          instructions, it is said,that the
          related critical, code-sections
          are executed, atomically - in these cases, 
          critical,code-sections are said to be atomically, 
          executed   
--->can you provide a similar definition, for 
    non-atomic executions of related critical, 
    code-sections ??
     --->for the same scenario, when instructions 
         of concurrent, critical-sections overlap, 
         we say, that the related critical- 
         sections  are executed, non-atomically 

---->like mentioned before, many of these basic issues 
     are common, across platforms and contexts

========================================================= 
---->now, refer to Linux kernel development, 
     3rd edition, chapter 9/section is atomic 
     operations, 
     which explains similar critical-sections, 
     in KEPs, in kernel-space  ?? in this reference, 
     most of the issues are related to kernel-code/
     kernel-space, but basics remain the same
---->otherwise, most scenarios are thread based, 
     not process based.. 

========================================================

------>there are similar scenarios, that you 
       can visualize  
------>based on the above discussions/scenarios, 
       critical-sections and atomic-cenarios 
       are acceptable, but critical-sections 
       and non-atomic scenarios are 
       unacceptable  
  --->following discussions are, for multiprocessor
      systems and related scenarios
---->refer to "chapter 6 of your text book, 
      parallel processing" 
---->before we go futher, let us understand 
     that concurrent and non-overlapping scenarios, 
     in multi-processor systems do not interleave
     operations of critical code sections of 
     processes --->there will be no inconsistency 
                   problems, in these contexts
 
 ---->before we go futher, let us understand 
     that concurrent and overlapping scenarios, 
     in multi-processor systems do  interleave
     operations of critical code sections of 
     processes  --->there will be consistency 
                    problems, in these contexts

----->a detailed explanation of critical-sections and 
      atomic/non-atomic executions 
         --> let us assume the following sequence
             of events, in a multiprocessor 
             system, with Pi on cpu 0 and 
             Pj on cpu 1:

              -->read1(into cpu-reg of cpu0)    at ti,cpu0
              -->modify1(increment cpu-reg of cpu0)  at ti',cpu0
              -->read2(into cpu-reg of cpu1)    at ti'',cpu1
              -->modify2(increment cpu-reg of cpu1)  at ti''',cpu1
              -->update1(store value of cpu-reg of cpu0 into i)  at ti'''',cpu0
              -->update2(store value of cpu-reg of cpu1 into i)  at ti''''',cpu1 
              -->in this context, initial value 
                 of i is 100 - follow the 
                 concurrent operations, as below:
              ---->effectively, read1 will see 
                   100, as per the lecture diagram 
              --->read2 will also see 100, as 
                  per the lecture diagram 
              --->update1 will update i to 101
              --->update2 will also update i to 
                  99  
              --->in this case, the expected
                  correct value, after the 
                  operations is 100, but the 
                  actual value is 99, which 
                  is incorrect/inconsistent           
   --->irrespective of scenarios, the basic
       problems of concurrency/parallelism 
       are the same       
          ---> in the case of multiprocessor
               scenario, instructions/operations
               of CS11 and CS12 are said to be 
               "interleaved/non-atomic" and this 
               leads to inconsistency, in results
                -->this is due to shared-data, 
                   parallelism  and non-atomic
                   operations, which lead to 
                   inconsistent results - 
                   unacceptable - non-atomic 
                   operations are due to 
                   race-conditions, due to 
                   concurrent executions/parallelism, 
                   in multiprocessor systems
 --->in the text book, uniprocessor scenarios 
     may not be provided     
   -->uniprocessor scenarios are described 
          above 
 
          -->refer to chapter 6/Crowley, for 
             a diagram on parallel execution 
             of critical sections and how 
             interleaving of operations/
             instructions are done - this 
             describes the multi-processor
             scenarios - "in these scenarios, 
             "race conditions" are possible due 
             to unpredictability of concurrency 
             , along with shared-data access" -
             due to all these, there will inconsistencies,                in final results of operations
                 --->chapter 6 does not discuss 
                     certain uniprocessor 
                     scenarios


          ---> what happens, if the above processes
               and their operations/instructions 
           are executed concurrently/non-atomically,  
               in an uniprocessor scenario ??
               -->refer to the critical sections 
                  of P1 and P2, as described above
               -->what are the possible conditions/
                events, that may lead to interleaving
               of instructions/operations of 
              critical sections,in user-space ??
                  
           --->typically, following should be 
               a scenario, for a race-condition
             ,due to preemption, in uniprocessor:

                        -->read1    at ti
              		-->modify1  at ti'
            --->there is an user-space preemption
                       due to timer/time-slice
                        or due to IO interrupt 
                       and priority issues
           --->P1 is preempted 
              		-->read2    at ti''
              		-->modify2  at ti'''
              		-->update2  at ti''''
                        .....
                        .....
            P1 is scheduled/dispatched, 
            again, in the future
                        .....
       	-->update1  at ti'''''
               -->in the above scenario, there
            is a condition of user-space preemption
             and due to this, there is a
            a race-condition/an unpredictable 
            condition  - eventually, 
            there is inconsistency 
            of operations/results - this is 
          unacceptable - this is "a non-atomic
          operation"  

=========================================================================
   -->similar concurrency, problems occur, in system-space
                      and embedded-scenarios
      --->in the context of embedded, resource-sharing 
          will be also covered, in addition to data-sharing 
          issues
          --->appropriate solutions are recommended, 
              for embedded-scenarios...
              --->application-designs are provided
              --->we need to understand and implement 
                  coding, in our applications... 
===========================================================================

---->certain conclusions 
--->in all the above illustrations, there are scenarios, 
    where cs11 and cs12 are competing/racing, due to 
    concurrenct-executions, preemptions, and parallel-executions
--->these scenarios are known as race-conditions 
       -->irrespective of uniprocessor or 
          multiprocessor platforms/scenarios, 
         there will be "race-conditions"/"unpredictable 
         race-conditions" - in these cases, order
         of executions of processes/critical 
         sections' code will differ, due to uni-processor
         scheduling/multi-processor scheduling/preemptions 
         and other OS mechanisms - 
         "race conditions lead to interleaving/overlapping
          of operations/instructions" of 
         "related critical sections" - eventually, 
        results are inconsistent - unacceptable
        -->so, the critical-sections are 
         executed non-atomically 
---->race-conditions are due to unpredictable 
     concurrent-executions, in these systems 
         --->in all the above cases, we say that 
             the code-paths of critical-sections 
             are not executed, atomically 
              -->non-atomic executions
                -->analyze these scenarios 
----->lunch break on 23.12.2021....


       -->following demos. will illustrate 
          race-conditions, 
          in multiprocessor and uniprocessor systems:
              --->refer to race_updated.c, which uses 
                  a parent-child process set-up 
                  and shared-data access -  
                  we can test this demo application
                 , in uniprocessor and multiprocessor
                 systems
---->let us repeat certain multi-processor and uni-processor
     demos., using race_updated.c
     --->if you try race_updated.c/race_u, using 
         multiple processors, you will see frequent, 
         inconsistencies, in final-results ??
     --->if you try race_updated.c/race_u, using 
         a single processor, you will seldom, see
         inconsistencies
         --->however, it may occur rarely, but it 
             will occur
    ---->we may increase the length of critical 
         sections, by adding more shared-data 
         variables or increase the count of 
         iterations of operations on shared-data 
         variables - you will come across many of 
         these scenarios, in assignment-4/problem 2  


---->system call APIs and related coding will be 
     discussed, at the end of this document -
     we can read this , along with system call APIs 
      --->many of these points will be discussed, 
          along with system call APIs of shared-memory  
             -->in the race_updated.c, a shared-memory 
                region is set-up, in the parent 
                process - in addition, an object 
                is placed, in the shared-memory 
                region/shared-memory segment
                 -->in addition,a fork() is invoked, 
                    in the parent process to create
                    a duplicate child process
                 -->as part of this duplication, 
                    private-data virtual segments 
                    are duplicated and their 
                    respective memory blocks 
                    are duplicated - which means, 
                    mappings are private  
                 -->as part  of this duplication, 
                    shared-data virtual segments 
                    are duplicated, as well, but  
                    respective mappings/memory 
                    blocks are shared between 
                    parent and child - this means, 
                    shared virtual segments are
                    duplicated, but effectively 
                    they map to the same set of 
                    physical memory blocks/
                    page-frames - so, these shared-data,
                    virtual segments not be private 
                    
                 
                 -->load/execute race_updated.c on 
                    multi-processor systems 
                    and un-iprocessor systems 
                
                 -->ideally, we will see 
                    race-conditions, in both
                    the above scenarios and 
                    this will lead to inconsistent 
                    results
---->in multi-processor systems, race-conditions
     occur more often and due to this, we may see
     inconsistent results, often 
---->in uni-processor systems, preemptions are
     needed, for different race-conditions, but 
     we may not experience enough preemptions, 
     during the execution of race_updated.c/race_u and 
     hence, we may not see inconsistent results  
     --->however, we must conclude, that currently 
         we are unable to see race-conditions and 
         inconsistences, in results
     ---->however, if we increase the length of 
          critical, code-sections, there will be 
          more preemptions, and eventually, there
          will be more race-conditions, so 
          inconsistencies  
     --->in addition, if the load-conditions/ 
         scheduling change, we will encounter 
         preemptions/race-conditions and 
         inconsistencies, in the results  
 

       --> possible causes/reasons, 
               for race-conditions :

       -->preemptions, in user-space/system-space
       -->parallel executions of processes/
          threads, in multiprocessor 
          systems
  --->the above problems due to concurrency 
      are known as "shared-data, race-conditions"/
      "data-race, conditions"

========================================================
---->some of the following scenarios will be 
     practically enabled, in EOS/RTOS platforms 
----->some these scenarios will be discussed, 
      in embedded OS/RTOS scenarios - these
      will involve hw peripherals or shared
      hw buses, along with multi-tasking/
      concurrency 
  --->in addition, in embedded contexts, we 
      will encounter other forms of race, 
      due to shared resources, like hw buses
      and peripherals  
       --->if a shared hw bus is used to 
           access multiple sensors, the 
           jobs must not be interleaved - 
           meaning, one job/task must access
           the specific busi/sensorj,complete and 
           next, access another specific busi/sensorj+1
       -->if a shared peripheral is being used, 
          first complete a job/task on th e
          peripheral, before starting another 
          job/peripheral in the peripheral - 
          this will avoid interleaving of
          data to the peripheral    
 
       --->basic principles remain the same, 
           but the context and the embedded 
           issues are different 
       --->such scenarios are known as 
           shared resource race-conditions/
           "resource-race conditions"
       --->embedded context will also have 
           "data-race" 
---->along with these discussions, we will also 
     see appropriate locking mechanisms and their
     programming techniques  
============================================================

---->first break on 09.07.2021

---->most of these topics come under a broader topic, 
     known as OS-synchronization - more specifically, 
     OS-synchronization 

==================================================================
---->for instance, you can refer to certain embedded
     documents, like DHT0008A_arm_synchronization_primitives.pdf
     , which is from ARM
        ---->this document will discuss only from processor/
             hw perspective, but will provide you a different 
             perspective - will not cover much of OS issues
        ---->some of the definitions, in this reference may 
             conflict, with actual OS definitions - so, you 
             must take the contents, accordingly, with a bit 
             of caution 
        --->read this document, after completing the current 
            txt document of IPCs ... 
=================================================================== 

---->when we connect OS-synchronization and embedded-scenarios,
     we must give preference to OS-definitions and OS-terminology  

    --> for all such, concurrent, data-race/
        resource-race(embedded) problems, we need to use
        "some form of locking-mechanism/synchronization-mechanism", 
        which is "invariably, OS-supported"

---->terms like OS-supported,locking-mechanisms and synchronization- 
     mechanisms are related 

         --> most of "these OS locking-mechanisms/locks and
             synchronization-mechanisms
             are implemented,
             by "OS/core,using OS-techniques/
             mechanisms", along with 
             "hw/embedded/low-level, mechanisms" 
---->in most cases, sw techniques/mechanisms of OS + 
                    hw techniques/mechanisms of procesor, 
                    are used   

        --> critical-sections/race-conditions/ 
            ,along with "OS-supported locks" can lead to 
            run-time overheads and this may lead
            to poor, timing-performance, for 
            certain, applications - we need to be aware 
            of these issues - we need to test 
            performance issues, as per context  
             -->eventually, we need to use locking, 
                as per requirements and 
                test the applications, for
                functional and performance issues - we need to 
                test the performance of applications, 
                after adding locks/protection 
                to critical sections - check, if 
                the performance is acceptable
---->in the current discussions, we will mostly, focus 
     on functional-correctness - in the future discussions, 
     we need to focus on , both functional and performance 
     correctness issues, in EOS/RTOS-platforms ... 

         --> we may face critical-sections/
             race-conditions, "in user-space" or 
            "system-space", or "embedded" scenarios
                  - basic principles are the same,
                    but there will practical 
                    differences, so 
                    but implementation details of 
                    locks may differ/usage of the 
                    locks differ - "accordingly, 
                    we need to use lock-services 
                    of a given OS-platform, as per applications' 
                    requirements"  

---->before all these application-design issues, we need to 
     understand OS-locks and their characteristics 
 
  --->we need to understand different forms of 
      OS-locks and used them appropriately, as per the 
      contexts       

     --> we will be using "OS supported
         locks/OS-aware-locks", for locking / unlocking/
         protecting critical sections
        , such that there are no 
         race-conditions - we need to test, 
        after using locks to protect 
        critical sections of our concurrent 
        application code blocks 
          --> once we use locks to manage 
              critical sections, critical 
              sections are executed atomically
               with respect to each other - 
           due to the locks, operations/instructions
             of critical sections are 
             not interleaved, with other critical 
             sections -  
             will be executed atomically
          --->if there are two related critical sections, 
              in two processes/threads, these will be 
              atomically executed, if an OS lock is used
              to protect the critical sections
          --->the above applies to two or more related 
              critical sections, in two or more processes, or
              threads

---->we need to understand locks and how these
     locks solve many of these race-conditions 

--->follow the discussions, below and we will 
    justify usage of locks and their operations

============================================================
        -->in the context of embedded, apart from 
           shared-data problems, there will be 
           shared-resource/hw problems, which 
           need to be solved, using appropriate 
           locking/protection mechanisms - this 
           protection is not same, as memory protection  
                  -->it may be a shared-IO-bus 
                     problem - the bus must be 
                     accessed atomically, for 
                     specific IO transfer, without 
                     interference from another 
                     IO transfer
                  -->it  may be a shared-peripheral 
                     problem   
  --->we will study and understand different 
      embedded scenarios, in RTOS/EOS  context
=============================================================

--->in the context of OS-aware-locks, we need to first, 
    study and understand semaphores - these are
    basic, locking-mechanisms ??

----->let us start conventionally, with semaphores and 
      their working - initially, let us not be biased 
      towards applications/scenarios 
 
  --->initially, let us understand an "OS supported
      mechanism,like semaphore mechanism", 
      which can be used, as a form of 
      lock - this enables us to understand 
      most, basic-principles of an OS-supported 
      lock
---->conventionally, we must study and understand
     semaphores, first - many other IPC synchronization mechanisms/
     locks are based on the principles of 
     semaphores - based on semaphores, other mechanisms
     were derived 

---->we can use a semaphore-mechanism, for 
     different requirements, other than locking 
     --->in this context, we will be understanding
         a semaphore-mechanism , for locking 
         related critical-sections, accessing 
         shared-data - a semaphore, as a lock  
---->let us understand semaphore set-up and 
     its standard-operations - once these are
     clear, we will understand the usage of 
     a semaphore, as a critical-section lock  

---->let us run a few demos., using race_updated_sem.c /
                                    race_u_s
---->before we jump into the discussions, let us 
     illustrate usage of a semaphore lock, for 
     related critical sections, in "race_updated.c" 
     --->refer to the modified code,"race_updated_sem.c"
      --->gcc race_updated_sem.c -o race_u_s 
        --->run race_u(without semaphores) on 
            a multi-processor set-up and check 
            the results ?? 
        --->run race_u_s(with semaphores) on 
            a multi-processor set-up and check 
            the results ?? 
        --->run race_u(without semaphores) on 
            a uni-processor set-up and check 
            the results ?? 
        --->run race_u_s(with semaphores) on 
            a multi-processor set-up and check 
            the results ?? 

 -->let us understand the set-up and  operations
    of semaphores :
---->refer to a freehand_diagrams_21.12.2021.pdf / slide 43 and 
                                                   slides 26/27
     --->specifically, slide-43 :
         --->there is a dec() operation on a semaphorei, 
             in Pi, before a critical-section, CS11 is
             entered/processed
         --->there is an inc() operation on  a semaphorei,
             in Pi, after a critical-section, CS11 is
             completed
         --->there is a dec() operation on a semaphorei, 
             in Pi+1, before a critical-section, CS12 is
             entered/processed
         --->there is an inc() operation on  a semaphorei,
             in Pi+1, after critical-section, CS12 is 
             completed 
         
---->the above coding-model/programming-model was 
     provided, by OS-developers - we need to understand
     , modify and use, as per our requirements ??

     ---->read the comments
     --->shared-data is, in system-space
     --->critical code-sections are , in system-space 
     --->core of the OS ensures, that these critical sections
         are atomic....



---->refer to a freehand_diagrams_21.12.2021.pdf / slide 43 and 
                                                   slides 26/27
     --->specifically, slide-43 :
--->similar to pd, most of the OS services/mechanisms
    are represented, using system-objects, in 
    system-space - like pipe-object or message-queue
     object or semaphore-object, or some, other object ?? 
           -->typically, each sempahore-instancei is represented
              and managed, using a semaphore-objecti,
              in system-space
               -->a semaphore-objecti is maintained, 
                  in system-space - it is a system- 
                  objecti - there are other, 
                  possible designs/implementations,
                  in other, OS-platforms - possibly, 
                  the structure of the object and 
                  other details may differ  - something 
                  similar will be present - refer to 
                  respective, OS manuals.. 
           -->in "a sempaphore-objecti", there is a 
              semaphore-variablei/it is a counteri - it is a 
              counter-variable - there
              are well defined rules, for this 
              special-counteri, in a semaphore-objecti - 
              a system-managed, 
              counter - this "special-counteri
              variable is, known as a semaphorei" - 
              however, a semaphore-variablei/semaphorei requires
              a semaphore-objecti to manage the state and 
              other operations - there has to be a 
              "frame-work"  - there has to be a "set-up" 
 
          --->semaphorei is a system-variable/
              system-counteri  
              maintained, 
              in the system-space,in a system 
              objecti  - this is a design  
------->refer to free-hand diagrams of a semaphore
        object/a semaphore 26/27  also, refer to the 
        comments below free-hand diagrams 
 ===============================================================
          -->such semaphore-objects are typically, 
             maintained, in system modules/components
             of the core/kernel of the OS - this will
             be part of an IPC-module of the core
             of the OS - there will "a static 
             kernel-module", for this set-up - this 
             kernel-module will contain semaphore-objects
             and service-routines -  
             some of these points will be cleared, during 
             Linux kernels-space and RTOS discussions..
==============================================================      
-------->also, refer to your text-book, for 
         more details, in chapter-8 of Crowley 
         --->slide nos.36 - 44
    ---->we will refer to this set of slides, after
         some more understanding - refer to our 
         freehand diagrams and this text, which 
         explain the system-space and user-space
         parts of a semaphore-objecti/semaphorei 
         and operations/system-call APIs/service-routines 
 
--->following discussions will explain the operations
    on a semaphorei/semaphore-objecti and related 
    rules - typically, these semaphore-operations 
    are provided, as system-call APIs and service-routines.....
---->our third-break on 23.12.2021...

          -->there are "well-defined semaphore 
             operations", for 
             semaphores/counters and these are implemented, 
             using "system-call APIs", in GPOS- 
             "these system- 
             call APIs' service-routines are designed 
             and implemented,
             to be atomic-operations", in 
             OS-platforms - for now, let us 
             accept it ..... 
---->refer to free-hand diagram/slides 26/27
           -->these operations operate on a 
              shared, semaphore-objecti/semaphorei -  
              in addition, there can be concurrent-executions of 
              these system-call APIs/their operations - 
              still, they are atomically, executed 
           --->these are not part of application's code, 
               but system-code only 
           --->OS-designs and implementations 
               take care of such operations  

           --> for instance, if an 
             atomic, system-call APIi's service-routine is executing 
             on a semphore-objecti/semaphore-instancei,
             in the context of 
             a processi, another, system- 
             call APIi, or APIi+1's service-routine, 
             in the context of another, 
             processi+1,  is disallowed to access 
             the same semaphore-objecti/semaphore-instancei
             , concurrently -  
             this is a form of  
             atomicity, not absolute atomicity - 
             limited/relative, atomicity
---->the above atomicity of semaphore-operations/
     system-code is taken care, by the 
     OS, not application's code - such OS features 
     are taken care, in the core of an OS ...

---->what do you understand from the above statements ??
     --->the system-call APis' system-services routines will be 
         executed atomically, with the help of OS set-up
     --->which means, if we use semaphores' system-call 
         APIs, their executions are atomic, when 
         accessing shared, semaphore-objecti/its fields... 
  
--->if two or more processes are accessing a 
    semaphore-objecti/semaphore-instancei, using 
    system call APIs/service routines, is this 
    a case of shared-data and concurrency problem ??
    --->in this context, processes are executing 
        concurrently, in kernel, execution-paths/KEPs 
    --->in this context, these concurrent, kernel, 
        execution-paths will be accessing 
        a shared, system-object , like semaphore-objecti 
        and its fields
    --->we still have shared-data/race-conditions, 
        but these operations/service routines/code
        use "special, hw-supported locks/hw 
        instructions", for 
        implementing the actual, atomic-operations - 
        these are not user-space/application specific
        shared-data /shared-resource problems/issues 
         
    -->these system call APIs's service 
       routines  are implemented 
      ,using "sw techniques of OS" and "hw 
       mechanisms of processor/embedded" - 
       using OS sw techniques and 
       hw processor mechanisms, 
       atomic operations of system call APIs'
       service routines are implemented

---->initially, let us assume the above set-up
     --->it is a standard set-up   
---->initially, let us not dig into the hw-mechanisms/
     low-level, processor-details - instead, let us 
     understand the high-level operations of 
     system-call APIs/their service-routines of 
     semaphores, first   
--->we will see the low-level/micro-level 
    details of these service-routines, 
    after describing high-level, semaphore-operations/rules 

--->first break on 01.06.2022...
--->for now, let us understand high-level details of 
    semaphore-object/semaphore and its operations ??
-->following are the well-defined/strict 
             OS-rules, for the semaphorei/ counteri and
             its operations ,
             as per typical, design :
                -->this counter's value cannot 
                   be -ve, typically(there may be exceptions)
                --->a typical semaphore's value 
                   can be 0 or +1, or >1, not 
                   <0 (typically)   
                -->its value is typically, 0 or +1
                   (common-usage scenario )
                   --->this is true for 
                       a binary-semaphore
                -->other values, like >+1 are 
                   also possible, 
                   as per applications' requirements- 
                   --> a range of "0 -> a +ve value(>1)", 
                       is possible
                     --->this case is true for 
                         a counting-semaphore 
               --->initial assumption will be a binary-
                   semaphore with values, 0 and  1
                ->as per the application's requirements,
                  the developer must initialize the
                  semaphore/ counter value to an inital value- 
                  in our case, the initial value will
                  be typically, 1(other, +ve  values are also 
                  possible) - it certain cases, it can be 0  


->in our initial discussions, "we will be using a semaphore-
  objecti/semaphorei, as a semaphore-locki", so a binary-
  semaphore will be used  
                  
===============================================================
                  a semaphore 
                  instance can be
                  used, for other application requirements, 
                  as well -  we will be using it, for 
                  typical, synchronization of threads, or 
                  threads and ISRs, in embedded-
                  scenarios 
================================================================

-------------->   however, in the current use case,
                  a semaphore-lock/a binary-semaphore will 
                  used with an  
                  initial-value 
                  set to 1 and during run-time, its value
                  will not exceed 1 - it can be decremented to 
                  0,or incremented to 1, and repeated - 
                  in the current use-case of a
                  semaphore-lock/a binary-semaphore, 
                  the only, possible values are 0 or 1 - 
                  we will be, using a semaphore-instance
                  ,as a binary-semaphore, for locking 
                  shared-memory, critical-sections - 
                  we will be, using a binary-semaphore, 
                  as a lock, IPC-mechanism for 
                  critical-sections accessing a 
                  shared-data in different processes, 
                  concurrently 
--->refer to slide 43 of freehand_diagrams_21.12.2021.pdf

----->in short, the counter-value/semaphore-value, in 
      the case of a binary-semaphore is restricted to 
      0 or 1...so, basically, a counting-semaphore is 
      used, as a binary-semaphore...
============================================================== 
                -->in terms of usage, we may come across
                   binary-semaphores and counting
                   semaphores, but actually, there 
                   are only counting-semaphores, in 
                   GPOS -  counting-semaphores can 
                   be used, as per our requirements
==============================================================
===============================================================
---->if so, what is a binary semaphore, in a GPOS 
     system ?? we use a counting-semaphore, but 
     restrict its value to 0 or 1, using our 
     programming techniques - do not name it, as
     a mutex - mutex is a different mechanism - we will
     discuss mutex, in another document of multi-threading..  

        --->right now, let us just use a counting semaphore
            , as  a binary-semaphore, only 
               --> as of now, let us understand 
                   binary-semaphores, in the context 
                   of GPOS - later, we will understand
                   counting-semaphores, in the context 
                   of GPOS
--->in a typical, EOS/rtos, there are different 
    forms of semaphores and mutexes - after completing 
    these basics, you can easily connect to the 
    specific details ??

===============================================================
===============================================================  
               ---> eventually, we will understand 
                   different forms of semaphores, in 
                   the context of "RTOS/eos/embedded" - 
                   there are other IPC mechanisms,which 
                   are based on the basic principles 
                   of semaphores
                   --->these specialized platforms 
                       will implement specialized, 
                       semaphores / mutexes  
================================================================
                
--->now,let us understand a typical, working 
    of semaphore-mechanism: 
 
     --> as part of the life-cycle of a semaphorei/
         semaphore objecti, 
         let us understand the following operations/
         APIs :
---->for each of the operations, let us use a 
     typical freehand_diagrams_23.12.2021.pdf and 
     illustrations :
--->refer to slides 28-38
             - creation and initialization,using 
               system call APIs
              --->if we invoke a specific, system-call 
                  API, a semaphore-objecti/semaphore 
                  instancei will be set-up - an object 
                  is set-up, in system-space 
                -->a system object will be created
                   and "related idi", or "handle" will 
                   be returned to the user-space, 
                   application-code  - using a system-call API, 
                   a new,semaphore-object/semaphore can be created - 
                   the semaphore-object is created
                   and it has fields, including 
                   a semphore-instance/counter - after 
                   creation of a  semaphore
                   instance,it needs to be initialized, 
                   as per the rules of semaphores... 
---->a semaphore does not have a default value,typically - 
     we need to 
     explicitly, initialize its value  
            -->using this returned handlei or idi , we can
               do further operations on the 
               semaphore-objecti/semaphorei,using 
               appropriate system-call APIs - there is 
               a handlei/idi, for accessing 
               semaphore objects/semaphore instances  
               -->there is another system call API, for
                  initializing a semphore-instancei, in a semaphore-
                  objecti  
                 -->for instance, once we have 
                    the idi/handlei, we can initialize
                    the semaphore value to 1 or 0, or 
                    another +ve value - "in this 
                    scenario, let us assume, that the 
                    initial value is set to 1, for
                    using a binary-semaphorei, as 
                    an OS, lock-mechanism" - 
                    we will be using it, as a binary- 
                    semaphore, with initial value of 
                    1 - some of these are done, as per 
                        rules,as well as application- 
                        requirements 

--->we will resume on 24.12.2021..at 10.30 a.m....

                  -->once initialized, we can operate
                     on the semaphore-instancei, using 
                     "decrement-operation/a system-call API" and 
                     "increment-operation/another 
                     system-call API", as per 
                     our application's requirements
                     -->these decrement/
                        increment operations are atomically, 
                       implemented - internally, OS uses
                       "hw-atomicity + hw-instructions" + OS-code - 
                       let us assume, for 
                       now - read the discussions above, as
                       well as discussions below 
                        
                    -->by default, "semaphore, decrement-operation 
                       is a blocking-operation" 
                      --->for the common-case, decrement- 
                          operation is a non-blocking 
                          operation
                       --->for certain conditions, 
                           decrement-operation will 
                           block the current-processi/pdi 
                           and add it to wqi of semahore
                           instancei/semaphore-objecti
                    -->these operations are implemented,
                       as system-call APIs  
================================================================
---->we may edit and push this down ??
             -->first, let us understand the usage
                of a semphore, as lock, for managing 
                critical sections:
     --->refer to lecture diagrams ??
                   -->in the context of a set of 
                      critical sections, in two 
                      different processes using 
                      shared-segments/shared-data, 
                      we can protect the critical 
                      sections, using dec() and 
                      inc() operations sorrounding 
                      critical sections' code -
                      refer to the lecture diagram 

     --->semaphore operations are very standard 
         operations and their usage patterns/coding
         patterns are also standard - we need to 
         understand 
         our application's requirements and use
         semaphores, as per one of the standard
         coding patterns of semaphore usage 
   -->once we understand the semaphore operations, 
      we can understand a typical usage of a 
      semaphore lock , for protecting critical 
      sections of a shared-data problem       
---->push the above contents below
====================================================================     
           -->initially,let us understand decrement 
              and increment operations - there are
              well-defined, OS-rules, for decrement 
              and increment operations - once we 
              understand these operations, we will 
              illustrate a scenario, for using a semaphorei,
              as a lock-mechanism ??
                
---->refer to freehand_diagrams_21.12.2021.pdf/ slides 43-47 and
     30 - 34  ??
 
    -->following is a summary of a decrement- 
       operation on a semaphore/semaphore object:
                   - if a processi invokes decrement 
                     operation/system-call API on 
                     a semaphore-instancei/semaphore-objecti
                     and if
                     the current-semaphorei's  value is 
                     +ve(>=1)/a scenario, decrement-operation's 
                     service-routine will decrement the
                     current-semaphorei's value, by 1 and 
                     return success(0) - this 
                     operation is done atomically, in the 
                     system-space, using "hw-atomicity +
                     os-coding" 
--->so, in the current-case, since the value is 1, 
    it will be decremented to 0

---->following is another scenario, when decrement 
     operation is invoked : 
                   - if a decrement-operation/system-call 
                     API is invoked on a semaphorei/
                     semaphore-objecti and the current- 
                     semaphorei's value is 0, decrement- 
                     operation's service routine will 
                     block the current-processi and add its pdi 
                     to the wqi of the semaphore-objecti - actually, 
                     there is no decrement-operation, just 
                     blocking is done -  
                     in this scenario, 
                     the current-processi/pdi is moved to 
                     blocked-state and added to
                     the wqi of the semaphore-objecti - at the end of 
                     this blocking-operation, the cpu-scheduler 
                     is invoked - another processj will be 
                     scheduled /dispatched  
                   - this blocked-processi/pdi will be 
                     unblocked, when this semaphorei of
                     semaphore-objecti is 
                     incremented, in the future, by another
                     processk, using increment system-call 
                     API of semaphore(refer to the increment- 
                     operation explained, next) 
----->let us try to capture the above key points, 
      in a set of free-hand diagrams - 
      also, read the comments - you can refer to free-hand 
      diagrams/ slides 30-35

--->refer to slides 35-38   of freehand_diagrams_21.12.2021.pdf 

--->first-break on 24.12.2021....
 
         -->following is a summary of an increment 
            operation on a semaphore-instancei/semaphore-obji:
                  - if a processk invokes an increment 
                    system-call API and there is no 
                    blocked-processi/pdi, in the wqi of the 
                    semaphore-objecti(a scenario), 
                    the operation will just 
                    increment the current, 
                    semaphore-value, by 1
                    and return success - such operations
                    are atomic - this is needed, as part 
                    of kernel-space/shared-data concurrency
                    issues
--->in another scenario, following will be 
    the behaviour :
                  - if a processi/pdi is blocked, in the 
                    wqi of the semaphore-objecti(another,
                    scenario), in addition
                    to increment-operation on semaphorei, a blocked   
                    processi/pdi will be unblocked and 
                    added to an Rq, and success is returned
                -->this operation is also done,  
                   atomically

----->let us try to capture the above key points, 
      in a set of free-hand diagrams/ slides 35-38 - 
      also, read the comments
----->this is a logical end and we will resume on 
      after lunch on 09.07.2021.... 
===============================================================
---->in typical, scenarios, there will be only 
     one waiting-process(thread or another, entity)
---->whereas, in some other scenarios, we may have multiple-
     processes/threads waiting, in a wq of a semaphore 
     objecti ??
--->yet another scenario - what happens, if there are
    2 or more processes/pds blocked, in the wq of a
    semaphore and an increment operation is done - 
    meaning, what happens to semaphore-value and 
    the multiple, blocked processes ??
       -->find the OS implementation details, 
          based on further reading and 
          understanding ??
===============================================================

---->for the above reasons, initial usage scenarios will 
     be simple - there will be a single-process, in 
     a wqi of semaphore-objecti - another assumption  

 -->in the case of deletion of a semaphore, 
    there is another system-call API/or another
    operation/service  - 
    this will delete the semaphore-objecti
    and forcibly, unblock/awaken one or 
    more blocked-processes, in the wqi
---->care must be taken, if we are deleting a live
     semaphore-objecti/semaphore-instancei, in
     our applications 
---->deleting will be completely removing 
     a semaphore-objecti/semaphorei from the system

---->in all the above scenarios, follow the rules and
     code responsibly...you must do a code walk-through
     and review your code...

---->once we understand all these basics, developers
     provide us with usage-designs and code-samples
     --->we must understand these application-designs
         and code-samples, in order to solve our 
         applications' designs and problems...we 
         cannot copy, since our requirements will 
         be different, but design-reuse can be done ?? 
 

     --->one of the samples is race_updated.c - check the 
         deletion of a semphore object ?? 
             -->the application code is responsible, 
                for such creations/other operations/
                deletions and must be 
                done, at right places, in the code
               -->OS provides services/system call 
                  APIs and rules, but using these
                  services correctly is the 
                  responsibility of the developers



 -->let us understand the usage of  
    semaphore operations, in a scenario:
    -let us use a semaphore , for locking
     /protecting critical-sections of 
     2 related-processes
 
=========================================================== 
    --->in a typical, GPOS-system, every semaphore-
         instance is a counting-semaphore, but can 
         be used, as a binary-semaphore or a 
         counting semaphore

===========================================================
     -->however, in embedded platforms, like 
        EOS/RTOS, there can 
        be distinct forms of semaphores - some of 
        the characteristics and operations will change ??
        -->so, there will be differences from 
           one OS-platform to another, OS-platform
===========================================================

---->in the following usage-scenario, developer
     recommendations and designs are followed :
---->refer to free-hand diagrams/slides 39/40
     , for a typical
               usage of a binary-semaphore lock, for 
               locking critical-sections, using 
               dec() and inc() operations - this is the 
               convention, we use in semaphores - 
               this is a form of locking and 
               unlocking(), as recommended, by developers
 ---->refer to race_updated_sem.c - this is the code
      sample used , in this usage scenario - what happens, 
      if we set the initial value of the semaphore to 0, 
      instead of 1 ?? --->after the following discussions, 
      we will understand system-call APIs and programming ??

---->following is a typical, usage-scenario, for 
     a binary-semphore of a GPOS, as a lock-mechanism :
     --->in this document, we are referring 
         to GPOS-semaphores only   
         -assume, that we have initialized the initial-value to 
          1 - let us use the "counting-semaphore", 
          as a "binary-semaphore", for locking 
          a set of related critical, code-sections of
          processes
                --->Pi/CS11 and Pi+1/CS12	 
               - in this context, let us assume that 
                 we are working, in an uniprocessor- 
                 system
               - let us assume, that the initial value
                 of the binary-semaphore is set to 1 - 
                 this initialization must be done, as
                 discussed above - we will see specific
                 code later  

---->lunch break on 24.12.2021...at 2.10 pm

--->switch to the free-hand diagram/slides 35-42 and come back 
               - let us assume, that Pi is 
                 scheduled/dispatched/executed, first
               - when Pi is executed, it will decrement 
                 the semaphorei's value by 1,using 
                 dec() operation and proceed
                 into the CS11 - Pi enters its critical 
                 section, CS11(semaphorei's value is changed
                               from 1 -> 0)
              --->assuming there is no preemption, 
                  Pi will complete CS11 and invoke 
                  inc() operation - semaphorei's value 
                  will be changed from 0 -> 1
              --->Pi will be out of CS11 and continue 
                  executing 

	
--->switch to the free-hand diagram/slides 35-42 and come back 
               - what happens, if Pi is preempted, in 
                 the middle of CS11 ?? this is one 
                 of the scenarios - due to this 
                 preemption, Pi will be preempted and 
                 Pi+1 will be possibly, scheduled
                 and dispatched/executed, which will attempt 
                 to decrement the same semaphorei, using 
                 dec() operation/system-call API, but 
                 Pi+1/pdi+1 will be blocked, in the wqi of the 
                 semaphorei, since the current value of 
                 the semaphorei is 0 
               - so, while CS11 of Pi is executing, CS12 
                 of Pi+1 is 
                  disallowed to execute, which means 
                 interleaving of their instructions of 
                 critical-sections, 
                 is disallowed - this means, CS11 will 
                 be executed, atomically, alongside 
                 CS12  - CS11 will not 
                 be disturbed by CS12, and vice-versa
           ---->in the above scenario, what happens, if
                Pi+1 is scheduled/dispatched, first and 
                it is preempted, in the middle of its
                critical section, CS12 ?? fill the answers ??

          ----->also, analyze, what happens, if there are
                no preemptions, in CS11 and CS12 of Pi and
                Pi+1, during concurrent-executions ?? 
                do you visualize, any problems ??
 
---->also visualize the other scenarios mentioned above ??
---->switch back to free-hand diagram / slides 35-42 


     --->code-blocks/instructions of CS11 and CS12 
         will not be interleaved/overlapped, due to the use
         of semaphore/semaphore-operations, for locking, 
         critical-sections 
            - in one of the above cases, since Pi was 
              preempted and Pi+1 is blocked,
              in the near future, Pi will be 
              scheduled and dispatched - due to 
              this, Pi will resume, continue and complete
              its critical code section, CS11 - after completing 
              the critical section code,CS11,  Pi will invoke 
             "increment operation/system call API - 
              in this context, since Pi+1 is blocked, in 
              wqi of semahore objecti/semaphorei, 
             , so the increment operation will increment
             the semaphorei's value, as well as 
             unblock Pi+1/pdi+1"
---->switch back to free-hand diagram / slides 35-42 
           - in the near future, Pi+1 will be scheduled 
             and dispatched - Pi+1 will resume execution, 
             complete its decrement operation and 
             enter the CS12 - dec() operation succeeds, in 
             Pi+1 - while Pi+1 is inside the 
             CS12, Pi will be disallowed to enter 
             CS11 - visualize this, as another scenario - 
             while Pi+1  is in its critical 
             section, CS12 and the current value of 
             semaphore is 0, Pi cannot enter its 
             critical section, CS11, as it will 
             be blocked, due to a decrement operation on 
             semaphorei
 ---->switch back to free-hand diagram/slides 35-42 and 
      come back
      --->slides 41 and 42 are just time-line diagrams, 
          that describe the above scenarios and the 
          actions 
          
---->such understanding is needed, if we need to deal, with 
     OSes and concurrent-programming, in our applications
     ---->very much needed, for EOS/rtos scenarios ?? 
 
----->if you are involved, in concurrent-programming and
      related OS-services, we need to understand the influence
      of OS-services on our application's code/
      concurrent-executions, 
      at micro-level - in addition, we need to test and
      verify, that our interpretations are working 
      correctly - we may use certain tools - 
      our interpretations must be correct 
      and our application - 
      designs must work, as expected - if not, our interpretations of 
      the usage, and concurrent-designs and programming need to be 
      changed ?? some of these points will be 
      extended, during RTOS-scenarios ?? some of these
      basics are indispensable, in EOS/RTOS environments 
      and these are mandatory-requirements 
            	
             - effectively, a  binary-semphore lock 
               has serialized the execution of 
               related, critical-sections of Pi and Pi+1
----->although Pi and Pi+1 are concurrently executed, 
      their related critical-sections' codes are 
      serialized, to execute atomically 
 
                   -->if a binary-semaphore lock is used 
                      appropriately, Pi and Pi+1 are 
                      disallowed from accessing the 
                      shared-memory segment/data, 
                      concurrently - rather, Pi and Pi+1 
                      are allowed to access the shared-memory
                      segment/data,exclusively and atomically, 
                      one critical section, at a time    
 
                 --->in the above cases, what happens, 
                     if Pi's code acquires the semphore-lock 
                     again, not Pi+1, after Pi increments
                     the semaphorei and unblocks
                     Pi+1 ?  -->meaning, if Pi has a loop, 
                     where the semaphore-operations and 
                     critical-sections are repeated ??

                     meaning, what happens, if Pi again invokes
                     dec() operation, after an inc() 
                     operation, in its code's loop - assumption
                     is, that Pi continues, using semaphorei   ??  
                  -->Pi+1 will not be able to get 
                     its share of access to semaphorei and 
                     shared-memory data, and this will 
                     lead to a form of starvation - these
                     are possible problems ?? just be aware..

==================================================================
--->is there any specific-solution, for such a problem ??
       ---->we can add some form of blocking operation, 
            along with Pi's application's requirements - 
            however, we need to check, if these blocking 
            operations are acceptable to application's
            requirements 
           ---->we need to change the code of Pi, using 
                certain models ??
===================================================================

--->we have introduced synchronization once, at the
    top of this document, during message-queues ??
    ---->synchronization was implicitly taken care 
         , by message-queue operations
    --->primary IPC is message-queue mechanism...

---->in the above case of binary-semaphore, is the 
     synchronization explicit, or implicit ??
     ---->here, the primary IPC-mechanism is 
          shared-memory data-exchange only - shared-memory 
          IPC-mechanism does not provide implicit, 
          synchronization, so we need to add
          explicit, synchronization  
     --->in the above case of critical-sections, 
         we have explicitly used a binary-semaphore
         and its operations, to provide an OS, locking- 
         mechanism - as part of this, synchronization 
         is done, by semaphore-operations - so, we 
         can treat this , as an explicit-synchronization..
---->if we understand the micro-level details, during 
     dec()/inc() operations, there are conditional 
     blocking-operations and unblocking-operations
---->so, there is a co-ordination established between 
     processes, during concurrent-executions of 
     critical-sections, using dec()/inc() operations 

---->what is process synchronization, in this context ??      
     ---->in these scenarios, if one of the processes
          enters a related critical, code-section/CS11, 
          the other processi+1 is blocked, using a 
          common-semaphorei/semaphore-objecti - in 
          addition, if a processi+1 is blocked, due to 
          another processi executing, in  its critical,code-
          section/CS11, after completing its critical code 
          section/CS11, processi+1 will be unblocked - 
          these actions are taken, by semaphore-object'
          /semaphorei/dec() and inc() operations  
---->in this context, a binary-semaphorei/semaphore objecti
     is being used  
    ---->such a co-ordination of processes, using
         an OS-mechanism is,  known as synchronization - 
         in this case, a binary-semaphorei/semaphore-objecti
         is the OS mechanism  
    ---->in the current-document, we will be using 
         a form of semaphore to achieve appropriate 
         process-synchronization
    ---->in the future, we will come across many 
         other, "OS synchronization-mechanims", in 
         different contexts  

         - in the above context, a semaphore is used, 
           for "explicit-synchronization", 
           "which facilitates 
                  atomic-access to a shared-data", in a 
                  shared-memory region between two-processes - 
                  we need to understand the details of working 
                  of a semaphore mechanism, for understanding 
                  the synchronization mechanism provided by 
                  semaphore - in this "context of 
                  synchronization", 
                  there are "high-level details" and "low-level 
                  details" - the above discussions described 
                  "high-level details"
                  - below descriptions cover certain, 
                  "low-level details"

--->we will resume at 3.00 pm on 01.06.2022....

          --->using semaphores, does not completely remove
              concurrency, but provides a restricted 
              form of concurrency, as described above - 
              some parts of the concurrent-codes are
              serialized - certain critical-sections 
              are serialized, to enable atomic-executions
---->there is a form of OS-aware-locking, using 
     semaphore/semaphore-operations 
---->visualize restricted executions, in the above 
     scenarios ??
 
          --->what is synchronization, in the context 
              of OS platforms, including RTOS /embedded
              contexts ?? we will  be seeing other 
              synchronization-models 
               -->controlled /restricted access to 
                  shared-data, in the context of 
                  concurrent processes or threads 
               --->controlled /restricted access
                   to shared hw-resources, in the 
                   context of concurrent processes/
                   threads - coding techniques will 
                   differ - there will hw related code/
                   driver related code 
         ---->for many of the above embedded-scenarios, 
              we need other, techniques and skills.. 

               --->such controlled/co-ordinated 
                   execution of concurrent 
                   processes/threads is, known 
                   as synchronization 
               --->such OS mechanisms are known 
                   as synchronization mechanisms 
               --->these OS mechanisms and their 
                   operations will definitely 
                   use certain low-level, hw 
                   synchronization-mechanisms
----->in all the above discussions, did we mention/discuss
      low-level/processor level synchronization mechanisms ??
      --->or, only OS level mechanisms ?? only OS level 
          mechanisms were covered, NOT hw-level mechanisms 
                
---->following discussions are purely on low-level mechanisms 
     used, in semaphore operations, like dec()/inc(), using 
     hw-atomicity and related instructions and programming 
--->however, we will just understand certain, important 
    points, using slides 43-49 of freehand_diagrams_21.12.2021.pdf
    --->you may optionally, read the following discussions - 
        line nos.3263 -    
--->third-break on 24.12.2021....

--->again, refer to slides 43-49 of freehand_diagrams_21.12.2021.pdf
    --->if we analyse the scenario and the comments, 
        there is are serious race-conditions, in kernel-space
        , which lead to race-conditions, in user-space
    --->so, the kernel-space, SRi must be modified and 
        forced to execute, atomically 
        --->an uni-processor scenario , as well as 
            a multi-processor scenario are well 
            illustrated and described below 

---->if so, what are the practical-solutions, for 
     enabling atomicity, in kernel-space SRis of 
     semaphore-operations ???
       

--->again, refer to slides 43-49 of freehand_diagrams_21.12.2021.pdf
---> "low-level details of semaphores" and their 
      synchronization mechanism :
         -we will be using "low-level hw features of 
          processors", like "atomic machine-instructions", 
          which work , at "hw /processor level" and 
          "provide absolute atomicity", during the 
          execution of the atomic machine instruction
          --->such atomic machine instructions will be
              used, in semaphore operations, or other 
              IPC mechanisms
                --->in the case of absolute atomicity, 
                    hw interrupts are masked,during 
                    these service-routines - there may be 
                    other such scenarios 
--->in the first case, let us assume, that hw interrupts
    are disabled during SRi and SRi+1 - meaning, 
    hw-interrupts are disabled at the entry of SRi and 
    SRi+1 and re-enabled at the exit of SRi and SRi+1
--->such interrupt-masking/unmasking is known as 
    an interrupt-lock, in OS terminology  
--->if this set-up is true, will there be a preemption 
    allowed, in SRi and SRi+1 ??
now, refer to slide 43 and will there be an interrupt 
event, followed by a preemption ??

===========================================================
--->refer to 15.4.1 of rt_e_concepts.pdf, where a different 
    scenario of interrupt-lock usage is described - however, 
    this scenario is also, an EOS/rtos scenario, not 
    a bare-metal scenario 
===========================================================
            
  ---->however, for specific details, refer to 
       hw/processor/vendor related documents 
  ---->atomicity, at OS level/high-level is 
       different from atomicity, at low-level/
       processor level
---->many of these mechanisms are very close to 
     embedded/hw , but also connected/linked to
     OS mechanisms/sw mechanisms 
---->many of these hw, low-level mechanisms are
     useful, in the context of OS mechanisms...

============================================================ 
refer to ....DHT0008A_arm_synchronization_primitives.pdf, 
page-6, where disabling hw-interrupts in an uni-processor
system is described - in this case, an OS platform is
assumed, but an uni-processor hw-platform is used - 
this interrupt-lock will not efficiently/effectively work on 
multi-processor systems - read more, if required  
============================================================

---->why is an OS/level atomicity known as non-absolute
     atomicity  ?
     ---->certain related pieces of critical code-
          sections are atomically executed, with 
          respect to each other 
     ---->however, such a critical code section can 
          be interrupted and ISRs can overlap
     ---->in addition, a critical code section
          can preempted and another, unrelated 
          process/its code sections can be 
          executed /overlapped 
 
    --->atomicity, at OS level is not absolute 
        atomicity - in this case, hw interrupts
        may not be disabled, but certain code 
        sections will not be allowed to 
        interleave their instructions 
 
     --->atomicity, at low-level is absolute 
              --->in the following discussions, we 
                  will understand absolute 
                  atomicity and its usage  
             - based on these "absolutely atomic low-level 
               features/instructions", we can "build other
               high-level/OS-level synchronization/
               OS-aware-lock mechanisms"
            - during the execution of these atomic machine 
              instructions, there is no disturbance, 
              including hw interrupts - absolutely, 
               atomic - typically, hw interrupts are
               masked,during their execution  
            --->some of these low-level mechanisms will 
                be used, in providing low-level support 
                to semaphore's atomic operations/
                service routines
---->in the above discussions, we have mentioned, that 
     semaphore operations are atomic - they are atomic
     , due to certain low-level, absolute, atomic 
     mechanisms 
---->so, now we will focus on the service routines/operations
     of semaphores, in kernel-space...


---->we need atomic semaphore operations, in kernel-space,
     such that, there is no disturbance, when a decrement 
     or increment operation is processed... 
     

----->let us first understand, why do we need low-level
      hw supported atomicity ?? 
      refer to a freehand_diagrams_21.12.2021.pdf/slides 43-49
 
              - in uniprocessor or multiprocessor systems, 
                if semaphore operations are operating on 
                identical semaphore objects/semaphores,
                there will 
                be race-conditions, due to concurrent or 
                parallel executions of semaphore operations/
                system call service routines, which are accessing 
                shared semaphore objects/semaphore counter/
                other such shared system data
------>refer to one of the free-hand diagrams / slides 43-49, that 
       illustrate semaphore operations on a 
       common semaphore object ?? read the commentss 
       --->now, continue and understand the following :
----->we will resume on 10.07.2021, at 10 am...
 
           --->these are problems, in system-space, not 
               user-space ??
                --->"are you able to visualize such 
                     problems" ?? refer to free-hand 
                     diagrams /slides 43-49 
                -->in these contexts, system-space 
                    shared-data is accessed concurrently, 
                    by system call APIs/service routines, which 
                    are invoked by two or more processes 
                    operating on the same semaphorei/semaphore
                    objecti
                     -->in uniprocessor scenarios, 
                        when a processi is using a dec() 
                        operation/system call API/service 
                        routine 
                        on a semphore objecti/
                        semaphorei, it can be preempted 
                        and another processi+1 may be accessing 
                        dec() operation/system call API/service
                        routine 
                        and the same semaphore
                        objecti/semaphorei is accessed, 
                        concurrently
                        --->this preemption is currently, 
                        in system-space, when the dec 
                        operation's service routine is 
                        being executed
                        --->assuming, the next processi+1 
                        invokes dec() operation, the 
                        corresponding service routine
                        will be accessing the same 
                        semaphore objecti/fields, while 
                        the first process was preempted, 
                        in the middle of a dec operation 
                        on the same semaphorei/semaphore 
                        objecti - this 
                        is a form of critical sections/
                        data-race, in system-space 
--->in the above case, what happens, if first decrement 
    operation of Pi has completed reading the semaphorei's
    value, 
    but before updating the value, Pi is preempted, in 
    the middle of service routine 
-->Pi+1 may be scheduled/dispatched and it will again 
   use dec/service routine and operate on semaphorei 
--->what will be the outcome, in the above case ??
    Pi will assume, that it can use the semaphorei, 
    since the value seen is 1
--->since the value was not atomically updated, 
    Pi+1 will also see semaphorei's value, as 1
--->So, both the processes may enter their 
    critical sections , CS11 and CS12 - this 
    will lead to inconsistency, in the applicaton's
    shared-data operations ??
    -->refer to free-hand diagrams/slides 43-49 and
       the relevant comments   
 
    -->in multiprocessor scenarios, 
       when a processi is using a dec
                        operation/system call API/service routine 
                        on a semaphore objecti/
                        semaphorei, another processi+1 may 
                        simultaneously access the same 
                       semaphore objecti/semaphorei,using 
                       dec() operation/system call API/service routine
  ---->                in this scenario, there will 
                       be parallel execution of semaphore
                       operations/service routines accessing 
                       a common semaphore objecti/semaphorei,
                       in system space
---->assumption is that Pi/dec() /service routinei execute 
     on cpu0 and Pi+1/dec()/service routinei execute on 
     cpu1 
---->refer to another free-hand diagram / slide 47 
     ---->are you able to visualize and understand the problem,
          in a multi-processor system ??
     ---->in these typical cases, you will observe that 
          the same system code/method/call back is being 
          concurrently executed by different KEPs, in uni-processor,
          or multi-processor scenarios ?? in uni-processor, 
          such executions are due to preemptions and in 
          multi-processor, such executions are due to 
          concurrent/parallel executions 
                    
                    -->these are critical sections, in 
                       system code/system/system-space and 
                       shared-data is system objects/
                       system data-structures
               --->similar to user-space shared-data/race 
                   conditions, but in system-space 
                     --->what are the solutions, to such 
                         problems ??    
                         -->there are different solutions
                            available and each platform /
                            OS may support one or more of these
                            solutions
---->before going forward, let us refer to 
     DHT0008A_arm_synchronization_primitives.pdf from ARM Ltd.

---->if you are following the above discussions, what are 
     the possible solutions ??
      --->definitely, the solutions are different and 
          peculiar - however, these solutions are closer 
          to hw/processor/embedded 


---->in the following discussions, certain standard 
     solutions are provided, but there can be differences, 
     in different OS platforms - different models are
     discussed, so the details will differ 
----->in the following discussions, understand the 
      different models, but we need to adjust , as per
      hw and sw platforms    

   --->in a GPOS platform, we cannot directly manipulate
                 hw interrupts, in user-space -  we cannot 
                 disable/enable hw interrupts, in 
                 user-space - these are due to restricted
                 processor/execution privileges, in 
                 user-space -  however, 
                 it is allowed to disable/enable 
                 hw interrupts, in system-space code, 
                 as there are no processor/execution 
                 privilege restrictions - in fact, 
                 full, processor privileges are provided to 
                 system-space code - these privileges are
                 based on cpu's execution-mode - refer to 
                 our earlier documents 

====================================================================== 
---->do you have such restrictions, in typical 
     embedded code ?? are we allowed to disable/enable
     hw interrupts, in typical bare-metal embedded code ?? YES

---->do you disable/enable hw interrupts, in your 
     embedded code, without OS  ?? why do you do this ?? 
      ---->fill the answers
          --->if we need to atomically execute, certain 
              application code, without any disturbances from 
              ISRs  
 
             --->in the context of RTOS, there will be 
                  more privileges, for developers/
                  applications' code - this will be 
                  better, in RTOS platforms  
                  ---->hw and OS set-up will be different
                       and will provide  more features/
                       privileges to application code
--->refer to your RTOS/EOS manuals ??
 
-->refer to slide 43 of freehand_diagrams_21.12.2022.pdf               
---->we are back into our discussions on 
     dec()/inc() operations and their atomicity ?? 
 
              --->for instance, in one of the 
                  kernel-space solutions, hw interrupts are 
                  disabled, during dec/inc operations, 
                  as per OS support, in system-space - 
                  such operations
                  are known as interrupt lock operations
           --->disabling hw interrupts is a form of 
               locking, in this context 
           --->renabling hw interrupts is a form of 
               unlocking, in this context  

    -->refer to a freehand_diagrams_21.12.2021.pdf/slides
       43/47-56  - refer to 
       llock() and lunlock() operations, in the service
       routines ?? read the comments and interpret, as 
       per uniprocessor context or multi-processor context ?? 

           --->as per this, if a dec() service routine
               is being processed, hw ints. of the 
                        processor are disabled,in llock(), so there 
                        cannot be interruptions and due to 
                        this, there cannot be kernel-space
                        preemptions, during semaphore servicei's
                        operations 
                        --->which means, no other process
                        can be scheduled and can invoke 
                        another dec() operation on the 
                        same semaphorei/semaphore objecti
---->concurrent executions/operations of dec()/SRi are 
     disallowed, due to disabling hw interrupts.. 
---->concurrent executions/operations of inc()/SRi+1 are 
     disallowed, due to disabling hw interrupts.. 

---->meaning, if hw interrupts are disabled, preemptions
     are disabled - this is an OS mechanism of locking 
     interrupts/disabling interrupts   
          -this interrupt-locking mechanism 
           prevents race-conditions, 
           in uniprocessor systems, for 
           dec()/inc() service-routines of semaphores 
----->in this context, disable hw-interrupts, disables
      preemptions and we see the effectiveness
===============================================================
---->do you use such hw disabling mechanisms, in 
     typical purely embedded code - if so, what 
     are the reasons, for using, such interrupt locking
     mechanisms ?? you need to fill-up some more 
     answers here ??  
===============================================================
---->however, such a solution may not be effective, 
     in a multi-processor system ?? WHY ??

          -this interrupt locking mechanism
           does not solve race conditions, 
           in a multiprocessor system - overlapping 
           executions of processes, in multi-processor
           systems cannot be prevented, in system-space,
           and dec()/SRi  and inc()/SRi+1 will execute concurrently 
           on different processors  
---->so, this interrupt masking/unmasking solution is effective only 
     , in uni-processor system, not multi-processor system
            --->however, interrupt masking/locking
                solution is good enough for 
                uniprocessor systems, not 
                multiprocessor systems
--->some of these mechanisms will be effective on uni-processor
    systems only - so, they will be used, for certain 
    platforms and may not be effective, for other platforms, 
    so unused, in multi-processor platforms... 

================================================================
         -->we will see more details, during RTOS/
            embedded contexts, for interrupt locks - 
            one issue will be increasing 
            hw interrupt latencies - still, if correctly 
            used, interrupt locks work properly
--->we will revisit many of these details, during 
    embedded scenarios - there are good references, that 
    discuss, such problems and solutions
 
=================================================================

--->first break on 10.07.2021....

--->so, we will study more alternative, low-level 
    mechanisms, for locking/atomic operations
--->again, refer to freehand_diagrams_21.12.2021.pdf/slides 50
    --->in the place of llock() /lunlock(), we will use 
        preemption disabling/enabling  
       --->one alternative low-level mechanism
           is disabling preemption, during 
           the dec and inc operations of 
                     semaphores - when OS preemption disabling 
                     is done, scheduler will not be 
                     invoked,when preemption points 
                     are encountered - this is a form 
                     of temporary disabling of 
                     preemptions/preemption points - 
                     these mechanisms are popular, in 
                     system-space of GPOS and also
                     in RTOS,for certain scenarios

---->in free-hand diagram/slide 50, visualize 
     llock(), as disabling preemption 
      --->there will be some kernel counter/flag 
---->in free-hand diagram/slide 50, visualize 
     lunlock(), as re-enabling preemption 
      --->there will be some kernel counter/flag 
                   
             -->this preemption lock will 
                allow hw interrupt events, 
                but will disallow preemption 
                events -effectively, protects
                system-space critical sections/
                related objects 
---->which means, hw interrupts are allowed - their 
     ISRs are allowed - but these ISRs will not be '
     allowed to trigger cpu scheduler - meaning, 
     after the ISRs are completed, they will resume
     the interrupted code, as long as preemption 
     disabling is on - there can be interruptions, 
     but no preemptions  
 
    --->effectively, no other processi+1 
        can preempt the current processi
        and hence, no other processi+1 
        can invoke dec()/SRi operation
---->so, low-level dec/SRi operations cannot overlap 
     and create inconsistency problems 

---->again, revisit a free-hand diagram/ slide 50, 
     that illustrates
     llock() and lunlock(), but instead of hw int. 
     locking, pre-emption locking will be done ??
     --->visualize llock(), as a preemption diabling operation
     --->visualize lunlock(), as a preemption enabling 
                                 operation 
 ---->are you able to visualize and understand, what 
      is a preemption lock ?? current process can be
      interrupted, but not preempted

===============================================================
----->for now, just follow our discussions - in addition, 
      read Preemption disabling, in chapter 10/LKD,page 225,
      3rd edition
      and try to connect - it will not be straightforward ??
--->also, refer to MasteringFreertos.....tutorial.pdf/ 
    section 7.2 - interrupt-lock and preemption-lock/
    scheduler-lock are described for embedded-OS scenarios 
===============================================================
--->refer to slides 43/47 of freehand_diagrams_21.12.2021.pdf      
----->will this preemption locking/disabling  
      and enabling/unlocking  solve race-conditions, 
      for dec()/inc() operations/their service routines, 
      in system-space, in uni-processor systems ??
       ---->YES

----->will this preemption locking/disabling preemption 
      and enabling/unlocking solve race-conditions, 
      for dec()/inc() operations/their service routines, 
      in system-space, in multi-processor systems ??
       ---->NO 
          --->disabling preemptions does not disable 
              concurrent executions of Pi/dec/service 
              routine and Pi+1/dec/service routine on 
              cpu0 and cpu1, concurrently
---->for instance, if preemption is disabled on cpu0,
     still, Pi+1 can be scheduled dispatched on cpu1 -
     preemption disabling is effective on cpu0 only, 
     in this case - these are typical OS rules... 
      --->in a typical GPOS, some of the 
                           core services are not exported 
                           to user-space, but available, 
                           in system-space - disabling hw
                           interrupts/preemption disabling 
                           are not exported to user-space -
                           these services are available, 
                           in system-space only   
        -->some of these services are
                          exported to user-space/applications, 
                          in embedded OS/RTOS - we will encounter
                          many of these details, in the future 

              --->many of these locks/mechanisms apply to 
                  GPOS/system-space and RTOS/embedded contexts
                   --->however, restrictions differ, 
                       in different contexts - in embedded 
                       contexts, there will be more 
                       freedom, but responsibilities will be
                       more..
                  --->in GPOS systems, there is lesser freedom,
                      but OS platform will take care of 
                      most reponsibilies... 
 
             -->as per the OS platform/uniprocessor/multi 
                processor, different locks/mechanisms/
                solutions may be used, for llock()/
                lunlock()


---->refer to chapter 8/slides 38/42 of Charles Crowley/text-book ??       
     ---->there is an illustration similar to our 
          discussions
          ---->ignore slide 38, for GPOS discussions
               --->however, we can use slide 38, for 
                   RTOS discussions 
          --->we will be referring to slide 42, for a 
              counting semaphore
             --->connect these details of service routines, 
                 with free-hand diagram/slides 43 and 50 
 
---->again, we are back into our major discussions :
---->since the above low-level solutions are unable to 
     solve multi-processor requirements, we need to 
     look, at other alternative solutions ??
--->refer to slides 43,47, and 50 of freehand_diagrams_21.12.2021.pdf
---->refer to chapter 8/slide 42 of Charles Crowley/text-book ??       
               ---> there are "alternative solutions, 
                    for multiprocessor systems"  
                     --->in one of the solutions, there
                         is a "lock-variable", in every 
                         "semaphore-object" - this is not 
                         the semaphore variable, but just 
                         a lock variable, known as 
                        "spin-lock" mechanism
----->refer to our free-hand diagrams/slides 47/50-56 
      of semaphore 
      object - we will be adding a lock variable to 
      it - it will be used , as per implementation 
      details - each semaphore object contains a 
      spinlock variable, locki located, 
      in its system-objecti
          --->this lock-variablei and its mechanism/
              operations is known as spinlock-mechanism 
 
                       --->this spin-lock 
                           variablei/mechanism is 
                           implemented, using "hw machine
                           instructions" - in this
                           context, "atomic 
                           swap instruction", which can 
                           do "an exchange of data between 
                          a register and a memory location,
                          atomically " 
---->refer to provided ARM document, 
     DHT0008A_arm_synchronization_primitives.pdf, which has 
     descriptions on, such atomic machine instructions
        --->refer to section A.1 of this reference, before
           going forward....
              --->based on the processor architecture, 
                  such low-level instructions may differ 
              --->such an OS lock, spinlock may be known as 
                  mutex lock, in embedded language - 
                  in OS platforms, it is known as 
                  spin-lock only, not a mutex-lock
---->refer to the above ARM related pdf, slides 24/25
      --->you must read these sections, understand
          and connect the details below : 
 
---->we will separately introduce OS mutex-locks, 
     in the near future ??
             ---->mutex-locks will be covered, in 
                  multi-threading and RTOS related 
                  sections 

--->refer to any free-hand diagram/slides 47/50 , for 
    a semaphore objecti and a spin-lock variable, 
    locki, in semaphore objecti
 
                    --->following is the typical working
                        of this lock variable/spin-lock
                         and its 
                        operations,in OS context:
                     -there is a "locki" variable, 
                      in each semaphore objecti
                     -this is managed, as a "spin-lock"
                      variable/mechanism 
                     -if its "state/value is 0", 
                      it is said to be, "in unlocked
                      state" - the spin-lock is 
                      unlocked/free/available - this is 
                      the convention, and we must follow
                     -if its state/value is 1, "it is 
                      said to be, in locked state/unavailable" - 
                      the spin-lock is said to be 
                     , in locked state/locked/unavailable 
                   - there is a "spin-lock operation,llock()" 
                     and 
                     there is another "spin-unlock operation,lunlock"  
---->llock(), in this context, refers to a lock operation on a 
     spinlock variable, locki 
        --->set the value of locki to 1(locked-state), 
            if the current value is 0(unlocked-state),
            atomically 
---->lunlock(), in this context, refers to an unlock operation on a 
     spinlock variable, locki 
        --->set the value of locki to 0(unlocked-state), 
            atomically 

           --->in this OS context,  the spin lock and its 
               operations are available, in system space 
               only - these services are not exported
               to user-space - these are system-space 
               services only 
 
            -->in the following discussions, a processi
               invokes dec()/inc() operations on a  
               semaphorei of semaphore objecti  and 
               as part of these operations, 
               spinlock operations are invoked to 
               protect access to shared system-objecti/
               semaphorei
--->refer to free-hand diagram / slide 47
       --->refer to lock variable 
       ---->refer to llock()(a)/lunlock()(b) operations used, 
            in the service routines 
--->revisit one of the free-hand diagrams/slide 50, that illustrate
    llock()/lunlock() usage, in semaphore's service routines
             - if a processi invokes a decrement 
               operation/dec() on a semaphorei/semaphore
               objecti, as part 
               of the service routinei of dec() 
               operation, spin-lock operation , llock() is 
               invoked on  
               the lock variablei of semaphore
               objecti - during this
               execution, we say that the current 
               processi is executing, in system-space - 
               it is a KEP, a kernel execution path... 
             -so, the context of execution is a 
              process context, in a KEP  
            - if the "spin-lock operation, llock() is invoked"
              and the "lock-state of lock variablei of 
              semaphore objecti is currently unlocked,0", 
             "its state will be changed to 
             "locked-state,1", atomically,using 
              an atomic machine instruction - typically, 
              a swap-machine instruction, or another
              machine-instruction 
----->refer to A.1 of the above mentioned ARM document...
--->refer to free-hand diagram / slide 47

---->in the above context, a spin-lock is said to 
     be acquired /locked 
   
    - however,if the "spin-lock operation, llock() is invoked" 
      and the "lock-state of the lock-variable 
      is currently locked,1", 
      the "spin-lock operation will keep 
              busy-waiting, until the spinlock 
              is in unlocked-state,0" - that is 
              why, it is known as a spinlock - there will be a
              form of busy-waiting...
----->refer to A.1 of the above mentioned ARM document...
--->refer to free-hand diagram / slide 47-55-56

---->Pi+1-->dec-->llock(), 
---->in the above context, spinlock is unavailable, 
     so the llock() code will be spinning/
     busy-waiting  
           - in this context, current processi+1 
             is , in running state/executing, 
             but spinning and waiting for 
             spinlock to be available, in a KEP    
---->if the above scenarios is an uniprocessor scenario ,
      analyze and connect ??

 
 ---->revist a free-hand diagram/slide 47 illustrating 
      llock() and lunlock() on a lock variable 
      of a semaphore objecti, as well as the 
      ARM synchronization document /slide 24
      
   - unlock operation is straightforward
            - just set the state of the 
              "spin-lock variable to unlocked state 
               and return"  
            - spinlock lock operation is 
              implemented using atomic hw
              machine instructions, like 
              "swp" or  more sophisticated 
               machine instructions of processor
               architectures  
           - such low-level spin-lock is used 
             to protect semaphore operations/
             service routines 
             and semphore/semaphore operations
             /service routines 
            enable high-level lock /operations
         - at lower-levels, spin-lock 
           uses low-level machine instructions
           to manage atomicity 
           of semaphore operations/service routines

     -->we need to use the correct terminology, in 
        the hw/embedded and sw/OS contexts, respectively - 
        for instance,there is a "embedded mutex 
        implementation" - in the context of 
        OS/kernel, the same embedded mutex implementation is 
        known as "spin-lock(s)/spin-lock operations" - these
        operations use atomic machine instructions                   
     -->there is a separate "OS mutex mechanism/
        implementation" - we will 
        see the details,during threads and embedded/RTOS 
        discussions
     
       - based on the above set-up and discussions, 
         "using low-level hw features/machine instructions"
         and "certain low-level OS operations", 
           semaphore operations are atomically 
           implemented - based on these semaphore 
           operations/service routines, semaphores 
           can be operated
          atomically and used , for "high-level 
          locking of a shared-memory access or 
          a shared-resource access, in applications"
----->if low-level operations of semaphores are atomic, 
      due to the above, high-level operations are
      atomic , as well

--->also, visualize multi-processor scenarios in 
    the above cases - what do you interpret ?? 
    what are the problems ?? 

----->we will resume after lunch on 10.07.2021....

---->we will resume on 02.06.2022...

---->let us understand and finalize the details, based
     on the following scenarios ??
                   
        - following are the conclusions, in an uniprocessor
          system, if we just use spinlock mechanism 
          only, for low-level locking of dec/inc 
          operations :
           -if a process(Pi) has invoked a dec operation 
            on a semaphore instancei, the operation will 
            first lock(using atomic hw instruction) 
            the spinlock/locki of semaphore objecti, 
             using llock()  
             and continue 
            (Pi)'s dec operation, in a KEPi -
            but Pi's KEPi may 
            be preempted -hw interrupts are allowed 
            and preemption is allowed, in decrement operation, 
            after llock()  -  
            once preempted, another process(Pi+1)
            may access dec operation on the same semaphore
            instancei and hence, will spin, for the spinlock,
            locki of the semaphore objecti,  
            as it is already locked,taken - 
            what happens to Pi+1's 
            KEPj ?? - Pi+1's KEPj will be spinning on 
            the processor, until the locki 
            is unlocked, or time-slice/time-share of this
            process(Pi+1) expires - so, if the time-share/
            time-slice of Pi+1 expires, 
            the other process(Pi) will possibly,  
            be scheduled/dispatched and complete its job, 
            in the near future - since Pi is already 
            holding spinlock, locki, 
            it will decrement the semaphorei and enter
            CS11 - Pi's critical code 
            section, CS11 will be executed atomically, 
            at high-level,since the semaphorei's 
            value is 0 - once Pi completes CS11, 
            it will increment semaphorei, using inc() 
            operation -  
            the second process(Pi+1) will get 
            scheduled/dispatched, in the near future - 
            it will acquire spinlock, locki and then,
            semaphorei is decremented - so, Pi+1 
            will enter into its CS12  
            and progress - the above dec() cycles 
            continue, followed by inc() cycles - 
            are there overheads,due to dec()/inc() 
            operations and  
            llock()/lunlock() mechanisms/operations  ??
             --->latencies and wastage of cpu cycles 
                 is true, in this set-up... 
---->is the atomicity of dec/inc operations/SRi/SRi+1
     achieved, in 
     these scenarios ?? YES


----->are you able to visualize the behaviour, at the low-level,
      as well as the high-level ??

     -->we will be visualizing another scenario ??
---->refer to a free-hand diagram/slides 43-56 illustrating 
     spin-lock operations, using llock()/lunlock() ??
          --->for the same set-up, 
              in yet another uniprocessor scenario, the 
              current process(Pi) holding the spinlock may be
              preempted, by another higher priority process(Pj)
              which will never release the processor, as it 
             is the highest priority process(Pj) and it will be
              spinning, for the spin-lock variable - 
             this will lead
             to a form of dead-lock, where Pj cannot progress
             and Pi cannot progress, so none of these 
             processes can  progress
      ---->is this behaviour, in this scenario, acceptable ??
            ---->applications cannot progress and there is 
                 a dead-lock scenario, involving Pi and Pj   
----->are you able to visualize ??? 

              --->in this context, Pi+1 is unable to progress, 
                  as it is waiting for a spin-lock resouce, 
                  but 
                  currently using a cpu resource - Pi+1 
                  is of higher importance and it has
                  preempted Pi, and Pi+1 has exclusive 
                  use of cpu
              --->in this context, Pi is unable to progress, 
                  as it is holding the same spin-lock resource, 
                  but waiting for a cpu resource - Pi is 
                  of lower importance and it  has been 
                  preempted by Pi+1 
             --->each process is holding a resource and
                 waiting for another resource, which 
                 is held, by the other process, in this 
                 scenario 

             --->this is a form of dead-lock scenario ?? 
             --->refer to chapter 8 of Crowley, where
                 other, such scenarios are well described
                 ---->read more on the specific 
                      scenarios  

---->can we say, that the above solution has problems, 
     in uniprocessor systems ?? YES, there are problems
              --->wastage of time-slice or time-share, 
                  in some scenarios
              --->or, a dead-lock problem, in other
                  scenarios

---->if all the above points on spin-lock/low-level locks 
     are clear, can we say, that the above solution will 
     work perfectly, in a typical multi-processor scenario, 
     where Pi is scheduled/dispatched on cpu0 and Pi+1 is 
     scheduled/dispatched on cpu1, with all the other
     aspects of Pi and Pi+1 being the same ???
---->switch to a free-hand diagram/slides 43-56 and come
     back ?? in this context, assume a multi-processor 
     set-up...

     ---->on cpu0, 
          let us assume, that Pi locks the spinlock,locki,
          first 
     ---->next,on cpu1,  Pi+1 attempts to access spinlock, locki
     ---->on cpu1,
          in this scenario, Pi+1 will typically spin, for 
          certain/reasonable time, 
          until Pi unlocks the spin-lock,locki on cpu0 
          --->since system routines, like dec operations/
              system routines, 
              are optimally(of course, not real-time 
              performance)  designed and implemented, 
              the spinning will be reasonable and acceptable, 
              in GPOS 
     ---->since this is a multi-processor scenario, 
          ideally, Pi will complete its dec./SRi and unlock
          spin-lock variable, after certain reasonable time 
          time,  and Pi+1 should be able to lock the 
          spin-lock variable and progress - there will 
          be certain latencies/overheads, but no serious problems
---->there will be overheads of spinning, but acceptable overheads
      --->this is an acceptable solution, in multi-processor 
          systems, when the processes are executing on 
          different processors
--->however, there can be problems, if Pi is preempted, 
    while holding a spinlock - however, such problems 
    are overcome, by OS-designs - keep reading further ??  

=================================================================
------->in the same multi-processor context, 
        if there's another process with higher 
        real-time priority than Pi, 
        scheduled on the same cpu0, 
        Pi will still be cpu starved and 
        a deadlock can still take place on a  
        multiprocessor system, as well - 
        yes, it is possible - however, this is not 
        due to spinlock/semaphore mechanisms, but poor selection 
        scheduling policies/parameters of processes and
        and usage of semaphores/spinlocks, in the system
---->do not conclude - keep reading and understanding more 
     scenarios ?? 
=================================================================

--->we can refer to one of the free-hand diagrams illustrating 
    dec/inc operations and their low-level locks ?? slides 43-56
   
---->following is an improvised solution, based on all 
     the above mechanisms...
 
---->most of these are issues, due to uni-processor 
     set-up, or multi-processor set-up 
         - due to the above reasons, an improvised solution 
           is provided(there may be other improvised 
           solutions, as well) :
          
          - as part of SRi, 
            before the spinlock is acquired, local cpu hw
            interrupts are disabled(or,  preemption is 
            disabled in another design) and spinlock is 
            acquired - refer to a1 and a2 below - this is 
            an improvised solution 
---->disabling interrupts, or preemption is design/implementation 
     specific  

          - similarly, after the spinlock is released, 
            local cpu hw 
            interrupts are re-enabled,or 
            preemption is re-enabled, in SRi   

       --->following is the typical dec/inc service routines'
           low-level, lock 
           handling, with this new model/solution ??     
                a1) --->disable local cpu hw interrupts,
                        or disable local preemptions 
                a2) --->acquire spinlock,locki of semaphorei
                          ...... dec/SRi operation on the semaphorei
                b1) --->release/unlock spinlock,locki of semaphorei
                b2) --->reenable local cpu hw interrupts, or
                        reenable local cpu preemptions 
               
-->effectively, llock() will be ----->a1 + a2 
-->effectively, lunlock() will be   ---->b1 + b2  

================================================================      
---->refer to spinlock section of chapter 10 of 
     Linux kernel development, 3rd edition....there
     is a good discussion..
================================================================

 
 -          -the above improvised solution works, for 
             uniprocessor systems and multiprocessor 
             systems - you need to analyze the above 
             solution, for different scenarios, in 
             uniprocessor /multiprocessor
        --->refer to the above dead-lock scenario 
            and try to check, if this solution 
            prevents dead-lock in uni-processor ??  
              
 ---->let us apply the improvised solutions to 
      a multi-processor scenario :
                    
         - if it is a multiprocessor system, what 
           happens, if a process(Pi) is executing on 
           one processor(cpu0) and locks the spinlock of 
           a semaphore instance(preemption or 
           local hw int. is disabled) and another process(Pi+1) 
           executing
           on another processor(cpu1) attempts to lock 
           the same spinlock of the same semaphore 
           instance, as part dec operation
                 -->in this case, while the other 
                    process(Pi) is using the spinlock 
                    on a specific processor(cpu0), the other
                    process(Pi+1) will be spinning, for 
                    the same spinlock on another 
                    processor(cpu1)  
               - once the first process(Pi) on a different 
                 processor(cpu0) releases the same spinlock, 
                 the second process(Pi+1) on the other 
                 processor(cpu1) will acquire the same spinlock 
                 and progress - a few cpu cycles will 
                 be wasted, but this is the basic 
                 mechanism - there are overheads  

------->if there's another process with 
        higher real-time priority than Pi
        scheduled on the same cpu0, Pi will not be preempted, 
        during semaphore operations and
        a deadlock can never take place on a 
        multiprocessor system - in this case,  
        spinlock/semaphore mechanisms have already taken 
        care of certain scenarios, by disabling 
        interrupts or disabling preemption - 


----->a hybrid-solution is highlighted below :
     a1) disable scheduler-preemption
     a2) lock a spinlock--->Lock-variable 

     b1) unlock a spinlock-->Lock-variable
     b2) enable scheduler-preemption 


----->another,hybrid-solution is highlighted below :
     a1) disable local, cpu-interrupts
     a2) lock a spinlock--->Lock-variable 

     b1) unlock a spinlock-->Lock-variable
     b2) enable local, cpu-interrupts 

--->refer to the above scenarios/discussions and 
    analyze for this improvised solution, for 
    uniprocessor set-up
---->refer to slides 43-56, for the current scenario
    - let us assume, that Pi has acquired the 
      llock() and processing SRi service routine of dec(), 
      is there a possibility of preemption ??

    - meaning, is there a possibility of preemption 
      due to a higher priority processi+1 
      , in the 
      system ?? NO 

    - since Pi's dec service can never be preempted, 
      Pi+1(a higher priority) can never be scheduled  
      and dispatched, while dec operation is in 
      progress - so, Pi and Pi+1 will never enter a 
      dead-lock scenario, like the one mentioned, 
      just above these discussions ?? in addition, 
      all other low-level locking issues are taken care

 --->so, we will not have any specific problem, in 
     uniprocessor, as well...

=============================================================
---->refer to chapter 10/spinlocks/LKD, 3rd edition ??
     ---->you can find a decent description of spinlocks
     ---->contexts will be different 
     ---->in addition, spinlock operations may do 
          other additional operations, like disabling 
          hw-interrupts or disabling-preemption 
=============================================================

----->now, refer to free-hand diagrams/slide 50 onwards, 
      again ??
========================================================================

--->in the near future, you may need to refer to 
    professional references, for such scenarios 
    and discussions - for application related 
    issues, as well as OS-related issues 
    ?? 

-->  following is a generic definition for synchronization:
------>you may read these and will come across more
       scenarios 
        -->synchronization involves co-ordination between 
           processes/threads/or other 
           OS/eos/rtos entities, using 
           OS-services, like IPC-mechanims - one of them
           is a semaphore-mechanism...- it involves
           synchronization 
        -->as part of synchronization/co-ordination
           services,tasks/processes
           threads
           may be blocked/unblocked, using system call 
           APIs/wqs  - there will specific conditions, for 
           blocking/unblocking and there will exclusive
           wait-queues
        -->in certain IPCs, synchronization is built-in 
           and managed implicitly - the main job will 
           be data-exchange, but synchronization is done, 
           implicitly - say, the case of a message queue or
           a pipe IPC / a socket IPC  
        -->in certain IPCs, synchronization is built-in 
           and these IPCs are used explictly, for providing 
           some form of synchronization and solving problems, 
           like 
           shared-mem/shared-resource locking/shared-IO 
           locking issues/problems

            --->we need to do explicit programming 
                to solve synchronization issues, like 
                locking - there are well-defined 
                APIs and design-patterns 
                --->there are well-defined design- 
                    patterns, for programming  
                    synchronization mechanisms 
            -->one of the synchroniation 
               IPCs is semaphore-IPC and another 
               is mutex-IPC- however, we may come across other 
               IPCs, which also, provide explicit synchronization 
            -->binary-semaphores are typical lock mechanisms, 
               but 
               can be used for other functions, like 
               "synchronizing device drivers and applications"
                  -->we will see more scenarios/
                     use-cases during embedded/RTOS
---->for instance, refer to 
     section 15.6 of  rt_e_concepts.pdf....
      --->in many of these design-patterns, a binary 
          semaphore or a counting-semaphore is used
          to provide synchronization between tasks, or
          an ISR and a task 
      --->for instance, a binary-semaphore with an 
          initial value of 0 can be used to synchronize
          two tasks - refer to figure 15.6 - Taski+1 
          just uses a dec() operation, before its jobi+1,so
          it will be blocked - 
          Taski uses inc() operation to unblock Taski+1, 
          due to certain conditions - so, there is a 
          synchronization between Taski and Taski+1  

      --->for instance, a binary-semaphore with an 
          initial value of 0 can be used to synchronize
          two tasks - refer to figure 15.7 - Taski+1 
          just uses a dec() operation, before its jobi+1,so
          it will be blocked - 
          ISRi uses inc() operation to unblock Taski+1, 
          due to certain conditions - so, there is a 
          synchronization between ISRi and Taski+1  
     
      --->for instance, a counting-semaphore with an 
          initial value of 0 can be used to synchronize
          two tasks - refer to figure 15.10 - Taski+1 
          just uses a dec() operation, before its jobi+1,so
          it will be blocked - 
          ISRi uses inc() operation to unblock Taski+1, 
          due to certain conditions - so, there is a 
          synchronization between ISRi and Taski+1 
          -->in this case, if ISRi runs several times, 
             due to interrupt-events, counting-semaphore
             will maintain a count of events 
          -->Taski will process its jobi+1, as per the 
             number of events counted in the counting semaphore 
     
--->also, refer to MasteringFreRTOS...tutorial.pdf / 
    chapters 6 and 9, for binary and counting 
    semaphores 

      -->we will see certain simple examples, for 
               counting semaphores and binary 
               semaphores, in the context of GPOS
          --->we will come across counting semaphores/
              binary semaphores  and 
              their usage, in the context of embedded   

        -->refer to a reference, "Linux-kernel development", 
           where such synchronization/locking issues
           are discussed, in the context of system-space
           ,but the basic principles still apply to 
           other OS platforms       
       --->system-call APIs and related features will 
           differ from one OS-platform to another, 
           OS-platform, but most of the basic features
           and principles of semaphores still apply - 
           programming will 
           differ, but basic-principles will not 



     ---->test race_updated.c/race_u on a uni-processor 
          set-up, without a binary semaphore lock 
          --->check the behaviour and understand
              your observations ??
            --->based on our testings, did we 
                face consistent results or
                inconsistent results ??
                --->we may not face race-conditions/
                    inconsistent results, if there 
                    are not enough preemptions, in
                    system - this may be due to load
                    conditions, like no. of processes
                    and IO/interrupts   
               ---->however, if there is sufficient load
                    conditions/IO /interrupts, we will still
                    face race-conditions and due these, 
                    inconsistent results ?? 
     ---->test race_updated.c/race on a uni-processor
          set-up, with a binary semaphore lock
          around critical sections -refer to 
          the discussions below ??
          --->check the behaviour and understand
              your observations ?? 

---->as part of the following discussions, we will 
     cover shared-memory IPC set-up and working, 
     along with race_updated.c/race_updated_sem.c - 
     so, you may need to 
     refer to 
     the code sample and its comments, if required    
     -->first-part covers semaphore set-up and 
        usage, using system-call APis
     -->second-part covers shared-memory set-up and 
        usage, using system-call APIs 

---->first break on 02.06.2022...
 
--->before we jump into the shared-memory set-up/coding
    issues, we will cover semaphore set-up and 
    its coding  

       ---->first, we will have a demo. of 
            using race_updated_sem.c, along with 
            a binary semaphore, as a lock 
            mechanism, for CS11	and CS12 of 
            parent and child processes ??

   ----> let us understand semaphore related 
           "system call APIs and their usage":
       -->refer to code samples, as we progress - 
          immediately, let us refer to 
          race_updated_sem.c ??

---->second break on 10.07.2021....
  --->step 1: creation of a semaphore objecti/semaphorei

          -semget() API  --->semaphore creation/set-up
             --->ret = semget(p1,p2,p3); 

  --->p1 must be an unique KEY value, a number
      to identify a semaphore objecti
      --->we will just select a number, like 
          1333, for our current requirements - 
          KEY value will be 1333, in this case ??
--->so, p1 will be set to 1333 and the new 
    semaphore objecti will be assigned a KEY value of 
    1333 
 
  -->p2 is used to request for a single semaphore, 
     in a semaphore objecti, or multiple semaphores, 
     in a semaphore objecti 
     --->if p2 is set to 1, we will be requesting 
         for a single semaphorei, in a semaphore-objecti
     --->if p2 > 1, we will be requesting multiple, 
         semaphores, in a semaphore-objecti - 
         if p2 is set to 2, we will be assigned  
         two semaphores, in a single semaphore objecti
---->for most common case, we will be using 
     1 for p2 - so, we will be requesting, for a single
     semaphorei, in a semaphore-objecti 

     --->p3 is for flags - refer to code samples/
                         comments
        --->by using a set of default-flags, for
            p3, we can enable required access permissions
            to 
            semaphore object/semahore, in a
            multi-user set-up 
        --->initially, let us use the default settings
            for p3 - follow the code-sample 
 
   --->ret value will be a semaphore idi/handlei
        of the new semaphore objecti/semaphorei
        set-up - refer to comments, 
       in code sample, race_updated_sem.c - 
       this semaphore idi/handlei
        needs to passed
       to further operations on the semaphore objecti/
       semaphorei - for instance, this idi/handlei will 
       be passed to dec() and inc() operations
--->the KEY value will not be used, in dec() / inc() 
    operations. and system-call APIs.. - a KEY is not 
    a handle, but just an unique identity, but idi is 
    a handle to access a semaphore-objecti

--->let us refer to freehand_diagrams_21.12.2021.pdf/slides 
    28-30, once ?? creation of a semaphore objecti and 
    initialization of semaphorei, in a semaphore objecti
---->you may refer to man 2 semget, for more details 
     on the parameters, like KEY and id - typically, 
     KEY is passed, by us and id is generated, by the 
     system - internally, id can be a pointer to a
     an object or an 
     index into a table of objects - whereas, KEY is 
     an unique value stored,in a semaphore objecti - 
     however, we 
     may not see too many details, in these discussions..
---->this is one type of semaphore implementation - there
     are others, where the KEY and id may be replaced, with
     other parameters
 

  ---->let us initially, refer and use race_updated_sem.c , 
       and understand the comments
   
  --->also, refer to semt_1.c and other samples
                  -->refer to the samples, for 
                     code and comments 
                  -->a typical semaphore object 
                     manages a single semaphore 
                     only 
                  -->in a GPOS set-up, a typical 
                     semaphore object may manage
                     multiple semaphore instances
                   - meaning, we may set p2 to 
                     1(for a single semaphore instance)
                          and 2 or 3 or n to create and 
                          manage multiple semaphore 
                          instances - initially, let us 
                          manage a single semaphore and 
                          later we will use multiple semaphores
                       -->initially, let us understand
                          the working of a single semaphore
                          , in a semaphore object 
             -->if a semaphore object is successfully
                created, "an id is returned", which 
                will be "used as the handle to 
                the semaphore object/semaphores"
                 -->as part of the life-cycle of a 
                    semaphore, creation, initialization, 
                    dec/inc operations and deletions are 
                    done 
--->after understanding the following details, 
    again, refer to race_updated_sem.c and 
    semt_1.c 
--->once a semaphore objecti/semaphorei is created and 
    set-up, 
    we need to use semctl() to initialize 
    semaphore's/counter's initial value, in a semaphore-objecti  
                  -->semctl() is used to initialize a 
                     semaphorei, in a semaphore objecti :
                     -->ret = semctl(p1,p2,p3,p4); 
                     -->"p1 will be the idi/handlei" 
                        of the specific 
                        semaphore objecti - idi will be 
                        used, in many system call APIs 
                     -->based on the parameters, one or 
                        more semaphores of a semaphore-
                        objecti will be initialized
                     -->if a semaphore-objecti maintains
                        multiple semaphores, each semaphore
                        instance is identified, using an 
                        index, 0,1, 2,.........
                     -->where 0 represents the first 
                        semaphore-instance, in the semaphore-
                        objecti - similarly, 1, 2,3, ...,so on 
                        represent other semaphore instances, 
                        in the semaphore objecti
                     --->if a semaphore-objecti maintains a 
                         single semaphore instance/semaphorei, 
                         it is 
                         represented,  by index 0 only
                     --->p2 is typically index of a semaphore-instance  
                         to be accessed/initialized, 
                         in the semaphore-objecti - in the normal
                         case, it is 0 - in other cases, this value
                         can be 1 or 2, or some i, i>1 
                     --->p3 is the command to be passed - 
                         for instance, if SETVAL is passed, 
                         a particular semaphore 
                         instance's(p2) value is 
                         set to a value passed, by p4, which is 
                         an union object
---->of p2 is set to SETVAL , set the value of a semaphore/indexi(p2),
     in this semaphore objecti, using p4's val field
                     -->p4 is an union, which contains 
                        several fields - once such field 
                        is "val-field, which is relevant 
                        for SETVAL" - val-field of union/p4
                        is used, in this context  
                    -->so, if SETVAL is the command, 
                       based on p1(idi of the semaphore
                       objecti) and p2(index of a semaphore
                       instance), 
                       "a semaphore-instance's value will be set 
                       to that of val-field of p4" 
   -->we can initialize the initial value of a semaphore 
      instance,as per our application's requirements - 
      it can be 1 or 0, or a +ve number, but not -1/-ve  
                  -->in the case of a binary semaphore, 
                     the initial value will be 1,if it 
                     is used, as a lock mechanism
----->now, refer to semt_1.c or race_updated_sem.c ...

==============================================================
        ---->we will be using a binary semaphore as a lock 
             , in the next set of code samples  
                  -->in the case of a binary semaphore, 
                     the initial value will be set to 0, 
                     if it is used, as an explicit 
                     synchronization mechanism
        --->we will be using a binary semaphore/
            counting semaphore for 
            synchronization of two embedded tasks, in 
            embedded contexts 
============================================================
             
           -->in the case of a counting semaphore, 
              the initial value will be set to a 
               +ve value, value>=1, as per the application's 
              requirements
              --->when we use a counting-semaphore, 
                  as a binary-semaphore, initial-value
                  will be set to 1

---->refer to race_updated_sem.c, once more, for 
     semctl() ??

----->we will resume on 12.07.2021..at 10 am....

            -->semop() is another system-call API used
              to implememt decrement and increment 
              operations on a semaphorei of a semaphore-
              objecti - using a set of parameters, 
              we can do a decrement-operation on a
              semaphore-instancei of a semaphore-objecti
              - using another set of parameters, 
              we can do an increment operation on a
              semaphore-instancei of a semaphore-objecti  

---->in all these system-call APIs, there are user-space
     parameters, which are passed to system-call /service-
     routines, so do not confuse these parameters, with 
     system-space objects/tables....
 
          -->let us understand semop(p1,p2,p3) system 
             call API - semop(p1,p2,p3) is the system-call API, 
             for decrement and increment operations 
             on a semaphore instance, in a semaphore 
             object
                 -->p1 will be the idi/handlei of the semaphore 
                    objecti to be operated upon
                 -->p2 is a pointer to an array/table 
                    of objects, for operations on 
                    one or more semaphore instances  
                    of a semaphore-objecti -  
                    p2 points to an user-space object or table of
                    objects, 
                    which is not a system-space 
                    semaphore table of 
                    object(s) - 
                    p2 is just a pointer to a set of 
                    object(s), 
                    which contains parameters to 
                    the semop() - in our simple case, 
                    we just pass pointer to table of 
                    a single 
                    object - refer to race_updated_sem.c  
                 -->p3 is the number elements/objects, in 
                    the user-space array, which 
                    also, defines the no. of operations to be done, 
                    as per p2 - in fact, p3 will be 
                    the number of elements, in p2's 
                    array/table of objects - based on 
                    p2 and p3, semop() will do a 
                    "single operation on a semaphore instancei"
                    or  "multiple operations on 
                    multiple semaphore instances" - in our case, 
                    p3 will be just 1 - meaning, p2 
                    is just passing a single object element 
                    of operation - refer to race_updated_sem.c
    
                      -->ret = semop(p1,p2,p3);

                --->p1 is the idi of the semaphore objecti
                --->p2 is a pointer to an array of 
                    structure-objects/struct sembuf{} 
                    elements - each sembuf{} object 
                    describes an operation on a semaphore
                    instance/index - each element of p2's 
                    array passes parameters to the 
                    system call, for a semaphore-index's
                    operations - 
                    these parameters
                    are used, in the service routine
                    of the system call to operate on 
                    different semaphores of this semaphore objecti
                      
                 -->each sembuf{} object/element of p2's array
                    is of the following
                    type:
                    struct sembuf{}
                 -->field1 --->sem_num //index of a semaphorei
                 -->field2 --->sem_op  //decrement or inc operation
                 -->field3 --->sem_flg  //flags - initially, set it to 0 
                    field1 provides the semaphore
                    index, which will be decremented(-1)
                    or incremented(+1), as per field2 - 
                    field2 passes the operation to 
                    be done(-1 or +1)  - requested operation 
                    can be decrement or increment 
                    operation  
                   field3 provides flags - initially, 
                   let us set the flags to 0 - currently, 
                   we are not using flags - flags are
                   used to modify standard behaviour 
                   of this system call  - this 
                   field can be non-zero, for special 
                   scenarios, like non-blocking
                   operations - we will initially use 0, for
                   field3 
                -->initially, let us assume, that p2
                  contains a single element/object only 
                  ,meaning we are operating on a
                   single semaphore-instance of a semaphore-
                   objecti

--->now, let us understand the usage of semop(), in 
    race_updated_sem.c 
     --->understand the parameters of semop() and 
         how they are initialized ??
     --->test the usage of semop(), in 
         race_updated_sem.c 
--->now, switch to shared-memory APis and shared-memory 
    set-up and access - go to line nos.4973

       we may come back some other time to touch 
       the following practical scenaios and code samples



       ---->you may also go through the following, 
            during the practicals 

--->during the testing, you may set the initial 
    value of a binary semaphore lock to 0, 1, or 
    2..justify the results....

---->in certain scenarios, we will be using 
     multiple semaphores, in a semaphore objecti
---->in addition, we will be operating on 
     multiple semaphores, simultaneously ... 
---->in certain scenarios, we may operate on 
     multiple semaphores of a semaphore object, 
     as application's requirements 
               -->p3 depends on p2 - meaning, the 
                  number operations requested by 
                  p2
              -->p3 is another parameter, which 
                 is used to set to the number of 
                 elements/operations used, in p2's array 
                 - in the simple case, this 
                  value will be set to 1, since we
                  are requesting a single operation only

---->after completing explicit, process shared-memory
     set-up, we will see certain examples, that 
     use multiple semaphores, in a semaphore objecti  

    - let us discuss the shared memory set-up and 
      system call APIs
----->we will be using race_updated.c  
     --> we need to use a set of system call APIs, 
         for "creating shared memory system objects"
         and "attach two or more processes to 
         to shared-memory system object(s)"
    --->in this context, shared memory set-up is , for 
        two processes 
    --->in addition, there are "different shared- memory 
        scenarios", based on the "parent-child, or unrelated 
        processes, in GPOS" --->coding will be different
        --->in this context, we are using shared-memory
            object/segments, in a parent-child set-up
    --->in addition, there are more scenarios, in 
        assignments  
---->during practicals, there are multiple scenarios:
      --->related processes(parent/children/siblings) and 
          their shared-memory objects/
          shared virtual segments
              ---->race_updated.c scenario  
     ---->unrelated processes(not parent/children/siblings) 
          and their shared-memory objects/shared virtual pages
             ---->one of these is , in the assignments ...
             --->this is case of different processes, 
                 and their applications/programs..
 
    -->for any IPC mechanism, we need a 
       system object to be used - this 
       system object will be part of the 
       frame-work of the IPC mechanism
    -->one or more processes will be 
       associated, with appropriate 
       system objects of IPCs - all these
       are part of the set-up of frame-works
       of core components, like OS ... 
    -->in this context, we will be dealing, with 
       shared-memory IPC objects  

---->first break on 12.07.2021..

---->lunch-break on 26.12.2021...at 1.25 pm.

--->in the following code-sample, a shared-memory is
    set-up in a parent-process and this shared-memory 
    is shared with a child-process - after fork(), 
    shared-memory virtual-segments of the parent-processi 
    are duplicated for the child process - in addition, 
    their mappings will  be shared, so effectively, 
    parent and child will be sharing this shared-memory 
    region - we need to understand coding and 
    internal set-up details ?? 
--->along with system-call APIs and race_updated_sem.c/comments, 
    refer to free-hand diagrams/slides 57 onwards... 
    of shared-memory IPC set-up
--->slides-57-58 :
    -->typically, if a processi invokes shmget(KEY1, sizei, flags);,
       a shared-memory object set-up is created in system-space
       and an idi is returned 
    -->typically, if a processj invokes shmget(KEY1, sizei, flags);,
       a shared-memory object set-up is not created, but 
       idi of existing shared-memory object set-up is 
       returned
    -->however, if Pi is a parent-process and Pi+1 is a
       child-process of Pi, we need not invoke 
       shmget() in child-process, since a copy of id variable 
       of parent process is given to child-processi+1 
--->slides-59-61 :
    --->if shmat(idi, 0,0); is invoked in a processi/Pi, 
        VASi is transformed into VASi' , along with a 
        new shared virtual-segment
    --->as part of this, there will be a new VASk set-up 
        and added to pd's address-descriptors - in addition, 
        shared-memory objecti/idi are linked to 
        VADk   

    --->if shmat(idi, 0,0); is invoked in a processj/Pj, 
        VASj is transformed into VASj' , along with a 
        new shared virtual-segment
    --->as part of this, there will be a new VADk set-up 
        and added to pd's address-descriptors - in addition, 
        shared-memory objecti/idi are linked to 
        VADk  

    -->however, if Pi is a parent-process and Pi+1 is a
       child-process of Pi, we need not invoke 
       shmat() in child-process, since a copy of shared-segments 
       of parent process is given to child-processi+1  
       --->so, in VASi+1 will be a copy of VASi'
       --->so, in system-space, a VADk' is provided to 
           pdi+1 
--->refer to slides 57-62 of freehand_diagrams_21.12.2021.pdf
    --->specifically, a copy of set-up of slide-60 is given to 
        child-processi+1

--->now, read and interpret slides 61 - 65
    --->in slide-62, Pj's set-up is illustrated - same
        applies to Pi+1,if Pi+1 is a child-process of 
        Pi 
    --->in addition, the same set-up applies to Pi - in fact, 
        Pi is the first process to set-up a shared-memory 
        IPC resource 
    --->now, continue and read the comments in the above 
        slides
        -->what does shmat() return in Pi or Pi+1, or 
           Pj ??
           -->starting virtual-address of shared-memory 
              segment 
           -->we can use shared-memory segment's addresses
              from the return value ??
              -->refer to race_updated_sem.c
        --->how do we access and use shared-memory 
            segments in Pi and Pj/Pi+1 ??
            -->store the above return value in 
               a pointer-variable 
            -->using this pointer-variable, we can 
               access the entire shared-memory segment 
               of  processi/processj/processi+1    
              -->refer to race_updated_sem.c

--->we will resume at 3.00 pm on 02.06.2022....
 
    - what happens, if a typical shared 
    memory system call API is  invoked, in a process ??? 
               -->id = shmget(p1, p2, p3); 
                -->p1 is the unique KEY value for creating a 
                  "new shared memory objecti" and "it will 
                   be used further, for setting up shared-memory 
                  segments, in two or more processes"
                --->p1 is used to set-up a shared-memory 
                    objecti, in system-space
                --->we will be invoking shmget() several-times, 
                    in different processes, using the same 
                    KEY value, so that, different processes
                    can use a common, shared-memory objecti's
                    set-up 
    -->p1 must be an unique-number --> it is similar
      to keys used, for semaphores or message queue objects, 
      but the scope is different - there will be one 
      range/set of KEYs, for semaphore objects, and another
      range/set of KEYs, for shared-memory objects... 
    -->p2 is sizei of the shared-memory segment to 
       be set-up,for this shared-memory 
       object -  as per rules, 
       p2 must be a multiple of 
       page-size, not sub-page size(not smaller, than a 
       page-size)  - even if 
       we provide a sub-page size, the system 
       will allocate, in multiples of page-size -
       so, do not pass sub-page size to p2 - only 
       a multiple of page-size 

    -->p3 is the flags, for setting-up a shared-
       memory segment - we will be using a 
       default set of flags - flags are used
       to provide appropriate access permissions
       to a shared-memory set-up/object / segments
       --->refer to race_updated_sem.c  

    -->return value will be the idi/handlei of the
       shared-memory objecti set-up, in the 
       system-space - this handle is exported
       to user-space - this idi/handlei will 
       be  used, with other system-call APIs of
       shared-memory set-up 
   
   --->if shmget() is successful, "it will set-up a 
       shared-memory objecti and initialize, related-fields", in 
       system-space and "return a shared-memory objecti's 
       idi/handlei", but will 
       "not set-up any user-space, shared-memory
       segments", in the process address-space of 
       Pi - VASi will not be modified - refer to race_updated_sem.c and 
       free-hand diagrams/slides 
--->refer to slides 57-58 and 11-13 of 
    freehand_diagrams_21.12.2021.pdf 
---->resume at 3.00 pm...on 12.07.2021...

          -->shmget() does not set-up shared-memory 
             segments, but sets up shared-memory 
             objecti, in system-space- 
             meaning, the processes 
             cannot access shared-memory data, without 
             the support of shared-memory segments/virtual
             addresses of 
             process address-space, using a shared-memory 
             segmenti  - we need to invoke shmget(), in
             one or more processes, using the same key
             values...these processes are said to share
             a common, shared-memory objecti
             --->once a shared-memory objecti is set-up, 
             we need to invoke shmat() to set-up a 
             shared-memory segmenti, for one or more 
             processes,which are using this 
             shared-memory objecti - 
             we need to invoke shmat() 
             multiple times, in each process, using 
             this shared-memory objecti, for IPC  data-exchange
         --->next, we need to invoke shmat(), 
             in Pi - similarly, in Pj and other 
             related-processes   

     --->refer to freehand_diagrams_21.12.2021.pdf/
         slides 59-65 
         -->using shmat() system-call API, we can set-up 
            a shared-memory segmenti, in a processi/pdi, 
            which will be connected to a shared-memory objecti
                ---->this must be repeated, in every process
                     ,using this shared-memory objecti
            --->shmat() will create a new, virtual-segmenti/its 
                virtual-address descriptori, in the pdi and this 
                virtual-address descriptori, 
                will be connected to the current, shared-memory objecti
                 --->idi of shm-obji is used to connect 
                     VADk to shm-obji, by storing idi, in 
                     VADk ---> refer to slides 59-65 of 
                     freehand_diagrams_21.12.2021.pdf
            --->based on p1 and p2 passed to shmat(), 
                corresponding shared, virtual-segmenti and shared
                memory-objecti are connected 
               --->refer to race_updated_sem.c 
               --->read further details below ... 

        -->next, let us understand the working of shmat() and
           its parameters ??
           -->vaddressi = shmat(p1, p2, p3);
               -->p1 is the idi of the shared-memory 
                  objecti, which we have created 
               -->p2 is the value of the starting 
                  virtual-address, for shared
                  virtual-segmentk(VADk) to be created, in 
                  the VASi of the current, processi/pdi - we can select p2, 
                  based on our understanding of virtual 
                  address-space/VASi of Pi - if we set p2 to 0, 
                  OS/kernel will set-up a shared, virtual- 
                  segmenti, in an unused part of the 
                  virtual address-spacei/VASi of the current
                  processi - once a shared-segmenti is set-up, 
                  in VASi, it is transformed into VASi' -  
                  if successful, 
                  return value is the first, virtual-addressi of the 
                  newly set-up, shared, virtual-segmenti - "however, 
                  if we provide a non-zero value, 
                  the system will use this as the 
                  starting virtual-addressi of the shared, 
                  virtual-segmenti, if there is 
                  unused space, in the address-spacei -
                  if the specified segment is in use, 
                  error will be returned" - "preferred 
                  approach is to pass 0 to p2, and let the 
                  system set-up the shared-memory segmenti
                  and return the first, virtual-addressi"
           -->p3 is for flags and in this context, 
              we will pass 0, as default - check 
              man shmat(), for some more details 
              on flags - for normal requirements, 
              0 is sufficient 
        -->if shmat() is successful, for a given 
          scenario, a new shared virtual segmenti
            /VADi is set-up, in the pdi/processi - this 
          "shared-virtual segmenti/VADi" is linked 
           /connected to the shared-memory objecti 
           of "p1", using idi of the shared-memory objecti - 
           the return value will be the 
           starting VAi of system allocated shared
           virtual segmenti - as part of shmat(), 
           a new virtual address-space segmenti 
           is set-up, using a new address descriptori-
           shared memory objecti's id is stored, in 
           VADi of pdi  - 
           this forms a connection between this 
           Virtual address-descriptori of the shared virtual 
           segmenti
           and the corresponding shared-memory objecti, in 
           pdi of Pi 
---->refer to a free-hand diagrams/slides 59-61 illustrating 
     shmat(), in a process, Pi ??
     --->connect the details and visualize ??
     --->based on the above points, what do you visualize
         and connect, so far ??

---->third-break....on 26.12.2021....

--->let us go back to race_updated_sem.c and come back ??
--->refer to shm_maps_1.txt
     --->refer to shared data access code and
         read the following discussions ??
     --->use cat  /proc/<pid>/maps of processi of 
         ./race_u_s and look for a shared virtual 
         segmenti ??
         --->if you see letter p, in permissions
             field, the virtual segment is 
             a private virtual segment of th e
             processi and its mappings are private, as well
         --->if you see letter s, in permissions
             field, the virtual segment is 
             a private virtual segment of the
             processi , but its mappings are common/
             shared with one or more processes, using 
             a common shared-memory objecti
   --->since race_updated_sem.c has a parent and its
       child share a common shared-memory object and 
       their respective shared-memory segments, 
       we will list /proc/<pid>/maps , for 
       parent and its child .....
   --->in this special case, fork() will do a duplication 
       of shared-memory segmenti and connection to 
       shared-memory objecti, for the child - so , 
       we need not invoke shmget() and shmat() explicitly, 
       in the child .... 
---->print shma address value, as well as other locations
     , in shared-memory segment....
---->now, check these values against the range of 
     shared-memory segments, in parent and child, 
     using cat /proc/<parentpid>/maps and 
     cat /proc/<childpid>/maps/.....

---->read further and you will get more clarity, for 
     shared,  physical-memory mappings and other 
     scenarios.... 


----->second break....

----->we will resume on 13.07.2021, at 10 am...

---->assuming, the above system call APIs are 
     successful, in respective processes/active 
     programs(Pi/Ai and Pj/Aj), 
     following will be true :
 
          ->at the end of the above set-up, 
          a shared-memory objecti is set-up, in 
           the system - a shared-memory segmenti 
           is set-up, in the address-space
           of a processi - corresponding 
           to the shared-memory segmenti, a segment 
           descriptori is set-up, in system-space,
           for the specific processi - 
           "the segment descriptori stores the idi 
            of the shared-memory objecti" - this is a
            link, which is set-up between the 
            segment descriptori of a processi/pdi and 
            a common shared-memory objecti
---->use race_updated_sem.c and generate race_u_s
     next, 
     load ./race_u_s and check the cat /proc/<pid>/maps
     of parent and child processes - find the shared-memory 
     segments(with "s" attribute),in the access
     permissions field - in the shared-memory 
     segments, find the "idi/handlei" of the shared-memory 
     objecti - this will support the set-up, as discussed
     above  --->ignore some of the attributes, like
                deleted, since these are some of 
                the internal kernel mechanisms, which
                are hardly documented - in this context, 
                the shared-memory segment is not 
                deleted   
 --->check vai and shared-memory data addresses, 
     by printing, in race_updated_sem.c.  
        -->in addition,since the starting VAi of the 
           shared-memory segmenti of a processi is 
           returned, we can access the shared-memory 
           segmenti of this processi, in our code - 
           using these
           virtual addresses, we can access user-space 
           objects/fields  resident, in this 
           shared-memory segmenti - we can code and 
           access contents, in a shared memory 
           segmenti 

       -----> we still need to 
           understand the following low-level 
           set-up :
--->refer to the free-hand diagram of a shared-memory 
    object and its fields :
    --->refer to slides 11-13 and 60 and 62
            -->as part of the shared-memory objecti, 
               there is a table/array of 
               shared page-frames'
               physical addresses - 
               meaning, this array/table will 
               maintain physical addresses of 
               shared page-frames of this 
               shared memory objecti - however, initially, 
               all these entries are set to NULL - 
               this means, initially, no mappings/
               shared page-frames are allocated  - 
               these are 
               not same, as regular page-tables/ptes 
            -->in addition,the virtual address-descriptor of 
               the shared virtual segment will be provided 
               its own ptes, in the page-table(s) of the 
               processi - these ptes are set to invalid state,
               initially - 
               meaning, there will no mappings,initially 
               -->so, shared, virtual-pages will not 
                  have mappings, initially 
  --->currently, this is the set-up of a shared-memory 
      objecti/segmenti/page-tables/ptes, for a 
      specific processi, in race_updated_sem.c 

---->again, refer to slides 11-13 of freehand_diagrams_21.12.2021.pdf
     --->provides a big-picture of a shared-memory set-up of 
         segments of processes and respective, page-tables/ptes
--->refer to slides 57-65 of freehand_diagrams_21.12.2021.pdf
    ---->connect all the details and understand ?? 

 
---->based on the above set-up of different programs, 
     in different processes, understand the following 
     descriptions :
       
          -->what happens, "if a processi/Pi is attempting 
             to access a virtual addressi/virtual pagei, 
             in a shared virtual segmenti of 
             its address-space/VASi" ???
            -->since there is no mapping, for the 
               shared virtual pages of shared segment(s), 
               initially, there will be a page-fault
               exception - this fault exception will be
               handled by page-fault, exception handler and the 
               exception handler will check the shared
               page-frames tablei maintained, in the 
                shared-memory objecti to extract 
                a shared page-framei - this is part 
                of the set-up and mechanism  
             -->since the shared-memory objecti 
                is linked to the shared segment's 
                descriptori, the shared-memory 
                objecti is accessed and eventually, 
                the shared page-frames' tablei is 
                accessed --->these operations are 
                done  by the fault exception handler - 
                this is not same, as a regular 
                page-fault handling operations 
            -->since these page-frames are initially 
               unallocated, a new page-framei will 
               be allocated using physical memory 
               manager - the base physical address of the 
               page-frame is stored, in one of the 
               elements of the shared page-frames' 
               array/tablei of shared-memory objecti 
           --->similarly, this base physical address of 
               a new page-framei is 
               used to set-up the actual pte of 
               the faulting shared 
               virtual-pagei - these actions 
               are, in a pte of a page-table of 
               processi - some 
               of these actions are peculiar
               to shared-memory segments/pages/
               ptes 
           -->other fields of pte are set-up, 
              as per shared virtual segment's 
              characteristics/attributes - for 
              instance, r and w permissions will be
              set-up - in certain cases, a shared-memory 
              segmenti/pages of a processi can be 
              read-only...           

           -->eventually, it is like any other
              page-fault handling,  but the 
             shared memory objecti and its 
             shared page-frames' arrayi are
             used, for intermediate operations 


   -->read the above steps again and understand, 
      that shared page-frames are allocated and 
      mapped and in addition, the base addresses of 
      these shared page-frames are also stored, in 
      a  arrayi of a shared-memory objecti - this is 
      needed, for further processes, that will 
      be using the same shared-memory objecti  
    --->keep connecting to race_updated_sem.c
    --->keep connecting to prod_1.c and cons_1.c
--->now, go to line nos.5242......??? 
                      
---->so far, are you able to connect to most of the 
     above background details ???


    --->keep connecting to race_updated_sem.c 
 
        -->let us assume that this parent process of 
           race_updated_sem.c 
           invokes a fork() system call API-
           following actions are taken:

        -->private virtual segments(containing
           private virtual pages)  of the 
           parent processi are duplicated for
           the child processi+1 - after duplication, 
           these are maintained as private
           virtual segments(virtual pages), in the child processi+1
           --->these segments/pages will be 
               assigned private page-frames/mappings

        -->shared virtual segments of the 
           parent processi are duplicated
          for the child processi+1 - as part of 
         this, the duplicated shared segments
         are maintained, as shared virtual segments/shared
         virtual pages
         ---->what is the consequence of shared
              segments /shared virtual pages, in 
              the context of parent/children processes ??
              --->their page-frames will be 
                  shared page-frames provided by 
                  the respective shared-memory objecti 

      -->effectively, parent and children 
         processes(Pi and Pj) will be sharing a common 
        shared-memory objecti ??
    --->since the shared-memory segments of parent 
        and child are sharing a common shared
        memory objecti, they will sharing  a 
        common set of page-frames/mappings

----->you can again refer to one of the initial
      shared-memory free-diagrams , in IPC slides
      and connect the set-up to a parent process 
      and its child process 

    -->based on the above set-up, we can 
       conclude,using the following illustrations
       --->in the following illustrations, there are 
           several logical connections, so interpret 
           , accordingly 

       - connect all the details :
  --->a shared, virtual segment and its mappings
     Pi(parent)-->VASi--->a shared-virtual-segmenti-->page-table/ptes
                                 |                      |
                                 |                      |
                                idi->shared-mem-obj->shared-page-
                                 |                   arrayj
                                 |                      |
                                 |                      | 
     Pi+1(child)-->VASi+1--->shared-virtual-segmenti'->page-table/ptes
    --->keep connecting to race_updated_sem.c
        
   ---->in the case of private segment/mappings
         Pi(parent)-->VASi--->a private-segmenti-->p.table/ptes
           |                                        |                
           |                                        +---->private page
           |                                              frames 
           |                                         
         Pi+1(child)-->VASi+1--->a private-segmenti'-->p.table/ptes
                                                          |
                                                          +--->private
                                                           page frames
         
             -->as per the above diagrams/set-up, can we
                conclude that the shared-segments of 
                parent process and child process map to 
                a common set of page-frames, as per the
                shared-memory objecti and its shared page-frames'
                arrayi - YES, it is TRUE

             -->if the above set-up is true, what happens, if 
                the shared virtual segment and its contents 
                are accessed, in the child process, after
                fork() ???
                    --->analyze, in the context of child process??
                         -->effectively, a shared-set of 
                            page-frames are accessed, in 
                            the child processi+1 - basically, 
                            the shared-virtual-segmenti 
                            enables access to 
                            shared-data, in the user-space - 
                            in the back-ground, 
                            the shared-segmenti/shared-data are 
                            mapped/loaded ,to a set of shared
                            page-frames  --->refer to the 
                            above diagrams, which describe 
                            a shared-segment's set-up 
---->now, let us get back to the code and 
     complete ---> race_updated_sem.c  

       ---->go through the demos and interpret the 
            behaviour ??
       ---->use cat /proc/<pid>/maps to check the 
            high-level understanding of the set-up ??
----->you can also use race_updated_sem.c , along 
      with semaphore....
---->compile and test the above code samples....
    --->current system call APIs and their features
        are relevant to GPOS systems                  

           -->assuming we are operating on a 
              single semaphore of a semaphore 
              object, we can implement a 
              decrement operation, using semop()
              and we can implement an increment 
                        operation, using semop() - 
                        accordingly, the parameters have to 
                        be modified - refer to the 
                        sample codes

                       -->semt_1.c 
                   
    ---->many of these samples use multiple-semaphores
         and related operations - counting-semaphores
         are used, as well...
    ---->also, these code-samples use separate-programs, in separate- 
         processes 
    ---->still, shared-memory segment(s) are used, 
         between processes 
                       -->prod_1.c and cons_1.c
                       -->prod_1_1.c and cons_1_1.c
                       -->prod_1_2.c and cons_1_2.c 
        --->in many of the above samples, binary and 
            counting semaphores are used - typically, 
            "a binary semaphore is used, for locking 
            critical-sections" and "counting-semaphores
            are used, for counting/managing resources/events" 
        --->in the context of embedded, counting- 
            semaphores are also used to "count 
            multiple-events" - so, in embedded, we 
            will come across event-counting, in many 
            scenarios....
--->we will see many practical use-cases, in RTOS 
    environments - still, let us understand the 
    basic usage, in GPOS-systems.....
---->let us quickly refer to rt_e_concepts.pdf and section-15.6
     --->provides typical application design  patterns, 
         using IPCs, like semaphores....
     --->we will revisit these, during our RTOS sessions, 
         again...
---->prod_1.c/cons_1.c is a simpler version of 
     prod_1_1.c/cons_1_1.c , so first, complete prod_1.c, 
     then use prod_1_1.c
---->similarly, prod_1_2.c and cons_1_2.c are more 
     sophisticated and efficient
--->revisit this section again, for code 
    walk through...??
      --->refer to prod_1.c and prod_1_1.c, for initializing 
          all the semaphore instances, using 
          a single system call API , semctl() and 
          SETALL command
      --->in the same code sample, refer to 
          semop() and its operations on 
          multiple semaphore instances - there
          are scenarios        
                    
===============================================================                                    
  --->most of the basics remain the same, but 
      programming differences may exist , like 
      using system call APIs and their parameters       
   -->in most multi-tasking applications, some form 
      of IPC(s) are used - in addition, there are typical 
      designs, for using IPC(s), in application's 
      processes/threads/other entities - 
      one such common design/model is 
      known as "producer-consumer concurrent programming 
      model/design"
        -->this is true, for GPOS systems/applications
        -->also true for RTOS/EOS systems/applications 
             -->coding/implementation details may differ
             -->there can be low-level details involved
=============================================================

        -->in the case of GPOS, we will be using 
           prod_1.c , prod_1_1.c and prod_1_2.c, 
           along with cons_1.c, cons_1_1.c, and 
                                        cons_1_2.c 

       -->to start with, let us understand "prod_1.c and
                                           prod_1_2.c"
  --->revist, during code walk through ??

    --->initially, let us understand shared-memory 
        set-up, in detail
        --->in this context, producer/prod_1.c and
            consumer/cons_1.c are unrelated 
            processes, meaning not parent-child set-up
            --->this is a different-scenario....

---->now, again refer to slides 66-
---->in these scenarios, we are dealing , with 
     unrelated processes and their shared-memory 
     mappings...
     --->refer to assignment 4, as we well as 
         code-samples, prod_1_1.c /prod_1_2.c 
         and cons_1_1.c / cons_1_2.c
    ---->the basic set-up remains the same, 
         but certain programming issues, will 
         differ ?? 
--->refer to slides 66-71 of freehand_diagrams_21.12.2021.pdf
    --->a shared-memory object and its segments are used to
        set-up an user-space PIPE-IPC mechanism 
    --->programming-models, for semaphores is also, 
        illustrated 
    --->we need to connect the above slides and code-samples,
        below ?? 
---->we will resume on 27.12.2021....at 10.30 am


  --->refer to prod_1.c and cons_1.c
      --->prod_1 will be loaded, in a Pi
      --->cons_1 will be loaded, in a Pi+1/Pj

  --->refer to prod_1_1.c and cons_1_1.c
      --->prod_1_1 will be loaded, in a Pi
      --->cons_1_1 will be loaded, in a Pi+1/Pj



  --->refer to prod_1_2.c and cons_1_2.c
      --->prod_1_2 will be loaded, in a Pi
      --->cons_1_2 will be loaded, in a Pi+1/Pj
--->switch to prod_1_1.c and cons_1_1.c and come back.... 
    --->also, refer to freehand_diagrams_21.12.2021.pdf/, 
        slides....67 onwards....

        --->so, we need to connect to shared-memory 
            object explicitly, in producer, as well
            as consumer, using system call APIs
            --->in  unrelated processes, we need to 
                set-up the shm. object and shm. 
                segments, explicitly 
                 --->prod_*.c and cons_*.c
                     --->prod_1.c and cons_1.c is 
                         a set of programs/processes
      --->first, understand shared-memory set-up, in 
          producer and consumer programs... 
============================================================== 
            --->however, in the case of parent/
                children processes, such a set-up 
                may be done by fork(), implicitly -
                this was done, in race_updated_sem.c  
================================================================
        --->we need to invoke shmget() , in producer, 
            as well as consumer - if it is already created, 
            in the producer, it will not be 
            created, in the consumer, but the same 
            shm. id /handle is returned to the consumer's
            code - 
            producer and consumer will use a common 
            shm. KEY value - these are responsibilies
            of developers/programmers 

        --->shmat() needs to be used, in producer 
            and consumer, explicitly 
            -->in each process, a separate shared segment 
               will be created and managed 
            --->in each process, the respective 
                shared memory segment will be 
                forced to point to the same 
                shared-memory object 
            --->in each process, virtual addresses
                assigned to shared segments will 
                be different, but their mappings/ptes 
                will be pointing to the same set of
                shared page-frames 
            --->refer to prod_1_1.c  and cons_1_1.c, 
                which 
                are part of a single application... 

        --->we must do any initialization of 
            shared-data, or shared-resource 
            only, once, for our concurrent, application/
            programs
            --->in this context, if initialization of 
                semaphores/shared-memory regions/their
                data  
                is done, in producer, we must not 
                do it, in the consumer
---->some of these are very common mistakes, in 
     concurrent-programming...please check...
            --->which means, if we need to initialize
                semaphores and shared-memory data, 
                we need to do it only, once, in the 
                producer or consumer process, or 
                a common initialization code, which 
                executes, before producer or consumer process - 
                it is project dependent .... 
            ---->there are different, coding-scenarios, but
                 rules are common... 
=================================================================
       -->such shared-data or shared-resource 
          initialization problems are common, 
          in embedded contexts, as well 
           ---->along with issues of OS mechanism, we will 
                be facing initialization of 
                hw buses/hw peripherals, as well  
===================================================================
 
       -->in these scenarios/code-samples, we will be using 
          counting and binary semaphores, for 
          application's requirements 

       -->a binary semaphore is typically used to 
          protect/critical sections, for mutually 
          exclusive access to shared-data/shared-resource
         -->connect the usage of a binary semaphore
            to the codes of producer/consumer, prod_1_1.c/
            cons_1_1.c and race_updated_sem.c
---->also, refer to slides 68-71 of freehand_diagrams_21.12.2021.pdf
     --->refer to slide 68 and make the following assumptions:
         -->ignore index=0 and index=2 semaphores 
         -->only index=1 semaphore is being used, as 
            a binary-semaphore
         -->now, refer to prod_1_1.c and cons_1_1.c  
--->if producer process continues to be scheduled and
    executed, it may read user-input and fill 
    all the buffer-slots of pipe-IPC, so new data
    will dropped, so it is incorrect processing and 
    inefficient processing 

--->if consumer process continues to be scheduled and
    executed, it may read filled-data from 
    buffer-slots of pipe-IPC, if there is data -
    however, if there is no data in buffer-slots of 
    pipe-IPC,  it will just be checking for new-data and 
    inefficiently processing 

--->again, refer to slide-68, but consider semaphore-index=0
    and semaphore-index=2 
    -->semaphore-index=0 will be used as a counting 
       semaphore with an initial value set to 50(or another
       value)
       --->this semaphore-instance is used to count 
           free-slots in the IPC-buffer
       --->now, refer to prod_1_2.c
    -->semaphore-index=2 will be used as a counting 
       semaphore with an initial value set to 0
       --->this semaphore-instance is used to count 
           filled-slots in the IPC-buffer
       --->now, refer to cons_1_2.c


     --->also, refer to prod_1_2.c and cons_1_2.c ,
         which are extensions to prod_1.c and cons_1.c and 
         more efficient  
--->you can also, refer to chap8.pdf of charles crowley, 
    for certain producer/consumer design patterns...
       --->slides 35-38 and 41-44
---->again, refer to slides 68-71 of freehand_diagrams_21.12.2021.pdf
---->we will resume, at 2.20 pm on 13.07.2021....

       -->in addition, "one or more counting-semaphores"
          are used - typically, "a counting-semaphore 
          is used to count and manage a set of 
          identical-resources",
          or "count and manage a set of identical-events" - 
          in this producer/consumer, code-samples , 
          we will be "using counting-semaphores, for 
          counting and managing one or more identical 
          sets of buffers of user-space pipe-IPC(resources)" - 
          here, 
          "each identical-set of buffers is treated, as 
           a resource-set" 
            -->a set of free/empty buffer-slots is treated, 
               as one set of identical-resources - 
               these free-slots are, in the shared-memory 
               IPC set-up - initially, this will be 50, 
               in the case of our producer/consumer problem 
            -->a set of filled buffer-slots is treated, 
               as another set of identical-resources - these
               filled-slots are, in the shared-memory 
               IPC set-up - initially, the number of 
               filled-slots is 0, in this producer/consumer problem  
          --->again, connect to the code blocks of 
              producer/consumer, prod_1_1.c/cons_1_1.c and
              prod_1_2.c/cons_1_2.c

---->in all the above discussions and diagrams, we 
     have described an application design/model - 
     this design/model uses semaphores, in 
     programs, like producer/consumer programs 
     
---->why use a binary-semaphore, to lock and 
     protect shared-data access of user-space
     pipe-object and buffers ??
     ---->this is part of locking critical-sections, 
          in producer and consumer programs/processes
     ---->producer's CS11 and consumer's CS12 are 
          concurrently accessing a shared, IPC-objecti/
          bufferi, in a shared-memory segment, so 
          we need a binary-semaphore lock 
  
---->why use counting-semaphores to manage identical- 
     set of buffers, in producer/consumer programs ?? 
     why not just use some other
     status-variables/simple-counters ??
     ---->keep reading and connecting....

---->refer to prod_1_2.c and cons_1_2.c, for counting 
     semaphores and their usage - connect and come back...
----->also, refer to slides 68-71 of freehand_diagrams_21.12.2021.pdf
---->we will resume at 2.55 pm on 27.12.2021, after lunch 

        --->in these code-samples, we are using 
            two counting-semaphores, index=0 and index=2 :
            --->one is to manage free-slots
                --->initial-value will be set to 50, in 
                    this case 
            --->one is to manage filled-slots
                --->initial-value will be set to 0, 
                    in this case 
           

        -->when counting-semaphores are used to count 
           and manage resource instances, there is an 
           added-advantage of explicit-synchronization 
           /co-ordination offered, by a semaphore instance/
           its operations - 
           for instance, if a producer-process requests, 
           for a resource-instance(a free buffer-slot)
           and if there is a free buffer-slot resource-instance, 
           first, index=0 semaphore is decremented and 
           the a free buffer-slot is used - refer to 
           prod_1_2.c -   
           for instance, if a producer-process requests, 
           for a resource-instance(a free buffer-slot)
           and if there is no free buffer-slot resource-instance(no 
           free buffer slot), 
          the requesting producer-processi will be blocked, 
          by the 
          counting-semaphore,index=0, that is managing 
          the free buffer-slots - refer to prod_1_2.c - 
          so, a free buffer-slot is 
          used, after decrementing free-slots' counting- 
          semaphore - all these operations are part of  resource 
          allocation and synchronization, using a counting-semaphore 
              --->refer to producer code or consumer
                  code, prod_1_2.c and cons_1_2.c 
                  respectively
---->blocking a process, due to non-availability of
     resources is convenient, in multi-tasking 
     applications ...
----->such requirements are popular, in non-embedded, 
      as well as embedded applications....
        -->similarly, if a counting-semaphore is used 
           ,for managing resource-instances(free buffer-
           slots) and there is an 
           availability of a resource-instance(free buffer
           slot), a blocked producer 
           processi will be awakened/unblocked, as per the
           code-design and usage of counting-semaphores
             --->these are part of resource management and 
                 synchronization 
              --->refer to the code samples , prod_1_2.c/cons_1_2.c
---->also, refer to slides 68-71 of 
     freehand_diagrams_21.12.2021.pdf...
---->similarly, analyze and understand cons_1_2.c, and 
     its usage of another counting semaphore, to 
     manage filled-slots and related synchronization...

---->in addition to the above discussions, another 
     detailed set of discussions are provided below.....


----->initially, we just don't use any semaphore, in 
      prod_1.c /cons_1.c - no binary and counting semaphores...
      --->it is a simple, shared-memory IPC problem, using 
          an user-space pipe, in a shared-memory segment
      --->you can build and test prod_1.c and cons_1.c  

---->next, we we add a binary semaphore to prod_1_1.c/cons_1_1.c
     ---->the binary semaphore is used to protect/lock,
          critical sections of user-space pipe IPC buffer
     --->no counting semaphores.....
     --->what are the problems, in this design/model ??
         ---->test and fill the answers - complete 
              assignment 4, along with these issues ??

----->finally, we add two counting semaphores to prod_1_2.c
      /cons_1_2.c  - one binary semaphore + two counting semaphores

----->you need to test the above example, program pairs, in the same
      order and understand their behaviour ??

        -->to start with, let us create/initialize  and
           use a binary semphore, for protecting the 
           critical sections, in producer and consumer
             -->analyze the code and its usage
--->use the code samples and test the following 
    scenarios to check , if the behaviour is 
    as expected ??
       -->in the case of  prod_1.c/cons_1.c and prod_1_1.c/
          cons_1_2.c, if 
          we use a binary semaphore instance/
          second semaphore instance, for protecting 
          critical sections, along with  
          cons_1.c,it still leads to an inefficient 
          solution, where there will be busy waiting,
          in the consumer solution  - 
          cpu cycles will be used inefficiently - 
          in the producer solution, there will be 
          no busy waiting, but there may be loss 
          of data, in the producer,after all 
          the free-slots are filled  
          - meaning, the 
          solution is incomplete, for producer, 
          as well as consumer  
          --->prod_1_1.c and cons_1_1.c together
              can used, for understanding a
              binary semaphore's usage

       -->refer to prod_1_2.c and cons_1_2.c, where counting 
          semaphores are added to support resource management 
          of free buffer slots and filled buffer slots 
          and synchronization 
          --->in these code samples, counting semaphores 
              are used - there are two counting semaphores
              used  
          --->a counting semaphore can have a value of 
              0 or >=1 - the max. value, for a given 
              counting semaphore is dependent on 
              the application and the use of this 
              counting semaphore - refer to code samples
          --->in these code samples, counting semaphores
              are used, for counting resources - the 
              resources are free buffer slots, in the 
              user-space IPC object and filled buffer 
              slots, in the user-space IPC object 
          --->there is "one counting semaphore instance/
              first semaphore instance" 
              associated, 
              with "free buffer slots" and its "initial 
              value is set to the max. number slots", 
              in the IPC object's buffer  - it can 
              assume a minimum of 0 and a max. value 
              of the total no.of buffer slots 
          --->there is "another counting semaphore instance/
              third counting semaphore instance" 
              associated, 
              with "filled buffer slots" and its initial 
              value is set to 0  - it can 
              assume a minimum of 0 and a max. value 
              of the total no.of buffer slots 
  
          --->first, understand the initialization of 
              these counting semaphores, in the code 
              samples - refer to prod_1_2.c 
 
           -->one counting semaphore(first semaphore 
              instance, in a semaphore object) is used to manage 
              number of free slots/resources - if the 
              number of free slots/resources drops to 
              0, the producer process is blocked, until 
              one or more free slots are enabled/freed
              --->this is a form of synchronization/
                  co-ordination 
              --->in this context, this first semaphore 
                  instance, in the semaphore object is 
                  used, for counting resources, as well
                  as co-ordinating the execution of 
                  the producer - the other part of the 
                  co-ordination will be clear, in the 
                  consumer 

           -->another counting semaphore instance/
              third semaphore instance, in the semaphore
              object is used to manage 
              number of filled slots/resources - mainly 
              used, in the consumer - if the 
              number of filled slots/resources drops to 
              0, the consumer process is blocked, until 
              one or more filled slots are enabled/filled

----->now, refer to slides 68-71 of freehand_diagrams_21.12.2021.pdf
      and again, revise the following illustrations...

       -->as per producer/consumer design, we need to 
          create and use two counting semaphores, 
          appropriately 

       -->following will be the usage of semaphores, 
          in the producer :
--->we are only discussing the operations on 
    semaphores, not initialization 


       while(1){
              ...
                ...........
          dec(semaphore instance 0); //counting semaphore
                                     //for free slots
                                    //if all the free slots 
                                   //are used up, the counting 
                                   //semaphore's value will 
                                  //drop to 0 and the producer
                                  //process will be blocked-
                                 //this is a form of 
                                //synchronization 
            dec(semaphore instance 1); //binary semaphore
                                       //for protecting 
                                       //critical sections

                .....
                ........                   //critical section 
                ..............

            inc(semaphore instance 1);//leaving the critical 
                                      //section
            inc(semaphore instance 2);//increment the third
                                     //semaphore instance
                                     //increment the filled 
                                     //slots counting semaphore 
                                    //a free slot is used 
                                    /and transformed into a 
                                    //filled slot 

         }



       -->following will be the usage of semaphores, 
          in the consumer :

         while(1){
            ...
            ...........
          dec(semaphore instance 2); //counting semaphore
                                     //for filled slots
                                  //initially filled slots 
                                  //do not exist and 
                                  //semaphore's value will 
                                  //0 and the consumer
                                 //process will be blocked-
                                 //this is a form of 
                                 //synchronization - the 
                                //semaphore value can be 
                                //incremented by producer
                               //during run-time and again drop
                              //to 0  
           dec(semaphore instance 1); //binary semaphore
                                      //for protecting 
                                      //critical sections

                .....
                ........                   //critical section 
                ..............

           inc(semaphore instance 1);  //leaving the critical 
                                       //section 
           inc(semaphore instance 0);//increment the first
                                     //semaphore instance
                                    //increment the free 
                                    //slots counting semaphore 
                                   //a filled slot is used 
                                   //and transformed into a 
                                  //free slot 

               }


--->following a good summary of producer/consumer jobs, 
    along with semaphores' operations and different 
    scenarios :

 -->in the above set-up of producer/consumer jobs/processes, 
    the counting 
    semaphores enable blocking,as needed and enable 
    unblocking, as needed
    --->what happens, if producer process is scheduled
        /dispatched first, in the above set-up ??
          --->as per the above producer code, 
              free slots' semaphore will be decremented
              and critical section/shared-data/IPC 
              processing will be done - after the 
              processing, filled slots' semaphore will be 
              incremented 
              --->in addition, we need to use  a binary
              semaphore/lock, before accessing a critical 
              section/shared-data/IPC processing 
          --->eventually, all the filled slots will be 
              exhausted, so the producer process will be 
              blocked, in the wq of first semaphore instance 
              /index 0 
          -->since filled slots' semaphore is incremented
             , after a new data is added to IPC buffer 
             slot, consumer will be unblocked, if 
             it is in a blocked - otherwise, consumer
             will just be scheduled and dispatched 
          --->consumer will be scheduled/dispatched, 
              eventually 
          --->in the context of consumer job/process, 
              assuming there are filled slots, filled
              slots' semaphore will be decremented 
              and processing will be done - after the 
              processing, free slots' semaphore
              will be incremented 
          ---->before accessing shared-data/IPC, 
               binary/lock semaphore will be 
               decremented and again, incremented
               after critical section/IPC is processed 
        --->in the above discussions, we are assuming 
            simple concurrent executions, but there will 
            be other scenarios, as well  
--->we need to add other scheduling issues , as well
    as IO services, if any to these scenarios 

    --->what happens, if consumer process is scheduled
        /dispatched first, in the above set-up ??
          --->as per the above consumer code, 
              filled slots' semaphore will be decremented
              , but the current value will be 0,since 
              there is no data, in IPC   - the consumer 
              process will be blocked, in the 
              filled slots' semaphore's wq - this is 
              a form of synchronization - this 
              enables efficient execution of the 
              consumer job/process  
          --->eventually, the producer will be 
              scheduled/dispatched and filled 
              slots' semaphore will be incremented, 
              after data is filled into free slots - 
              as part of this, filled slots' 
              semaphore's inc. operation will 
              unblock the consumer process, since 
              it is blocked - these operations 
              are part of synchronization 

          --->some time, in the future, the consumer 
              process/job will be scheduled/dispatched 
              - it will decrement the filled slots' 
              semaphore and read a filled slot - 
              in addition, increment free slots' 
              semaphore, after the processing
--->binary/lock semaphore is used, as part of 
    critical section code/IPC processing   
          ->eventually, when all the filled slots 
            are used/read, filled slots' semaphore's
            value will drop to 0 - when the filled 
            slots' semaphore value drop's to 0, 
            the consumer will be blocked, again
            --->these operations are part of 
                synchronization 
 

         --->the above cycles will be repeated 

         --->you can analyze other scenarios and 
              understand 

         --->during testing of producer/consumer, 
             we can use scripts and in addition, 
             we can use uniprocessor / multiprocessor
             scenarios 
          
          
 -->the above set-up works efficiently , using counting 
    semaphores and their services 

 -->in the above case, if remove the counting semaphores, 
    the application will execute inefficiently, but do 
    the job - however, in the real-world applications, 
    it is unacceptable

===========================================================
----->we will see many synchronization scenarios, in 
      eos/rtos discussions - refer 2_rtos.txt and 
      related code samples.......
=========================================================== 

------>on 13.07.2021....
------>now, you can attempt assignment-4, along with this pdf 
       and assignment4_hints.txt ....

------>we will resume again, in the next schedule of lectures..


  -->the next discussion will be on message queues, 
     in Linux/GPOS :

       -->refer to sample codes - msg_server.c(master) / 
                                  msg_client.c(slave) 


 - refer to problem 4 of assignment 4 of EOS:
    -->based on the hints provided and using sched_yield(), 
       at appropriate points of the two unrelated processes, 
       and generate a dead-lock() - we need to acquire the 
       semaphores, in different orders, in different 
       processes/programs
    -->verify, using ps  and  top commands, that both the 
       processes are blocked and unable to progress ??

    -->next, change the design of the programs/processes, 
       such that semaphore locks are acquired, in the same 
       order, in both the programs/processes 
   -->now, try to generate a dead-lock scenario, using 
      sched_yield(), at appropriate points - ideally, dead-lock 
      should not be generated 
   -->in addition, you can change the design to include 
      a run-time check, as follows:
      -->if you are holding a semaphore lock or another 
         lock and attempting to acquire another semaphore 
         lock/another lock, first, unlock the current 
         semaphore and attempt to re-lock both the locks, again ??
          -->hint - to implement this, we may need to 
                    non-blocking dec/locking operations,
                    in the case of the second semaphore/
                    lock  
   -->refer to Crowley/chapter 8, for more details on 
      dead-lock scenarios and dead-lock prevention 
      techniques 

   -->also, refer to dead-lock scenarios and dead-lock 
      prevention techniques provided, in Linux Kernel Development, 
      3rd edition 

===============================================================================
--->we are covering the following parts on 02.09.2021, post-lunch 

------>read the following, after completing multi-threading txt files 


---->read the following, for understanding 
     threads and IPCs 

  - following are typical problems faced, in multi-threading 
    applications:
       -->conventional, shared-data/shared-resource 
          race-conditions - this is more, in the context 
          of conventional-threads, as they share most of the 
          data-segments and resources of the processi  
       -->there an indirect problem due to non-reentrant 
          methods/APIs/lib APIs, if used incorrectly, 
          in two or more threads 
            --->refer to 9_multitasking_multithreading.txt on 
                re-entrant/thread-safe methods/apis/libapis 

       -->due to the above reasons, conventional-threads are 
          said to be less robust - so, less reliable - 
          we must be aware - 
          so, applications must 
          be designed appropriately and tested thoroughly 
          , for these issues - more shared-data, so more 
          concurrency issues, like classical shared-data and reentrancy/
          non-reentrancy problems...
          --->so, less robust ---> so, less reliable
--->we must be aware and do the required design/programming/
    testing/verification ?? in short, we need to make 
    it reliable  

       --->in embedded contexts, shared hw resources, 
           like shared hw buses/hw peripherals is more common, 
           in multi-threaded, applications - some of these
           will be better covered, in eos/rtos platforms - 
           perspectives - 3 +  2 + 4 + 1

--->conventional Threads and IPCs
---->most of the topics discussed below are applicable to 
     unconventional multi-threading of eos/rtos, but there will be 
     implementation differences...we will see such details, 
     in rtos contexts....once we cover these details, 
     learning and working, in RTos systems is much 
     easier and faster
    
    -->in a conventional multi-threading,
       since most of the data-segements of a processi
       are shared among threads of that processi, it 
       is more convenient to exchange data between 
       threads of the same processi, using shared-data 
       segments of the process - these shared-data 
       segments are not explicit shared-data segments
---->refer to freehand_diagrams_25.08.2021.pdf.... 
       --->slides 17-31
       --->go through these slides and understand the 
           scenarios and comments 
       --->specifically, refer to slides - 19-21
           --->shared-data and shared-resource 
               problems are highlighted  
       --->slides 27 - 30
           --->specific to thread-mutex IPC and 
               related programming-models 
           --->slide 27
               --->there are threads of a processi 
               --->there can be shared-data and shared-resource
               --->if shared-data or shared-resource is 
                   being accessed, by two or more threads, 
                   we need some form of locking 
               --->in this context, a mutex-lock/a mutex-semaphore
                   is present, in the same shared-data region
                   of threads - so, a mutex-object is present 
                   , in the same shared-data region 
               --->also, there are operations/apis used to 
                   lock a mutex-m1 and unlock the same mutex-m1
               --->all the other details are presented below - 
                   refer to slides 27-30, as we discuss more ??
=======================================================================
       -->there is lesser overhead, since most of the data-
          exchange operations  and locking-operations 
          are done, in the user-space
          , without system call APIs - improves performance 
          of threads, in an applicationi...this is related to 
          threads' data-exchange and related locking  

    -->- so, the performance
       of an applicationi, using multiple, conventional-threads 
       is better, than an applicationj using multiple 
       processes - these are different forms of 
       multi-tasking, which can be used, for our 
       applications - "in addition, performance is 
       also dependent on processor's power and other
       resources, like memory and IO" - so, overall 
       performance depends on other factors, like total 
       physical memory and IO operations/speeds of these
       operations -  
       however, typical 
       shared-data problems/race-conditions still exist 
       and need to be resolved, using appropriate locking 
       techniques - meaning, multiple threads may access
       shared-data resident, in shared-data segments - 
       basic principles of shared-data/
       race-conditions/locking apply, but "certain
       design and implementation details differ"
==========================================================================

 --->IPC mechanisms are implemented differently, in multi-threading 
     sw models  
    -->since data-segments are easily shared among threads, 
       IPC objects/ buffers can be placed, in the user-space, 
       implicit, 
       shared-data segments - this means, their access can be 
       faster and efficient, with minimal system-call APIs
        -->in the case of conventional-threads, shared-data 
           segments are set-up, by default, implicitly  
        -->in the case of multiple processes, shared-data segments 
           are set-up explicitly , as needed - we need 
           extra system call APIs and additional, address-space
           segments
        --->in the case of unconventional, multi-threading, 
            refer to 2_rtos_updated.txt ?? 

--->if you refer to message queues and pipes, in this 
    document, at the top/beginning, system-space pipes and 
    message queues are discussed - we cannot use these
    , for threads of a processi - if we use these 
    regular IPCs, performance will be sub-optimal, due to
    overheads
     --->why are these regular(process) IPCs present, in 
         system-space ?? why not, in user-space ??
          --->since processes have private address-spaces, 
              there are no shared-data segments, normally - 
              so, IPC objects and their buffers are maintained, 
              in system-space, so that they can be shared and
              accessed - for access, we use system call APIs  

    -->it is inefficient to use system-space pipes/system-space
       message queues, for threads of the same processi - 
       meaning,there will too many system call APIs and 
       exchange of data, with system-space - such overheads
       can be easily avoided, using user-space pipes/
       user-space message queues - in Linux like systems, 
       we need to explicitly set-up user-space message-queues 
       and pipe mechanisms, if needed  - 
       in the context of multi-threading,
       we need to set-up 
       these IPC objects/buffers, in shared-data 
       segments, in user-space - it will be faster and 
       efficient
           --->however, developer needs to set-up these
               user-space, IPC-data-exchange 
               mechanisms and manage them - very little 
               help is provided by GPOS operating-systems
           --->however, in the case of RTOS/embeddedOS 
               contexts, there is better support from 
               these OS platforms, for multi-threading development and 
               ipc mechanisms...better apis and programming 
               support 
               --->however, these platforms support 
                   unconventional, multi-threading
          ---->however, basics remain the same  

--->refer to thread_mutex_cond.c, 02_switch_LED_brightness_Queue_final.c,
    shared_LEDs_Mutex.c, and prodcons_threads1.c, 
    for typical user-space data-exchange mechanisms...
    --->thread_mutex_cond.c 
          --->here, shared-data access is being managed, using 
              a mutex-lock 
    --->02_switch_LED_brightness_Queue_final.c 
        ---->this uses shared-data for a message-queue 
        ---->in addition, uses mutex-lock, for 
             shared-data access
    ---->shared_LEDs_Mutex.c
         --->in this context, a set of LEDs are shared among 
             two threads
             --->while one thread is accessing this set of 
                 LEDs, it needs to lock, using a mutex-lock
             --->same applies to another thread 



--->in this document, we are restricting our discussions
    mostly, to Linux conventional threads and 
    supported IPC mechanisms - typically,
    GPOS systems...following descriptions are for GPOS systems 
    -->in addition, we may need locks and synchronization mechanisms
       to manage these user-space data-exchange IPCs , for threads
        --->there are no explicit pipes/mqs, for threads, 
            in Linux GPOS - but other IPC mechanisms, like 
            thread binary/counting semaphores/mutexes are explicitly, 
            provided
        -->there are thread "semaphores" and "mutexes" - these 
           are lightweight and more efficient, than 
           process semaphores/other mechanisms - the following 
           discussions explain the design differences

--->refer to thread_mutex_cond.c, 02_switch_LED_brightness_Queue_final.c,
    shared_LEDs_Mutex.c, and prodcons_threads1.c,for 
    managing and using thread semaphores and mutexes...
        -->objects of these thread-IPC mechanisms are located, 
           in user-space, but wq(s) of these thread-IPC mechanisms 
           are located, in system-space - applies to thread semaphores 
           and mutexes  
        -->the operations/service routines/library APIs 
           of these thread-IPCs do most of their operations, 
           in user-space and require limited no. of system- 
           call API invokations - most of these details are
           based on certain white-papers of core-developers
---->similarly, eos/rtos core-developers provide their 
     design/apis/coding of IPCs, in their eos/rtos manuals
     --->semaphore's design and usage will be explained  
     --->mutex's design and usage will be explained 

----->so, many of the thread APIs will do most of their 
      work, in user-space and limited usage of system call 
      APIs....
           --->there will be minimal overheads and better 
               performance... 
---->since there is implicit shared memory for threads of the same
      process, certain parts of 
      IPC objects can be resident, in user-space and 
      certain parts of IPC mechanisms will be resident, in system-space
         --->due this set-up , most of the IPC operations are 
             completed , in user-space 
         ---->a few operations will involve system call APIs.. 
      ---->more details follow below..... 
---->for different processes, typically, entire IPC objects
     are resident, in system-space ....
         ---->most of the operations require system call APIs ...  

      -->threads are light-weight and faster, 
           due to light weight 
           thread-IPCs, as well - this is one of the reasons, 
           for better performance using threads - this is 
           not real-time performance - there will be
           improvement, in performance 
---->if you refer to 9_multitasking_multithreading.txt, for 
     more details on light-weight processes/conventional
     threads and their set-up....

---->refer to slides 27-30 of freehand_diagrams_25.08.2021.pdf
-->first, let us understand a "thread-mutex" 
   --->in a typical, Linux-GPOS, thread-mutex is supported, 
       by the Linux thread-library and kernel-space - 
       thread-mutex apis are provided, by thread-library  
--->in a typical, GPOS context, we will use "thread-mutex"
    terminology, not thread-mutex-semaphore  

--->NOTE: in the context of EOS/RTos systems, there are
          mutexes, but their implementation details will 
          differ
--->NOTE: GPOS mutexes and RTOS mutexes are different, 
          but based on the principles a typical, OS-mutex
--->NOTE: in eos/rtos contexts, mutex-semaphore is a common
          terminology - however, we will understand and use it 
          , with care ??
--->NOTE: in a typical, OS-book, there is just an os-mutex - 
          it provides basic principles, for different 
          mutex implementations   

-->in a Linux platform/thread library, a thread-mutex object 
   is implemented, 
   as an abstract-object, but most of principles of 
   a typical OS-mutex are true - there is minimal 
   documentation on the thread-object - also, we can access
   the thread object, using thread-apis, only - 
   some of these connections will be clear, 
   when we see the 
   code/mutex operations   
        -->an OS-mutex is a lock mechanism, not a counting 
           mechanism, like an OS-semaphore - contrast this, 
		with an OS-semaphore - 
           however, an OS-semaphore instance is a counting mechanism, but 
           can used, as a lock mechanism - for instance, 
           a binary-semaphore instance can be used, as 
           a lock mechanism - 
           --->an OS-semaphore instance can be used, as a counting 
               semaphore instance 
           --->an OS-semaphore instance can be used, as a binary 
               semaphore instance, for locking   
           --->an OS-mutex instance can be used, as a lock only - 
               cannot be used, for other mechanisms, like 
               counting resources/events/other synchronizations
               --->we will see scenarios/code-samples below ??  
 
=====================================================================
---->let us refer to certain semaphores and mutexes of a typical 
     EOS/RTOS platform, for clarity and come back...
---->refer to 3_threadxug_g40c.pdf, page 68, for counting semaphores
---->refer to 3_threadxug_g40c.pdf, page 69, for binary semaphores vs
    counting semaphores 
---->refer to 3_threadxug_g40c.pdf, page 72-73, for mutex semaphores or 
     just mutexes....
---->the above is a typical user-manual, for developers, so will 
     not cover the basics of OS concepts and its mechanisms - we need
     to cover the basics of OS concepts and its mechanisms, before 
     we venture into such documentation....
--->also, you can refer to MasteringFreeRTOS...pdf ..., for 
    FreeRTOS semaphores and mutex ....
========================================================================

---->after completing the following discussions, you can come back 
     and again, read the above user-manuals....     
  
---->let us continue, with OS-mutex discussions - an OS-mutex 
     instance 
     will be allocated an objecti - this objecti will maintain 
     certain fields, as per the OS platform - in certain 
     certain OS platforms, mutex-objecti is  hidden and abstract..-
     in a typical GPOS system, os-mutex objects are abstract 
     entities - we can use them, using thread-apis, only , not 
     touch the fields ?? 
--->in certain OS platforms, you may be able to interpret the 
    design and implementation details....in many eos/RTOS systems, 
    certain details of os-mutex/os-semaphore are provided...
    --->still, we must use rtos-apis to access and 
        manipulate, not access their fields 
    --->however, design details are provided 

    -->a typical, os-mutex has two states, 
      locked-state and unlocked-state - a typical
      mutex-object does not support a counter field...
      no increment or decrement operations are 
      supported , for a mutex-instance
      -->typically, an OS-thread-mutex instance is 
         declared, in user-space, using an abstract 
         lib. data type - we will see Linux thread 
         mutex code, after this 
         discussion - there are code-sample below ??
--->refer to app_03_shared_LEDs_Mutex.c
----->just a recall a spinlock lock-mechanism ?? 
---->if a spinlock instancei is free/available,
     a requesting threadj of processi
     can immediately acquire the spinlock instancei
     --->there are only, two-states - free and busy  
----->if a spinlock is busy, a requesting threadj of processi
      will be forced to spin and wait, until the spinlock 
      instancei is free
---->there is no wait-queue, for a spin-lock 
------>for more details , read spinlock section at the top of
         this document ...
----->end of recall of spinlock lock-mechanism

        -->an OS mutex-lock is similar to an OS-spinlock, 
           but supports blocking, 
           using a wait-queue /Wq - spinlock is a busy-waiting 
           lock-mechanism - mutex is a blocking lock-mechanism
           and in addition, 
           a mutex will support more robust features - some of 
           these features will enable better dead-lock avoidance
           and avoidance of illegal mutex operations
       --->some of the features are covered below 
       --->we will see some more features, in the context 
           of rtos platforms...
---->similar to a spinlock mechanism, an OS mutex mechanism 
     supports two states, free/available and busy/unavailable
--->in the case of a binary-semaphore, the count value will be
    0 or 1 - now, see the difference
 
---->if an OS mutex-lock is available/free, a requesting threadj of
     processi
     can immediately acquire the mutex-lock and progress - 
     as part of acquiring a mutex-lock, 
     the current-state of this mutex-lock will be set to locked/busy
      state - a typical, os-mutex locking-operation - this is the 
     common scenario  

---->if an OS-mutex-lock is busy/unavailable, 
     a requesting threadj of a processi
     will be blocked and its tdj will be added to wq of this 
     mutex-lock instance - 
     so, there is a blocking operation of threadj....tdj's state will be
     set to blocked-state and tdj will be added to a wq of 
     this mutex-lock instance - a typical, os-mutex locking operation, 
     but a different scenario ?? 

---->an OS-mutex-lock also, supports ownership information 
     maintained in its  mutex-object, as per its design 
           -->if a mutex-lock is currently locked, by a threadj/tdj, the 
              ownership information of the thread-handlej/threadj's id
              will be stored, in the corresponding mutex 
              objecti, as part of locking-operation, in 
              the background  - if a threadj does a locking 
              operation on an os-mutex, and does unlockinging
              operation, the ownership info. is deleted, in 
              the os-mutex - we will discuss more details on the 
              ownership below - this feature will be used, 
              in os-mutex operations, like locking/unlocking 
              --->such a feature is not supported, for 
                  a counting or a binary semphore - 
                  the designs are different
---->so, an os-mutex-lock is similar to a binary-semaphore, 
     but the above design details are different - so, 
     an os-mutex-lock is not a binary-semaphore 


--->some of these points can be used to differentiate mutex and 
    counting/binary semaphores  
              --->due to different designs, their usage 
                  is also different 
              ---->we must understand their details and 
                   use them, accordingly... 
----->we will resume from here on 03.09.2021....


  -->a typical, OS-mutex supports different attributes - we will discuss
    the attributes after the basic discussion of 
            a mutex - based on the settings of attributes, 
            the behaviour of mutex instances/operations 
            will be different - some more differences between 
            mutex and semaphores  
        -->an OS  mutex-lock supports 2 basic operations - 
           lock(&m1) and unlock(&m1) operations - these operations 
            are atomic, in their implementations - 
            m1 is a typical mutex object, in 
            user-space, in a GPOS ...(for rtos, refer to RTOS documents)
        -->when a mutexi is created/initialized, its state is 
           set to unlocked state
        --->we do not use dec()/inc() operations on a mutex                 
            
        -->let us understand a scenario of using an OS thread-mutex-lock,
           for protecting critical sections, in threads of 
           a specific processi - this is a scenario :

             +----->Ti----lock(&m1)---cs11----unlock(&m1)------->
             |                         |
             Pi     shared-data, in a shared-data seg., or a shared-resource
             |                           |
             +----->Ti+1----lock(&m1)---cs12----unlock(&m1)------>

---->also, refer to freehand_diagrams_25.08.2021.pdf/slide 27
     --->visualize and connect the details ??
--->lunch-break on 22.02.2022....

   --->in the above set-up, what happens, if threadi of processi has
       locked the mutex, m1 and is preempted, and further, if threadi+1
       attempts to lock the same mutex, m1 ?? threadi+1 will 
       be blocked, in the wq1 of mutex,m1 and scheduler will 
       be invoked - this is done, in lock(&m1) operation - visualize ??
   --->so, threadi+1 will be blocked - so, cs11 will not be 
       entered, while cs11 is active - visualize ?? 
   --->what do we achieve ?? in the near future, threadi will 
       be scheduled and dispatched - so, cs11 of threadi will be 
       resumed and processed, 
       atomically - once cs11 completes, threadi will unlock 
       the mutex, m1 - unlock(&m1) operation - 
       so, mutex-m1 will be unlocked and threadi+1 will  be unblocked and 
       added to an rq - this unlocking and unblocking will be 
       done, in unlock(&m1) - in the near future, 
       when threadi+1 will be scheduled and 
       dispatched, it will acquire the mutex-lock, m1 and 
       enter cs12 -----> progress....- visualize ??
   --->similarly, when threadi+1/cs12 is being processed, 
       it will be atomically processed, like cs11 - so, 
       if there is a preemption, in cs12 and threadi is 
       scheduled/dispatched, threadi will be blocked, in 
       lock(&m1) ---> will not enter cs11, while cs12 is 
       active....-visualize ?? 
--->the above is the basic working of a mutex-lock mechanism 
    and its operations....
--->so, using a mutex-lock and its operations, we are achieving
    mutually-exclusive executions of cs11 and cs12 - so, we call
    this is a form of mutual-exclusion mechanism, using 
    os-mutex - that is why, it is a mutex - a mutual-exclusion 
    mechanism, that will enforce mutually-exclusive 
    executions....  

       --->refer to code samples, using mutex, after covering 
           the below details.... 
             --->app_03_shared_LEDs_Mutex.c
                  --->refer to the thread-lib apis and comments 
             --->02_switch_LED_brightness_Queue_final.c
             --->thread_mutex_cond.c

---->first break on 03.09.2021....30 minutes...
--->thread-lib. mutex operations/apis 
---->typically, thread lib. APIs do certain user-space operations 
       and also invoke system-call APis, if required - certain 
      operations are completed , without invoking system-call APIs 
----->following are elaborate details of mutex operations....all the 
      below operations are done, in one or more threads of a processi
            -->in the above case, what happens, if a lock(&m1) operation
               is invoked and this mutex  lock,m1 is currently, in unlocked
               state ?? 
               --->using an atomic machine-instruction, 
                   the mutex-lock is acquired and the mutex-lock state 
                   is set to locked-state, in user-space - all these
                   operations are completed, in user-space - 
                   there is no 
                   system-call API - lock(&m1) returns success -
                   this is a light-weight operation ...
       --->refer to spinlock related discussions, for 
           understanding embedded/atomic machine instructions- 
           refer to spinlock discussions, at the top of this document
           --->basics of spinlock and semaphores, still apply 

      -->in the above scenario, what happens, if a lock(&m1) operation
         is invoked and this mutex-lock,m1, is currently ,  in locked
         state ??
               -->using  atomic-machine instructions, the lock 
                  is tested/not acquired, since it is already, 
                  in locked state- assuming, that some thread1 has already 
                  acquired the mutex-lock, m1 - in addition, as per the 
                  mutex lock operations, the currenti-thread(threadi+1) 
                  is blocked, 
                  in the wq of this mutex-lock,m1,  using a system-call API 
                  mutex-operations are blocking operations,"not 
                  spinning operations, like spinlock operations" - 
                  meaning, if an OS-mutex is, in locked state and 
       		  a threadi+1 attempts to acquire this mutex lock, 
                  threadi+1/its tdi+1 will be added to 
                  wq of this mutex-lock,m1- 
                  lock/unlock operations are typically, thread
                  library APIs and they invoke system call APIs, 
                  if  needed - in these cases, we do not directly 
                  interact with "system-call-APIs" - we invoke
                  thread lib. APIs, which in turn invoke 
                  system call APIs, if needed
=====================================================================
---->some of these operations and their apis will change, in 
     EOS and RTOS platforms, but most of the basics apply...
     --->certain programming techniques will change...
======================================================================
---->we should revise ipc related basics, at the top of this 
     document, along with respective freehand diagrams/slides
     --->many of the basics of ipcs for processes apply to 
         ipcs for threads....
 
---->typically, thread lib. APIs do certain user-space operations 
       and also invoke system-call APis, if required - certain 
      operations are completed , without invoking system call APIs 
            -->what happens, if an unlock(&m1) operation is invoked
               and the current-state of this mutex-lock, m1, is locked state
               and 
               there is no blocked-thread/tdi+1, in wq of this mutex, m1 ?
           -->unlock(&m1) operation will set the state of this
                  mutex-lock to unlocked- 
                  state and return success - this operation is done, in 
                  user-space, and no system-call API is invoked -
                  a light-weight, fast operation...    
             -->what happens, if an unlock(m1) operation is invoked
               and the current-state of this mutex-lock, m1, 
               is in locked state 
               and 
               there is a waiting,blocked thread,threadi+1/tdi+1
                for this mutex-lock, in wq of this mutex, m1 ?
               -->unlock(&m1) will  set the state of this mutex to unlocked 
                  state, unblock/awaken a blocked threadi+1 and 
                  return success - however, this will require 
                  a system-call API invokation - this operation 
                  has overheads - since wq is, in system-space - 
                  that is the design  
          --->we will see more scenarios, in the context of 
              embeddedOS/RTOS ....many usage-scenarios will be 
              presented and related programming-patterns will 
              be discussed 
 
---->the above descriptions are typical, operations and their 
     working on a mutex-semaphore - the term mutex-semaphore is 
     taken from embedded OS/rtos terminology - in addition, there are
     more features to mutexes...keep reading.. 
         -->there are different forms/types of mutex, based on different 
            attributes :
             -->a "normal-mutex", which does not do error checking 
                , in  lock()/unlock() operations - this is a form 
                of mutex, with certain characteristics  
             -->an "error-checking mutex", which does error checking 
                 , in lock()/unlock() operations - this is another 
                 form of mutex, with certain characteristics
             -->a "recursive-mutex", which does error checking 
                 , in  lock()/unlock() operations plus some 
                 more features - yet another form of mutex - 
                 another set of characteristics
--->such types and characteristics are not associated, with 
    binary and counting semaphores - another important 
    difference  

---->we need to understand these features/attributes, in the 
     context of "GPOS", as well as "EOS/rtos" contexts - in this 
     document, we focus on GPOS platforms - in another set of 
     documents, we will focus on EOS/RTOS....
     --->in GPOS systems, there is a certain degree of seriousness
      --->in eos/RTOS systems, there is more seriousness

---->refer to app_03_shared_LEDs_Mutex.c - this code 
     needs to run on bb-b, using an embedded-linux and
     appropriate set-up - in this code, 
     we are using a NORMAL mutex - we can change to another
     mutex-type, if needed 
         --->a normal mutex - type attribute of the mutex 
                              is set to NORMAL 
--->based on the type of a mutex instance, its behaviour 
    will be different - for instance, a NORMAL mutex 
    will not check , for programmer's errors, in 
    mutex-operations - meaning, 
    operations on a NORMAL-mutex will be less robust - 
    less reliable....code will be less reliable...application 
    will be less reliable...keep reading   
--->if we use "error-checking mutex-type", its operations  will be more 
    robust/reliable, since it will check programmer's errors, in 
    mutex operations
    and return error, for such wrong/illegal  operations - 
    developers/programmers can test, for errors and 
    catch the illegal/wrong operations, in  their "mutex-semaphores"....  
    --->this will enable a more, robust coding practice...so, 
        code will be reliable and application will be reliable
---->such characteristics and robustness are missing, in 
     counting/binary semaphores - that is the design 
 
---->please refer to man pages of pthread mutex of Linux thread library 
---->man -k mutex ......go through appropriate manual pages...

---->refer to app_03_shared_LEDs_Mutex.c and  
                -->refer to thread_mutex_cond.c 
                   --->read the code and comments, and interpret
---->following are typical steps used, in setting up a mutex 
     and using it
                   --->a mutex object is instantiated - declared , 
                       in user-space - refer to code sample 
                   --->a mutex attribute object is also 
                       instantiated - declared 
                   --->in addition, certain attributes 
                       are initialized, in the mutex attribute object,using 
                       pthread lib. apis... 
                   --->eventually, pthread_mutex_init(&p1, &p2)
                       is invoked to initialize the 
                       mutex object 
                       --->p1 is pointer to the mutex object 
                           instance declared above
                       --->p2 is the pointer to a mutex 
                           attribute object instance declared above
--->refer to the above, code samples...

--->details of NORMAL-mutex vs other types of mutex?? 
---->refer to freehand_diagrams_25.08.2021.pdf / slides 28-29 
         --->a normal mutex does not check, for illegal 
             operations on the mutex - this can lead to 
             concurrent-programming bugs 
         --->this can lead to dead-lock, in an application - 
             threads cannot progress - application cannot 
             progress, so stalled - so, less reliable - 
             a reliability problem ??
         --->in addition, if a thread is not currently 
             the owner of a mutex, m1, it can unlock mutex,m1 - 
             although, this is a wrong operation, still allowed 
         --->so, no error checking  
---->refer to freehand_diagrams_25.08.2021.pdf / slides 30- 
         --->an error checking mutex does check, for 
             illegal operations and returns errors - 
             ideally, these errors must be diagnosed and 
             fixed, during development and testing 
         --->in addition, if a thread is not currently 
             the owner of a mutex, m1, it cannot unlock mutex,m1 - 
             since, this is a wrong operation, unlock(&m1) will 
             return an error 
         --->so, there is error checking  
         --->for error-checking features, mutex-operations use
             ownership information stored, in a mutex-objecti
         -->so, error checking mutex is useful, during 
            development and testing
         --->so, our program is more reliable, so application 
             is more reliable ....a reliable solution 
---->refer to freehand_diagrams_25.08.2021.pdf / slides 30 
     --->refer to the comments, in the above slides
--->so, error checking mutex is a better mutex type, during 
    development and testing stages..
--->once development and testing are completed, we can 
    replace error checking mutex, with a normal mutex type...
--->such error-checking characteristics are not present, in 
    semaphores - design does not support ??


--->more details on types of mutex will be discussed, during 
    EOS/RTOS....
          ---->in addition, there is a recursive- 
               mutex, which is tricky - this is a super-set of
               error-checking mutex 
               --->this does all the checking of 
                   error-checking and in addition, 
                   does more 
               --->if a thread is the owner of a 
                   mutex, it will be allowed to 
                   lock the mutex, recursively, several times 
                   --the same owner must unlock the 
                     mutex, as many times 
           -->depending upon application issues, we 
              may use normal or error checking, or 
              recursive  mutex-type                 
---->so, what is a typical NORMAL mutex ?? fill a summary
---->so, what is a typical  error checking mutex ?? fill a summary..

   ---->following is a good summary of a normal mutex - for 
        more details, refer to man pthread_mutexattr_settype 
        --->a normal mutex does not provide any form of 
             error checking on operations 
        ---->if a threadi/methodi invokes a lock(&m1) and 
             invokes a second lock(&m1), before unlock(&m1),
              threadi/tdi will be blocked forever
         ---->other threads dependent on m1 will also be 
              blocked forever
        ---> if  a threadi/methodi invokes unlock(&m1) on 
             an unlocked m1, the operation may lead to unexpected 
             behaviour - normal mutex will not check, such
             illegal behaviour... 
             --->interpertation is, that we should not do 
                  such coding 
        ---->if a threadi/methodi is not the owner of a mutex lock m1,
              and attempts to unlock m1, behaviour is unknown - 
             we must not attempt such operations
---->if a mutex,m1 is already, unlocked, is it okay to 
     do an unlock operation on that mutex ??
      --->this is an illegal operation - must not be done - 
          NORMAL mutex may not return error - ERRORCHECK 
          mutex will return error 

--->if a mutex, m1 is locked(owned), by threadi, is it okay 
    to unlock mutex,m1, by another threadi+1 ??
      --->this is an illegal, mutex operation 
      --->as per mutex rules, threadi(owner of mutex, m1) 
          must complete its critical section, cs11 and 
          unlock mutex, m1 - this is a legal operation 
      --->if we use a NORMAL mutex, the above operation 
          may not return error 
      --->if we use an ERRORCHECK mutex, the above 
          operation will return an error  
 
     ----->following is a good summary of an error checking 
            mutex ;

           ------>if a threadi/methodi attempts to access mutex lock m1,
                  before unlocking m1, lock(&m1) will return an 
                 error code - developer can check the error and catch
                 the cause of the error 
           ------>if a threadi/methodi invokes unlock(&m1) on unlocked m1,
                  and error will be generated - developer can catch 
                  the cause of the error - this will be treated, as 
                  an illegal operation...
            ----->if a threadi/methodi attempts to unlock mutex lock m1,
                  which is owned by another threadj, an error will be 
                  generated - developer can catch the cause of the 
                  error - this is an illegal operation 
         ---->error checking mutex uses an ownership field, in 
              this mutex object - this ownership field will 
                 store some thread id, that will be used by 
                 lock() and unlock() operations   
            ------>such OS features enable robust programming techniques...
-------->we will see similar features, in EOS and RTOS platforms - 
         however , there will be implementation differences and usage
           differences ....
----->now, refer to thread_mutex_cond.c and comments 

------>we will resume on  slightly 13.11.2020....

  ---->following is another, comprehensive summary of mutex-semaphores and
       their types :
       --->an OS mutex is a lock mechanism, not a counter mechanism
       --->typically, there is a lock state associated, with 
           a mutex object - there are two states, locked and 
           unlocked 
       --->there is a wq assoicated, with a mutex object 
       --->a mutex stores ownership details of the current 
           thread owner of the mutex lock 
       --->there are attributes associated, with a mutex 
           object - like NORMAL type or error checking type or 
           recursive type - there are other attributes also
       --->there are two possible operations on a mutex - 
           one is lock(&m1), which is a lock operation and 
           another is unlock(&m1), which is an unlock operation
       --->for more details, read the above sections.... 
       --->mutex semaphore vs counting/binary semaphores:
           -->a mutex is a lock by design - it is typically used 
              to protect shared-data/concurrency problems and 
              shared-resouce/concurrency problems - only two 
              states are supported - locked and unlocked states
           -->it supports ownership details,based on the 
              attributes - for instance, ownership details 
              are used, for error-checking and recursive 
              mutex operations - in the case of normal mutex, 
              this ownership detail is ignored
           -->many of these points apply to GPOS contexts, 
              but will be  slightly different, in the context of 
              RTOS /embedded
           -->refer to embedded documents/references, for more samples/
              designs 
           -->a semaphore is ideally meant, 
              for just synchronization 
              of processes/threads/tasks 
              or task/ISR and counting/synchronization of 
              resources or events - we cannot use a mutex, 
              for these synchronization jobs 
----->refer to typical producer/consumer models/programs,
      using counting semaphores... 
           --->a mutex is designed, for locking operations
              ,not synchronization operations - we need to 
              refer to code-samples/scenarios mutexes and 
              semaphores   
           -->if a mutex-lock is available, we must not use 
              a binary semaphore, for locking operations/jobs  
           -->by design, a semaphore does not have thread 
              ownership/identity information, as part of  
              a semaphore object  - since semaphore is used 
             for synchronization, ownership feature is not supported
             --->if we understand synchronization and semaphores, 
                 we can understand, why ownership feature is 
                 not supported
           -->binary semaphore can be used for synchronization 
              only, as per strict rules, if mutex-lock is 
              supported, in a platform  - binary semaphore should not be used, for 
              locking critical sections, in the presence of 
              a mutex mechanism
           -->binary semaphores can be used , for synchronization 
              between two threads, or between a thread and an ISR 
                -->refer to embedded/RTOS documents/ references, for 
                   samples/designs - refer to rtos documentation and 
                   code-samples/scenarios   

----> move on to 
      rtos study and connect the details .....
---->resume and complete on 04.09.2021...

----->following is a brief explanation on thread-semaphores - 
      basics remain the same, but these thread-semaphores
      are light-weight and very fast...
---->there is also a light-weight thread-semphore, which is 
     also, a counting semaphore , but implementation details
     differ - read the following sections and refer to 
     prodcons_threads1.c, app_02_switch_debounce_led.c, and 
     thread_mutex_cond.c
---->we can refer to prodcons_threads1.c and app_02_switch_debounce_led.c

     --->prodcons_threads1.c is similar to our 
         prod_1_2.c and cons_1_2.c of process-management code-samples
         --->for locks, thread-mutex-locks is used 
         --->for counting semaphores, thread-semaphores are used 
  
         -->refer to multithreading_n1.pdf, for details on 
            thread-semaphores, which is similar to thread 
            mutex - most of the basic principles of an OS- 
            semaphore
            apply to a thread-semaphore - in addition, 
            this is a lightweight semaphore
            and efficient, for threads of the same processi - in the
            context of lightweight design and implementation, 
            a thread-semaphore is similar to a thread-mutex - most 
            of the semaphore operations are completed, in user-space and 
            minimal number of system call APIs are used    
            -->there is a lightweight thread-semaphore 
               mechanism/object, for threads of a process
            --->these can be used, for typical 
                thread synchronization and resource or
                event counting 

         --->we will add more details to thread mutexes and 
             semaphores, for threads' assignment/hints,in 
             embedded scenarios   
--->now, refer to app_02_switch_debounce_led.c, for a typical 
    embedded scenario 
    ---->in this code-sample/scenario, a counting thread-semaphore
         is used, as event-counting, synchronization semaphore
    ---->hw switch-presses are the events 
    ---->in addition, consumer-thread and producer-thread 
         are synchronized, using this counting-semaphore - 
         there is an efficient, co-ordination between 
         consumer-thread and producer-thread 

----->now, you must complete first thread assignment..
      --->threading_assignment_I_feb2020.txt
      --->also, complete the multi-threaded, user-space code,
          for Linux device-model/sysfs assignment

---->now, move on to 2_rtos.txt and related references/slides...



---->certain additional points on thread semaphores and mutexes 
     ---->read the above details more than once and then, 
          read the following 
     --->a typical binary semaphore can be used, for locking, 
         if we do not have a mutex semaphore 
     --->if we have a mutex semaphore, do not use 
         a binary semaphore, for locking 
     --->however, we can use a binary semaphore, as a 
         synchronization semaphore between threads 
         ---->refer to figure 15.6 of rt_e_concepts.pdf 
         ---->we see typical coding, in the context 
              of EOS/RTOS platforms...
     ---->a counting semaphore can be used to manage 
          a set of identical events or a set of identical resources      
          --->refer to figure 15.10 of chapter 15 of rt_e_concepts.pdf
          --->this is a form of managing a set of events and 
              related processing ??
                 --->for each byte of data, a counting semaphore 
                     is incremented by one, in the ISR 
                 --->in a task, we decrement counting semaphore 
                     and use a byte 
                 --->when there are no bytes of data, counting 
                     semaphore's value will be 0 and th e
                     task will be blocked, in wq of the 
                     counting semaphore
                 --->in the future, when isr receives data, 
                     it will copy data into a buffer and 
                     increment counting semaphore and 
                     unblock the task, wq of counting semaphore
--->break on 28.11.2020....
      ------>refer to page 69 of 3_threadxug_g40c.pdf of ThreadX EOS/rtOS
              --->a scenario is briefly explained on usage of 
                  counting semaphores - meaning, a counting semaphore
                  can be used to allocate identical set of resources
                  --->meaning, if the counting semaphores value
                      is greater than 1, more than one thread
                      can decrement and allocate a resource instance 
                      per thread
                  --->the most common case is a binary semaphore, 
                      where only one thread can decrement 
                      and use a single resource or shared-data - 
                      this is the common case of the counting 
                      semaphore used to manage multiple, identical 
                      resources 
               ---->refer to our prod_1_2.c and cons_1_2.c code samples
                    and scenarios - here, we are using counting 
                    semaphores to allocate multiple, identical resources 
               ---->in the above code samples and scenarios, we have 
                    a single producer using multiple, identical resources
                    , using a counting semaphore - free buffer slots are
                    resources - main thread of producer is the only 
                    producer thread....
                    --->what happens, if we have multiple producer 
                        threads, in the producer ?? try to visualize 
                        and connect - if you have specific questions, 
                        do ask ?? this is another design pattern , 
                        which may be used, in our applications
----->we will defintely see more code samples/scenarios, in 
      embedded application AND EOS/RTOS platforms........  




----->following points are added to explain permissions 
      field of semget() /msgget() /shmget() 

      ---->for instance, semget(p1, p2, p2); 
            sem_id2 = semget(KEY1,3,IPC_CREAT | 0600);//with read/write permissions

             ---->refer to man semget 
              ---->man svipc

                ----->0600  means, the owner(uid) of this  semaphore will have read and 
                      modify permissions on this semaphore object/semaphores
                --->all other users/uids will not have permissions to access this semaphore object/semaphores

            
                   ---->the permissions field is not treated as a complete value, but 
                        3 sets of 3 bits each are evaluated - so, you can have values , like
                        0600, which means, most significant permissions are 110 - this means, 
                        owner/uid is provided read and write/modify access
 
            sem_id2 = semget(KEY1,3,IPC_CREAT | 0606);//with read/write permissions
                   ---->this will allow owner /uid and all other users/their uids, both access and modify

              ---->in the permissions field, second most significant position, provides 3 bits 
                            ---->this is for owner /uid 
                           ---->most signicant bit is for enabling read access    
                           ---->second  bit is for enabling write /modification access    
 
        
              ---->in the permissions field, second least significant position, provides 3 bits 
                            ---->this is for other users /uids 
                           ---->most signicant bit is for enabling read access    
                           ---->second  bit is for enabling write /modification access    


              ---->the permissions field must be interpreted differently , for 
                   different IPC  mechanisms......
                           --->in the case of message queue, it is about receiving and sending 
                          ---->in the case of shared-memory , it is about reading and writing
                            ----->in the case of semaphore, reading the value vs incrementing/
    										decrementing.... 



                          
    

 



   




       



 
             



   

  

             
  
                              



